{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from collections.abc import Iterable\n",
    "import functools\n",
    "import math\n",
    "from inspect import signature\n",
    "from numbers import Integral\n",
    "from numbers import Real\n",
    "import operator\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.utils.validation import _is_arraylike_not_scalar\n",
    "\n",
    "def validate_parameter_constraints(parameter_constraints, params, caller_name):\n",
    "    \"\"\"Validate types and values of given parameters.\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameter_constraints : dict or {\"no_validation\"}\n",
    "        If \"no_validation\", validation is skipped for this parameter.\n",
    "        If a dict, it must be a dictionary `param_name: list of constraints`.\n",
    "        A parameter is valid if it satisfies one of the constraints from the list.\n",
    "        Constraints can be:\n",
    "        - an Interval object, representing a continuous or discrete range of numbers\n",
    "        - the string \"array-like\"\n",
    "        - the string \"sparse matrix\"\n",
    "        - the string \"random_state\"\n",
    "        - callable\n",
    "        - None, meaning that None is a valid value for the parameter\n",
    "        - any type, meaning that any instance of this type is valid\n",
    "        - an Options object, representing a set of elements of a given type\n",
    "        - a StrOptions object, representing a set of strings\n",
    "        - the string \"boolean\"\n",
    "        - the string \"verbose\"\n",
    "        - the string \"cv_object\"\n",
    "        - the string \"missing_values\"\n",
    "        - a HasMethods object, representing method(s) an object must have\n",
    "        - a Hidden object, representing a constraint not meant to be exposed to the user\n",
    "    params : dict\n",
    "        A dictionary `param_name: param_value`. The parameters to validate against the\n",
    "        constraints.\n",
    "    caller_name : str\n",
    "        The name of the estimator or function or method that called this function.\n",
    "    \"\"\"\n",
    "    if len(set(parameter_constraints) - set(params)) != 0:\n",
    "        raise ValueError(\n",
    "            f\"The parameter constraints {list(parameter_constraints)}\"\n",
    "            \" contain unexpected parameters\"\n",
    "            f\" {set(parameter_constraints) - set(params)}\"\n",
    "        )\n",
    "\n",
    "    for param_name, param_val in params.items():\n",
    "        # We allow parameters to not have a constraint so that third party estimators\n",
    "        # can inherit from sklearn estimators without having to necessarily use the\n",
    "        # validation tools.\n",
    "        if param_name not in parameter_constraints:\n",
    "            continue\n",
    "\n",
    "        constraints = parameter_constraints[param_name]\n",
    "\n",
    "        if constraints == \"no_validation\":\n",
    "            continue\n",
    "\n",
    "        constraints = [make_constraint(constraint) for constraint in constraints]\n",
    "\n",
    "        for constraint in constraints:\n",
    "            if constraint.is_satisfied_by(param_val):\n",
    "                # this constraint is satisfied, no need to check further.\n",
    "                break\n",
    "        else:\n",
    "            # No constraint is satisfied, raise with an informative message.\n",
    "\n",
    "            # Ignore constraints that we don't want to expose in the error message,\n",
    "            # i.e. options that are for internal purpose or not officially supported.\n",
    "            constraints = [\n",
    "                constraint for constraint in constraints if not constraint.hidden\n",
    "            ]\n",
    "\n",
    "            if len(constraints) == 1:\n",
    "                constraints_str = f\"{constraints[0]}\"\n",
    "            else:\n",
    "                constraints_str = (\n",
    "                    f\"{', '.join([str(c) for c in constraints[:-1]])} or\"\n",
    "                    f\" {constraints[-1]}\"\n",
    "                )\n",
    "\n",
    "            raise ValueError(\n",
    "                f\"The {param_name!r} parameter of {caller_name} must be\"\n",
    "                f\" {constraints_str}. Got {param_val!r} instead.\"\n",
    "            )\n",
    "\n",
    "\n",
    "def make_constraint(constraint):\n",
    "    \"\"\"Convert the constraint into the appropriate Constraint object.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constraint : object\n",
    "        The constraint to convert.\n",
    "    Returns\n",
    "    -------\n",
    "    constraint : instance of _Constraint\n",
    "        The converted constraint.\n",
    "    \"\"\"\n",
    "    if isinstance(constraint, str) and constraint == \"array-like\":\n",
    "        return _ArrayLikes()\n",
    "    if isinstance(constraint, str) and constraint == \"sparse matrix\":\n",
    "        return _SparseMatrices()\n",
    "    if isinstance(constraint, str) and constraint == \"random_state\":\n",
    "        return _RandomStates()\n",
    "    if constraint is callable:\n",
    "        return _Callables()\n",
    "    if constraint is None:\n",
    "        return _NoneConstraint()\n",
    "    if isinstance(constraint, type):\n",
    "        return _InstancesOf(constraint)\n",
    "    if isinstance(constraint, (Interval, StrOptions, Options, HasMethods)):\n",
    "        return constraint\n",
    "    if isinstance(constraint, str) and constraint == \"boolean\":\n",
    "        return _Booleans()\n",
    "    if isinstance(constraint, str) and constraint == \"verbose\":\n",
    "        return _VerboseHelper()\n",
    "    if isinstance(constraint, str) and constraint == \"missing_values\":\n",
    "        return _MissingValues()\n",
    "    if isinstance(constraint, str) and constraint == \"cv_object\":\n",
    "        return _CVObjects()\n",
    "    if isinstance(constraint, Hidden):\n",
    "        constraint = make_constraint(constraint.constraint)\n",
    "        constraint.hidden = True\n",
    "        return constraint\n",
    "    raise ValueError(f\"Unknown constraint type: {constraint}\")\n",
    "\n",
    "\n",
    "def validate_params(parameter_constraints):\n",
    "    \"\"\"Decorator to validate types and values of functions and methods.\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameter_constraints : dict\n",
    "        A dictionary `param_name: list of constraints`. See the docstring of\n",
    "        `validate_parameter_constraints` for a description of the accepted constraints.\n",
    "        Note that the *args and **kwargs parameters are not validated and must not be\n",
    "        present in the parameter_constraints dictionary.\n",
    "    Returns\n",
    "    -------\n",
    "    decorated_function : function or method\n",
    "        The decorated function.\n",
    "    \"\"\"\n",
    "\n",
    "    def decorator(func):\n",
    "        # The dict of parameter constraints is set as an attribute of the function\n",
    "        # to make it possible to dynamically introspect the constraints for\n",
    "        # automatic testing.\n",
    "        setattr(func, \"_skl_parameter_constraints\", parameter_constraints)\n",
    "\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "\n",
    "            func_sig = signature(func)\n",
    "\n",
    "            # Map *args/**kwargs to the function signature\n",
    "            params = func_sig.bind(*args, **kwargs)\n",
    "            params.apply_defaults()\n",
    "\n",
    "            # ignore self/cls and positional/keyword markers\n",
    "            to_ignore = [\n",
    "                p.name\n",
    "                for p in func_sig.parameters.values()\n",
    "                if p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD)\n",
    "            ]\n",
    "            to_ignore += [\"self\", \"cls\"]\n",
    "            params = {k: v for k, v in params.arguments.items() if k not in to_ignore}\n",
    "\n",
    "            validate_parameter_constraints(\n",
    "                parameter_constraints, params, caller_name=func.__qualname__\n",
    "            )\n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def _type_name(t):\n",
    "    \"\"\"Convert type into human readable string.\"\"\"\n",
    "    module = t.__module__\n",
    "    qualname = t.__qualname__\n",
    "    if module == \"builtins\":\n",
    "        return qualname\n",
    "    elif t == Real:\n",
    "        return \"float\"\n",
    "    elif t == Integral:\n",
    "        return \"int\"\n",
    "    return f\"{module}.{qualname}\"\n",
    "\n",
    "\n",
    "class _Constraint(ABC):\n",
    "    \"\"\"Base class for the constraint objects.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.hidden = False\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_satisfied_by(self, val):\n",
    "        \"\"\"Whether or not a value satisfies the constraint.\n",
    "        Parameters\n",
    "        ----------\n",
    "        val : object\n",
    "            The value to check.\n",
    "        Returns\n",
    "        -------\n",
    "        is_satisfied : bool\n",
    "            Whether or not the constraint is satisfied by this value.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __str__(self):\n",
    "        \"\"\"A human readable representational string of the constraint.\"\"\"\n",
    "\n",
    "\n",
    "class _InstancesOf(_Constraint):\n",
    "    \"\"\"Constraint representing instances of a given type.\n",
    "    Parameters\n",
    "    ----------\n",
    "    type : type\n",
    "        The valid type.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, type):\n",
    "        super().__init__()\n",
    "        self.type = type\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return isinstance(val, self.type)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"an instance of {_type_name(self.type)!r}\"\n",
    "\n",
    "\n",
    "class _NoneConstraint(_Constraint):\n",
    "    \"\"\"Constraint representing the None singleton.\"\"\"\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return val is None\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"None\"\n",
    "\n",
    "\n",
    "class _NanConstraint(_Constraint):\n",
    "    \"\"\"Constraint representing the indicator `np.nan`.\"\"\"\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return isinstance(val, Real) and math.isnan(val)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"numpy.nan\"\n",
    "\n",
    "\n",
    "class _PandasNAConstraint(_Constraint):\n",
    "    \"\"\"Constraint representing the indicator `pd.NA`.\"\"\"\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        try:\n",
    "            import pandas as pd\n",
    "\n",
    "            return isinstance(val, type(pd.NA)) and pd.isna(val)\n",
    "        except ImportError:\n",
    "            return False\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"pandas.NA\"\n",
    "\n",
    "\n",
    "class Options(_Constraint):\n",
    "    \"\"\"Constraint representing a finite set of instances of a given type.\n",
    "    Parameters\n",
    "    ----------\n",
    "    type : type\n",
    "    options : set\n",
    "        The set of valid scalars.\n",
    "    deprecated : set or None, default=None\n",
    "        A subset of the `options` to mark as deprecated in the string\n",
    "        representation of the constraint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, type, options, *, deprecated=None):\n",
    "        super().__init__()\n",
    "        self.type = type\n",
    "        self.options = options\n",
    "        self.deprecated = deprecated or set()\n",
    "\n",
    "        if self.deprecated - self.options:\n",
    "            raise ValueError(\"The deprecated options must be a subset of the options.\")\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return isinstance(val, self.type) and val in self.options\n",
    "\n",
    "    def _mark_if_deprecated(self, option):\n",
    "        \"\"\"Add a deprecated mark to an option if needed.\"\"\"\n",
    "        option_str = f\"{option!r}\"\n",
    "        if option in self.deprecated:\n",
    "            option_str = f\"{option_str} (deprecated)\"\n",
    "        return option_str\n",
    "\n",
    "    def __str__(self):\n",
    "        options_str = (\n",
    "            f\"{', '.join([self._mark_if_deprecated(o) for o in self.options])}\"\n",
    "        )\n",
    "        return f\"a {_type_name(self.type)} among {{{options_str}}}\"\n",
    "\n",
    "\n",
    "class StrOptions(Options):\n",
    "    \"\"\"Constraint representing a finite set of strings.\n",
    "    Parameters\n",
    "    ----------\n",
    "    options : set of str\n",
    "        The set of valid strings.\n",
    "    deprecated : set of str or None, default=None\n",
    "        A subset of the `options` to mark as deprecated in the string\n",
    "        representation of the constraint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, options, *, deprecated=None):\n",
    "        super().__init__(type=str, options=options, deprecated=deprecated)\n",
    "\n",
    "\n",
    "class Interval(_Constraint):\n",
    "    \"\"\"Constraint representing a typed interval.\n",
    "    Parameters\n",
    "    ----------\n",
    "    type : {numbers.Integral, numbers.Real}\n",
    "        The set of numbers in which to set the interval.\n",
    "    left : float or int or None\n",
    "        The left bound of the interval. None means left bound is -∞.\n",
    "    right : float, int or None\n",
    "        The right bound of the interval. None means right bound is +∞.\n",
    "    closed : {\"left\", \"right\", \"both\", \"neither\"}\n",
    "        Whether the interval is open or closed. Possible choices are:\n",
    "        - `\"left\"`: the interval is closed on the left and open on the right.\n",
    "          It is equivalent to the interval `[ left, right )`.\n",
    "        - `\"right\"`: the interval is closed on the right and open on the left.\n",
    "          It is equivalent to the interval `( left, right ]`.\n",
    "        - `\"both\"`: the interval is closed.\n",
    "          It is equivalent to the interval `[ left, right ]`.\n",
    "        - `\"neither\"`: the interval is open.\n",
    "          It is equivalent to the interval `( left, right )`.\n",
    "    Notes\n",
    "    -----\n",
    "    Setting a bound to `None` and setting the interval closed is valid. For instance,\n",
    "    strictly speaking, `Interval(Real, 0, None, closed=\"both\")` corresponds to\n",
    "    `[0, +∞) U {+∞}`.\n",
    "    \"\"\"\n",
    "\n",
    "    @validate_params(\n",
    "        {\n",
    "            \"type\": [type],\n",
    "            \"left\": [Integral, Real, None],\n",
    "            \"right\": [Integral, Real, None],\n",
    "            \"closed\": [StrOptions({\"left\", \"right\", \"both\", \"neither\"})],\n",
    "        }\n",
    "    )\n",
    "    def __init__(self, type, left, right, *, closed):\n",
    "        super().__init__()\n",
    "        self.type = type\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.closed = closed\n",
    "\n",
    "        self._check_params()\n",
    "\n",
    "    def _check_params(self):\n",
    "        if self.type is Integral:\n",
    "            suffix = \"for an interval over the integers.\"\n",
    "            if self.left is not None and not isinstance(self.left, Integral):\n",
    "                raise TypeError(f\"Expecting left to be an int {suffix}\")\n",
    "            if self.right is not None and not isinstance(self.right, Integral):\n",
    "                raise TypeError(f\"Expecting right to be an int {suffix}\")\n",
    "            if self.left is None and self.closed in (\"left\", \"both\"):\n",
    "                raise ValueError(\n",
    "                    f\"left can't be None when closed == {self.closed} {suffix}\"\n",
    "                )\n",
    "            if self.right is None and self.closed in (\"right\", \"both\"):\n",
    "                raise ValueError(\n",
    "                    f\"right can't be None when closed == {self.closed} {suffix}\"\n",
    "                )\n",
    "\n",
    "        if self.right is not None and self.left is not None and self.right <= self.left:\n",
    "            raise ValueError(\n",
    "                f\"right can't be less than left. Got left={self.left} and \"\n",
    "                f\"right={self.right}\"\n",
    "            )\n",
    "\n",
    "    def __contains__(self, val):\n",
    "        if np.isnan(val):\n",
    "            return False\n",
    "\n",
    "        left_cmp = operator.lt if self.closed in (\"left\", \"both\") else operator.le\n",
    "        right_cmp = operator.gt if self.closed in (\"right\", \"both\") else operator.ge\n",
    "\n",
    "        left = -np.inf if self.left is None else self.left\n",
    "        right = np.inf if self.right is None else self.right\n",
    "\n",
    "        if left_cmp(val, left):\n",
    "            return False\n",
    "        if right_cmp(val, right):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        if not isinstance(val, self.type):\n",
    "            return False\n",
    "\n",
    "        return val in self\n",
    "\n",
    "    def __str__(self):\n",
    "        type_str = \"an int\" if self.type is Integral else \"a float\"\n",
    "        left_bracket = \"[\" if self.closed in (\"left\", \"both\") else \"(\"\n",
    "        left_bound = \"-inf\" if self.left is None else self.left\n",
    "        right_bound = \"inf\" if self.right is None else self.right\n",
    "        right_bracket = \"]\" if self.closed in (\"right\", \"both\") else \")\"\n",
    "        return (\n",
    "            f\"{type_str} in the range \"\n",
    "            f\"{left_bracket}{left_bound}, {right_bound}{right_bracket}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class _ArrayLikes(_Constraint):\n",
    "    \"\"\"Constraint representing array-likes\"\"\"\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return _is_arraylike_not_scalar(val)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"an array-like\"\n",
    "\n",
    "\n",
    "class _SparseMatrices(_Constraint):\n",
    "    \"\"\"Constraint representing sparse matrices.\"\"\"\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return issparse(val)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"a sparse matrix\"\n",
    "\n",
    "\n",
    "class _Callables(_Constraint):\n",
    "    \"\"\"Constraint representing callables.\"\"\"\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return callable(val)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"a callable\"\n",
    "\n",
    "\n",
    "class _RandomStates(_Constraint):\n",
    "    \"\"\"Constraint representing random states.\n",
    "    Convenience class for\n",
    "    [Interval(Integral, 0, 2**32 - 1, closed=\"both\"), np.random.RandomState, None]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._constraints = [\n",
    "            Interval(Integral, 0, 2**32 - 1, closed=\"both\"),\n",
    "            _InstancesOf(np.random.RandomState),\n",
    "            _NoneConstraint(),\n",
    "        ]\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return any(c.is_satisfied_by(val) for c in self._constraints)\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"{', '.join([str(c) for c in self._constraints[:-1]])} or\"\n",
    "            f\" {self._constraints[-1]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class _Booleans(_Constraint):\n",
    "    \"\"\"Constraint representing boolean likes.\n",
    "    Convenience class for\n",
    "    [bool, np.bool_, Integral (deprecated)]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._constraints = [\n",
    "            _InstancesOf(bool),\n",
    "            _InstancesOf(np.bool_),\n",
    "            _InstancesOf(Integral),\n",
    "        ]\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        # TODO(1.4) remove support for Integral.\n",
    "        if isinstance(val, Integral) and not isinstance(val, bool):\n",
    "            warnings.warn(\n",
    "                \"Passing an int for a boolean parameter is deprecated in version 1.2 \"\n",
    "                \"and won't be supported anymore in version 1.4.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "\n",
    "        return any(c.is_satisfied_by(val) for c in self._constraints)\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"{', '.join([str(c) for c in self._constraints[:-1]])} or\"\n",
    "            f\" {self._constraints[-1]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class _VerboseHelper(_Constraint):\n",
    "    \"\"\"Helper constraint for the verbose parameter.\n",
    "    Convenience class for\n",
    "    [Interval(Integral, 0, None, closed=\"left\"), bool, numpy.bool_]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._constraints = [\n",
    "            Interval(Integral, 0, None, closed=\"left\"),\n",
    "            _InstancesOf(bool),\n",
    "            _InstancesOf(np.bool_),\n",
    "        ]\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return any(c.is_satisfied_by(val) for c in self._constraints)\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"{', '.join([str(c) for c in self._constraints[:-1]])} or\"\n",
    "            f\" {self._constraints[-1]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class _MissingValues(_Constraint):\n",
    "    \"\"\"Helper constraint for the `missing_values` parameters.\n",
    "    Convenience for\n",
    "    [\n",
    "        Integral,\n",
    "        Interval(Real, None, None, closed=\"both\"),\n",
    "        str,\n",
    "        None,\n",
    "        _NanConstraint(),\n",
    "        _PandasNAConstraint(),\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._constraints = [\n",
    "            _InstancesOf(Integral),\n",
    "            # we use an interval of Real to ignore np.nan that has its own constraint\n",
    "            Interval(Real, None, None, closed=\"both\"),\n",
    "            _InstancesOf(str),\n",
    "            _NoneConstraint(),\n",
    "            _NanConstraint(),\n",
    "            _PandasNAConstraint(),\n",
    "        ]\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return any(c.is_satisfied_by(val) for c in self._constraints)\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"{', '.join([str(c) for c in self._constraints[:-1]])} or\"\n",
    "            f\" {self._constraints[-1]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class HasMethods(_Constraint):\n",
    "    \"\"\"Constraint representing objects that expose specific methods.\n",
    "    It is useful for parameters following a protocol and where we don't want to impose\n",
    "    an affiliation to a specific module or class.\n",
    "    Parameters\n",
    "    ----------\n",
    "    methods : str or list of str\n",
    "        The method(s) that the object is expected to expose.\n",
    "    \"\"\"\n",
    "\n",
    "    @validate_params({\"methods\": [str, list]})\n",
    "    def __init__(self, methods):\n",
    "        super().__init__()\n",
    "        if isinstance(methods, str):\n",
    "            methods = [methods]\n",
    "        self.methods = methods\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return all(callable(getattr(val, method, None)) for method in self.methods)\n",
    "\n",
    "    def __str__(self):\n",
    "        if len(self.methods) == 1:\n",
    "            methods = f\"{self.methods[0]!r}\"\n",
    "        else:\n",
    "            methods = (\n",
    "                f\"{', '.join([repr(m) for m in self.methods[:-1]])} and\"\n",
    "                f\" {self.methods[-1]!r}\"\n",
    "            )\n",
    "        return f\"an object implementing {methods}\"\n",
    "\n",
    "\n",
    "class _IterablesNotString(_Constraint):\n",
    "    \"\"\"Constraint representing iterables that are not strings.\"\"\"\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return isinstance(val, Iterable) and not isinstance(val, str)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"an iterable\"\n",
    "\n",
    "\n",
    "class _CVObjects(_Constraint):\n",
    "    \"\"\"Constraint representing cv objects.\n",
    "    Convenient class for\n",
    "    [\n",
    "        Interval(Integral, 2, None, closed=\"left\"),\n",
    "        HasMethods([\"split\", \"get_n_splits\"]),\n",
    "        _IterablesNotString(),\n",
    "        None,\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._constraints = [\n",
    "            Interval(Integral, 2, None, closed=\"left\"),\n",
    "            HasMethods([\"split\", \"get_n_splits\"]),\n",
    "            _IterablesNotString(),\n",
    "            _NoneConstraint(),\n",
    "        ]\n",
    "\n",
    "    def is_satisfied_by(self, val):\n",
    "        return any(c.is_satisfied_by(val) for c in self._constraints)\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"{', '.join([str(c) for c in self._constraints[:-1]])} or\"\n",
    "            f\" {self._constraints[-1]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class Hidden:\n",
    "    \"\"\"Class encapsulating a constraint not meant to be exposed to the user.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constraint : str or _Constraint instance\n",
    "        The constraint to be used internally.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, constraint):\n",
    "        self.constraint = constraint\n",
    "\n",
    "\n",
    "def generate_invalid_param_val(constraint, constraints=None):\n",
    "    \"\"\"Return a value that does not satisfy the constraint.\n",
    "    Raises a NotImplementedError if there exists no invalid value for this constraint.\n",
    "    This is only useful for testing purpose.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constraint : _Constraint instance\n",
    "        The constraint to generate a value for.\n",
    "    constraints : list of _Constraint instances or None, default=None\n",
    "        The list of all constraints for this parameter. If None, the list only\n",
    "        containing `constraint` is used.\n",
    "    Returns\n",
    "    -------\n",
    "    val : object\n",
    "        A value that does not satisfy the constraint.\n",
    "    \"\"\"\n",
    "    if isinstance(constraint, StrOptions):\n",
    "        return f\"not {' or '.join(constraint.options)}\"\n",
    "\n",
    "    if isinstance(constraint, _MissingValues):\n",
    "        return np.array([1, 2, 3])\n",
    "\n",
    "    if isinstance(constraint, _VerboseHelper):\n",
    "        return -1\n",
    "\n",
    "    if isinstance(constraint, HasMethods):\n",
    "        return type(\"HasNotMethods\", (), {})()\n",
    "\n",
    "    if isinstance(constraint, _IterablesNotString):\n",
    "        return \"a string\"\n",
    "\n",
    "    if isinstance(constraint, _CVObjects):\n",
    "        return \"not a cv object\"\n",
    "\n",
    "    if not isinstance(constraint, Interval):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # constraint is an interval\n",
    "    constraints = [constraint] if constraints is None else constraints\n",
    "    return _generate_invalid_param_val_interval(constraint, constraints)\n",
    "\n",
    "\n",
    "def _generate_invalid_param_val_interval(interval, constraints):\n",
    "    \"\"\"Return a value that does not satisfy an interval constraint.\n",
    "    Generating an invalid value for an integer interval depends on the other constraints\n",
    "    since an int is a real, meaning that it can be valid for a real interval.\n",
    "    Assumes that there can be at most 2 interval constraints: one integer interval\n",
    "    and/or one real interval.\n",
    "    This is only useful for testing purpose.\n",
    "    Parameters\n",
    "    ----------\n",
    "    interval : Interval instance\n",
    "        The interval to generate a value for.\n",
    "    constraints : list of _Constraint instances\n",
    "        The list of all constraints for this parameter.\n",
    "    Returns\n",
    "    -------\n",
    "    val : object\n",
    "        A value that does not satisfy the interval constraint.\n",
    "    \"\"\"\n",
    "    if interval.type is Real:\n",
    "        # generate a non-integer value such that it can't be valid even if there's also\n",
    "        # an integer interval constraint.\n",
    "        if interval.left is None and interval.right is None:\n",
    "            if interval.closed in (\"left\", \"neither\"):\n",
    "                return np.inf\n",
    "            elif interval.closed in (\"right\", \"neither\"):\n",
    "                return -np.inf\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        if interval.left is not None:\n",
    "            return np.floor(interval.left) - 0.5\n",
    "        else:  # right is not None\n",
    "            return np.ceil(interval.right) + 0.5\n",
    "\n",
    "    else:  # interval.type is Integral\n",
    "        if interval.left is None and interval.right is None:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # We need to check if there's also a real interval constraint to generate a\n",
    "        # value that is not valid for any of the 2 interval constraints.\n",
    "        real_intervals = [\n",
    "            i for i in constraints if isinstance(i, Interval) and i.type is Real\n",
    "        ]\n",
    "        real_interval = real_intervals[0] if real_intervals else None\n",
    "\n",
    "        if real_interval is None:\n",
    "            # Only the integer interval constraint -> easy\n",
    "            if interval.left is not None:\n",
    "                return interval.left - 1\n",
    "            else:  # interval.right is not None\n",
    "                return interval.right + 1\n",
    "\n",
    "        # There's also a real interval constraint. Try to find a value left to both or\n",
    "        # right to both or in between them.\n",
    "\n",
    "        # redefine left and right bounds to be smallest and largest valid integers in\n",
    "        # both intervals.\n",
    "        int_left = interval.left\n",
    "        if int_left is not None and interval.closed in (\"right\", \"neither\"):\n",
    "            int_left = int_left + 1\n",
    "\n",
    "        int_right = interval.right\n",
    "        if int_right is not None and interval.closed in (\"left\", \"neither\"):\n",
    "            int_right = int_right - 1\n",
    "\n",
    "        real_left = real_interval.left\n",
    "        if real_interval.left is not None:\n",
    "            real_left = int(np.ceil(real_interval.left))\n",
    "            if real_interval.closed in (\"right\", \"neither\"):\n",
    "                real_left = real_left + 1\n",
    "\n",
    "        real_right = real_interval.right\n",
    "        if real_interval.right is not None:\n",
    "            real_right = int(np.floor(real_interval.right))\n",
    "            if real_interval.closed in (\"left\", \"neither\"):\n",
    "                real_right = real_right - 1\n",
    "\n",
    "        if int_left is not None and real_left is not None:\n",
    "            # there exists an int left to both intervals\n",
    "            return min(int_left, real_left) - 1\n",
    "\n",
    "        if int_right is not None and real_right is not None:\n",
    "            # there exists an int right to both intervals\n",
    "            return max(int_right, real_right) + 1\n",
    "\n",
    "        if int_left is not None:\n",
    "            if real_right is not None and int_left - real_right >= 2:\n",
    "                # there exists an int between the 2 intervals\n",
    "                return int_left - 1\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:  # int_right is not None\n",
    "            if real_left is not None and real_left - int_right >= 2:\n",
    "                # there exists an int between the 2 intervals\n",
    "                return int_right + 1\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "\n",
    "def generate_valid_param(constraint):\n",
    "    \"\"\"Return a value that does satisfy a constraint.\n",
    "    This is only useful for testing purpose.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constraint : Constraint instance\n",
    "        The constraint to generate a value for.\n",
    "    Returns\n",
    "    -------\n",
    "    val : object\n",
    "        A value that does satisfy the constraint.\n",
    "    \"\"\"\n",
    "    if isinstance(constraint, _ArrayLikes):\n",
    "        return np.array([1, 2, 3])\n",
    "\n",
    "    if isinstance(constraint, _SparseMatrices):\n",
    "        return csr_matrix([[0, 1], [1, 0]])\n",
    "\n",
    "    if isinstance(constraint, _RandomStates):\n",
    "        return np.random.RandomState(42)\n",
    "\n",
    "    if isinstance(constraint, _Callables):\n",
    "        return lambda x: x\n",
    "\n",
    "    if isinstance(constraint, _NoneConstraint):\n",
    "        return None\n",
    "\n",
    "    if isinstance(constraint, _InstancesOf):\n",
    "        return constraint.type()\n",
    "\n",
    "    if isinstance(constraint, _Booleans):\n",
    "        return True\n",
    "\n",
    "    if isinstance(constraint, _VerboseHelper):\n",
    "        return 1\n",
    "\n",
    "    if isinstance(constraint, _MissingValues):\n",
    "        return np.nan\n",
    "\n",
    "    if isinstance(constraint, HasMethods):\n",
    "        return type(\n",
    "            \"ValidHasMethods\", (), {m: lambda self: None for m in constraint.methods}\n",
    "        )()\n",
    "\n",
    "    if isinstance(constraint, _IterablesNotString):\n",
    "        return [1, 2, 3]\n",
    "\n",
    "    if isinstance(constraint, _CVObjects):\n",
    "        return 5\n",
    "\n",
    "    if isinstance(constraint, Options):  # includes StrOptions\n",
    "        for option in constraint.options:\n",
    "            return option\n",
    "\n",
    "    if isinstance(constraint, Interval):\n",
    "        interval = constraint\n",
    "        if interval.left is None and interval.right is None:\n",
    "            return 0\n",
    "        elif interval.left is None:\n",
    "            return interval.right - 1\n",
    "        elif interval.right is None:\n",
    "            return interval.left + 1\n",
    "        else:\n",
    "            if interval.type is Real:\n",
    "                return (interval.left + interval.right) / 2\n",
    "            else:\n",
    "                return interval.left + 1\n",
    "\n",
    "    raise ValueError(f\"Unknown constraint type: {constraint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import platform\n",
    "import inspect\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import __version__\n",
    "from sklearn._config import get_config\n",
    "from sklearn.utils import _IS_32BIT\n",
    "from sklearn.utils._tags import (\n",
    "    _DEFAULT_TAGS,\n",
    ")\n",
    "from sklearn.utils.validation import check_X_y\n",
    "from sklearn.utils.validation import check_array\n",
    "from sklearn.utils.validation import _check_y\n",
    "from sklearn.utils.validation import _num_features\n",
    "from sklearn.utils.validation import _check_feature_names_in\n",
    "from sklearn.utils.validation import _generate_get_feature_names_out\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.validation import _get_feature_names\n",
    "from sklearn.utils._estimator_html_repr import estimator_html_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone(estimator, *, safe=True):\n",
    "    \"\"\"Construct a new unfitted estimator with the same parameters.\n",
    "    Clone does a deep copy of the model in an estimator\n",
    "    without actually copying attached data. It returns a new estimator\n",
    "    with the same parameters that has not been fitted on any data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : {list, tuple, set} of estimator instance or a single \\\n",
    "            estimator instance\n",
    "        The estimator or group of estimators to be cloned.\n",
    "    safe : bool, default=True\n",
    "        If safe is False, clone will fall back to a deep copy on objects\n",
    "        that are not estimators.\n",
    "    Returns\n",
    "    -------\n",
    "    estimator : object\n",
    "        The deep copy of the input, an estimator if input is an estimator.\n",
    "    Notes\n",
    "    -----\n",
    "    If the estimator's `random_state` parameter is an integer (or if the\n",
    "    estimator doesn't have a `random_state` parameter), an *exact clone* is\n",
    "    returned: the clone and the original estimator will give the exact same\n",
    "    results. Otherwise, *statistical clone* is returned: the clone might\n",
    "    return different results from the original estimator. More details can be\n",
    "    found in :ref:`randomness`.\n",
    "    \"\"\"\n",
    "    estimator_type = type(estimator)\n",
    "    # XXX: not handling dictionaries\n",
    "    if estimator_type in (list, tuple, set, frozenset):\n",
    "        return estimator_type([clone(e, safe=safe) for e in estimator])\n",
    "    elif not hasattr(estimator, \"get_params\") or isinstance(estimator, type):\n",
    "        if not safe:\n",
    "            return copy.deepcopy(estimator)\n",
    "        else:\n",
    "            if isinstance(estimator, type):\n",
    "                raise TypeError(\n",
    "                    \"Cannot clone object. \"\n",
    "                    + \"You should provide an instance of \"\n",
    "                    + \"scikit-learn estimator instead of a class.\"\n",
    "                )\n",
    "            else:\n",
    "                raise TypeError(\n",
    "                    \"Cannot clone object '%s' (type %s): \"\n",
    "                    \"it does not seem to be a scikit-learn \"\n",
    "                    \"estimator as it does not implement a \"\n",
    "                    \"'get_params' method.\" % (repr(estimator), type(estimator))\n",
    "                )\n",
    "\n",
    "    klass = estimator.__class__\n",
    "    new_object_params = estimator.get_params(deep=False)\n",
    "    for name, param in new_object_params.items():\n",
    "        new_object_params[name] = clone(param, safe=False)\n",
    "    new_object = klass(**new_object_params)\n",
    "    params_set = new_object.get_params(deep=False)\n",
    "\n",
    "    # quick sanity check of the parameters of the clone\n",
    "    for name in new_object_params:\n",
    "        param1 = new_object_params[name]\n",
    "        param2 = params_set[name]\n",
    "        if param1 is not param2:\n",
    "            raise RuntimeError(\n",
    "                \"Cannot clone object %s, as the constructor \"\n",
    "                \"either does not set or modifies parameter %s\" % (estimator, name)\n",
    "            )\n",
    "    return new_object\n",
    "\n",
    "\n",
    "class BaseEstimator:\n",
    "    \"\"\"Base class for all estimators in scikit-learn.\n",
    "    Notes\n",
    "    -----\n",
    "    All estimators should specify all the parameters that can be set\n",
    "    at the class level in their ``__init__`` as explicit keyword\n",
    "    arguments (no ``*args`` or ``**kwargs``).\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def _get_param_names(cls):\n",
    "        \"\"\"Get parameter names for the estimator\"\"\"\n",
    "        # fetch the constructor or the original constructor before\n",
    "        # deprecation wrapping if any\n",
    "        init = getattr(cls.__init__, \"deprecated_original\", cls.__init__)\n",
    "        if init is object.__init__:\n",
    "            # No explicit constructor to introspect\n",
    "            return []\n",
    "\n",
    "        # introspect the constructor arguments to find the model parameters\n",
    "        # to represent\n",
    "        init_signature = inspect.signature(init)\n",
    "        # Consider the constructor parameters excluding 'self'\n",
    "        parameters = [\n",
    "            p\n",
    "            for p in init_signature.parameters.values()\n",
    "            if p.name != \"self\" and p.kind != p.VAR_KEYWORD\n",
    "        ]\n",
    "        for p in parameters:\n",
    "            if p.kind == p.VAR_POSITIONAL:\n",
    "                raise RuntimeError(\n",
    "                    \"scikit-learn estimators should always \"\n",
    "                    \"specify their parameters in the signature\"\n",
    "                    \" of their __init__ (no varargs).\"\n",
    "                    \" %s with constructor %s doesn't \"\n",
    "                    \" follow this convention.\" % (cls, init_signature)\n",
    "                )\n",
    "        # Extract and sort argument names excluding 'self'\n",
    "        return sorted([p.name for p in parameters])\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"\n",
    "        Get parameters for this estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        deep : bool, default=True\n",
    "            If True, will return the parameters for this estimator and\n",
    "            contained subobjects that are estimators.\n",
    "        Returns\n",
    "        -------\n",
    "        params : dict\n",
    "            Parameter names mapped to their values.\n",
    "        \"\"\"\n",
    "        out = dict()\n",
    "        for key in self._get_param_names():\n",
    "            value = getattr(self, key)\n",
    "            if deep and hasattr(value, \"get_params\") and not isinstance(value, type):\n",
    "                deep_items = value.get_params().items()\n",
    "                out.update((key + \"__\" + k, val) for k, val in deep_items)\n",
    "            out[key] = value\n",
    "        return out\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set the parameters of this estimator.\n",
    "        The method works on simple estimators as well as on nested objects\n",
    "        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
    "        parameters of the form ``<component>__<parameter>`` so that it's\n",
    "        possible to update each component of a nested object.\n",
    "        Parameters\n",
    "        ----------\n",
    "        **params : dict\n",
    "            Estimator parameters.\n",
    "        Returns\n",
    "        -------\n",
    "        self : estimator instance\n",
    "            Estimator instance.\n",
    "        \"\"\"\n",
    "        if not params:\n",
    "            # Simple optimization to gain speed (inspect is slow)\n",
    "            return self\n",
    "        valid_params = self.get_params(deep=True)\n",
    "\n",
    "        nested_params = defaultdict(dict)  # grouped by prefix\n",
    "        for key, value in params.items():\n",
    "            key, delim, sub_key = key.partition(\"__\")\n",
    "            if key not in valid_params:\n",
    "                local_valid_params = self._get_param_names()\n",
    "                raise ValueError(\n",
    "                    f\"Invalid parameter {key!r} for estimator {self}. \"\n",
    "                    f\"Valid parameters are: {local_valid_params!r}.\"\n",
    "                )\n",
    "\n",
    "            if delim:\n",
    "                nested_params[key][sub_key] = value\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "                valid_params[key] = value\n",
    "\n",
    "        for key, sub_params in nested_params.items():\n",
    "            valid_params[key].set_params(**sub_params)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __repr__(self, N_CHAR_MAX=700):\n",
    "        # N_CHAR_MAX is the (approximate) maximum number of non-blank\n",
    "        # characters to render. We pass it as an optional parameter to ease\n",
    "        # the tests.\n",
    "\n",
    "        from sklearn.utils._pprint import _EstimatorPrettyPrinter\n",
    "\n",
    "        N_MAX_ELEMENTS_TO_SHOW = 30  # number of elements to show in sequences\n",
    "\n",
    "        # use ellipsis for sequences with a lot of elements\n",
    "        pp = _EstimatorPrettyPrinter(\n",
    "            compact=True,\n",
    "            indent=1,\n",
    "            indent_at_name=True,\n",
    "            n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW,\n",
    "        )\n",
    "\n",
    "        repr_ = pp.pformat(self)\n",
    "\n",
    "        # Use bruteforce ellipsis when there are a lot of non-blank characters\n",
    "        n_nonblank = len(\"\".join(repr_.split()))\n",
    "        if n_nonblank > N_CHAR_MAX:\n",
    "            lim = N_CHAR_MAX // 2  # apprx number of chars to keep on both ends\n",
    "            regex = r\"^(\\s*\\S){%d}\" % lim\n",
    "            # The regex '^(\\s*\\S){%d}' % n\n",
    "            # matches from the start of the string until the nth non-blank\n",
    "            # character:\n",
    "            # - ^ matches the start of string\n",
    "            # - (pattern){n} matches n repetitions of pattern\n",
    "            # - \\s*\\S matches a non-blank char following zero or more blanks\n",
    "            left_lim = re.match(regex, repr_).end()\n",
    "            right_lim = re.match(regex, repr_[::-1]).end()\n",
    "\n",
    "            if \"\\n\" in repr_[left_lim:-right_lim]:\n",
    "                # The left side and right side aren't on the same line.\n",
    "                # To avoid weird cuts, e.g.:\n",
    "                # categoric...ore',\n",
    "                # we need to start the right side with an appropriate newline\n",
    "                # character so that it renders properly as:\n",
    "                # categoric...\n",
    "                # handle_unknown='ignore',\n",
    "                # so we add [^\\n]*\\n which matches until the next \\n\n",
    "                regex += r\"[^\\n]*\\n\"\n",
    "                right_lim = re.match(regex, repr_[::-1]).end()\n",
    "\n",
    "            ellipsis = \"...\"\n",
    "            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n",
    "                # Only add ellipsis if it results in a shorter repr\n",
    "                repr_ = repr_[:left_lim] + \"...\" + repr_[-right_lim:]\n",
    "\n",
    "        return repr_\n",
    "\n",
    "    def __getstate__(self):\n",
    "        try:\n",
    "            state = super().__getstate__()\n",
    "        except AttributeError:\n",
    "            state = self.__dict__.copy()\n",
    "\n",
    "        if type(self).__module__.startswith(\"sklearn.\"):\n",
    "            return dict(state.items(), _sklearn_version=__version__)\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if type(self).__module__.startswith(\"sklearn.\"):\n",
    "            pickle_version = state.pop(\"_sklearn_version\", \"pre-0.18\")\n",
    "            if pickle_version != __version__:\n",
    "                warnings.warn(\n",
    "                    \"Trying to unpickle estimator {0} from version {1} when \"\n",
    "                    \"using version {2}. This might lead to breaking code or \"\n",
    "                    \"invalid results. Use at your own risk. \"\n",
    "                    \"For more info please refer to:\\n\"\n",
    "                    \"https://scikit-learn.org/stable/model_persistence.html\"\n",
    "                    \"#security-maintainability-limitations\".format(\n",
    "                        self.__class__.__name__, pickle_version, __version__\n",
    "                    ),\n",
    "                    UserWarning,\n",
    "                )\n",
    "        try:\n",
    "            super().__setstate__(state)\n",
    "        except AttributeError:\n",
    "            self.__dict__.update(state)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return _DEFAULT_TAGS\n",
    "\n",
    "    def _get_tags(self):\n",
    "        collected_tags = {}\n",
    "        for base_class in reversed(inspect.getmro(self.__class__)):\n",
    "            if hasattr(base_class, \"_more_tags\"):\n",
    "                # need the if because mixins might not have _more_tags\n",
    "                # but might do redundant work in estimators\n",
    "                # (i.e. calling more tags on BaseEstimator multiple times)\n",
    "                more_tags = base_class._more_tags(self)\n",
    "                collected_tags.update(more_tags)\n",
    "        return collected_tags\n",
    "\n",
    "    def _check_n_features(self, X, reset):\n",
    "        \"\"\"Set the `n_features_in_` attribute, or check against it.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {ndarray, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        reset : bool\n",
    "            If True, the `n_features_in_` attribute is set to `X.shape[1]`.\n",
    "            If False and the attribute exists, then check that it is equal to\n",
    "            `X.shape[1]`. If False and the attribute does *not* exist, then\n",
    "            the check is skipped.\n",
    "            .. note::\n",
    "               It is recommended to call reset=True in `fit` and in the first\n",
    "               call to `partial_fit`. All other methods that validate `X`\n",
    "               should set `reset=False`.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n_features = _num_features(X)\n",
    "        except TypeError as e:\n",
    "            if not reset and hasattr(self, \"n_features_in_\"):\n",
    "                raise ValueError(\n",
    "                    \"X does not contain any features, but \"\n",
    "                    f\"{self.__class__.__name__} is expecting \"\n",
    "                    f\"{self.n_features_in_} features\"\n",
    "                ) from e\n",
    "            # If the number of features is not defined and reset=True,\n",
    "            # then we skip this check\n",
    "            return\n",
    "\n",
    "        if reset:\n",
    "            self.n_features_in_ = n_features\n",
    "            return\n",
    "\n",
    "        if not hasattr(self, \"n_features_in_\"):\n",
    "            # Skip this check if the expected number of expected input features\n",
    "            # was not recorded by calling fit first. This is typically the case\n",
    "            # for stateless transformers.\n",
    "            return\n",
    "\n",
    "        if n_features != self.n_features_in_:\n",
    "            raise ValueError(\n",
    "                f\"X has {n_features} features, but {self.__class__.__name__} \"\n",
    "                f\"is expecting {self.n_features_in_} features as input.\"\n",
    "            )\n",
    "\n",
    "    def _check_feature_names(self, X, *, reset):\n",
    "        \"\"\"Set or check the `feature_names_in_` attribute.\n",
    "        .. versionadded:: 1.0\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {ndarray, dataframe} of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        reset : bool\n",
    "            Whether to reset the `feature_names_in_` attribute.\n",
    "            If False, the input will be checked for consistency with\n",
    "            feature names of data provided when reset was last True.\n",
    "            .. note::\n",
    "               It is recommended to call `reset=True` in `fit` and in the first\n",
    "               call to `partial_fit`. All other methods that validate `X`\n",
    "               should set `reset=False`.\n",
    "        \"\"\"\n",
    "\n",
    "        if reset:\n",
    "            feature_names_in = _get_feature_names(X)\n",
    "            if feature_names_in is not None:\n",
    "                self.feature_names_in_ = feature_names_in\n",
    "            elif hasattr(self, \"feature_names_in_\"):\n",
    "                # Delete the attribute when the estimator is fitted on a new dataset\n",
    "                # that has no feature names.\n",
    "                delattr(self, \"feature_names_in_\")\n",
    "            return\n",
    "\n",
    "        fitted_feature_names = getattr(self, \"feature_names_in_\", None)\n",
    "        X_feature_names = _get_feature_names(X)\n",
    "\n",
    "        if fitted_feature_names is None and X_feature_names is None:\n",
    "            # no feature names seen in fit and in X\n",
    "            return\n",
    "\n",
    "        if X_feature_names is not None and fitted_feature_names is None:\n",
    "            warnings.warn(\n",
    "                f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
    "                \" feature names\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if X_feature_names is None and fitted_feature_names is not None:\n",
    "            warnings.warn(\n",
    "                \"X does not have valid feature names, but\"\n",
    "                f\" {self.__class__.__name__} was fitted with feature names\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # validate the feature names against the `feature_names_in_` attribute\n",
    "        if len(fitted_feature_names) != len(X_feature_names) or np.any(\n",
    "            fitted_feature_names != X_feature_names\n",
    "        ):\n",
    "            message = (\n",
    "                \"The feature names should match those that were \"\n",
    "                \"passed during fit. Starting version 1.2, an error will be raised.\\n\"\n",
    "            )\n",
    "            fitted_feature_names_set = set(fitted_feature_names)\n",
    "            X_feature_names_set = set(X_feature_names)\n",
    "\n",
    "            unexpected_names = sorted(X_feature_names_set - fitted_feature_names_set)\n",
    "            missing_names = sorted(fitted_feature_names_set - X_feature_names_set)\n",
    "\n",
    "            def add_names(names):\n",
    "                output = \"\"\n",
    "                max_n_names = 5\n",
    "                for i, name in enumerate(names):\n",
    "                    if i >= max_n_names:\n",
    "                        output += \"- ...\\n\"\n",
    "                        break\n",
    "                    output += f\"- {name}\\n\"\n",
    "                return output\n",
    "\n",
    "            if unexpected_names:\n",
    "                message += \"Feature names unseen at fit time:\\n\"\n",
    "                message += add_names(unexpected_names)\n",
    "\n",
    "            if missing_names:\n",
    "                message += \"Feature names seen at fit time, yet now missing:\\n\"\n",
    "                message += add_names(missing_names)\n",
    "\n",
    "            if not missing_names and not unexpected_names:\n",
    "                message += (\n",
    "                    \"Feature names must be in the same order as they were in fit.\\n\"\n",
    "                )\n",
    "\n",
    "            warnings.warn(message, FutureWarning)\n",
    "\n",
    "    def _validate_data(\n",
    "        self,\n",
    "        X=\"no_validation\",\n",
    "        y=\"no_validation\",\n",
    "        reset=True,\n",
    "        validate_separately=False,\n",
    "        **check_params,\n",
    "    ):\n",
    "        \"\"\"Validate input data and set or check the `n_features_in_` attribute.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix, dataframe} of shape \\\n",
    "                (n_samples, n_features), default='no validation'\n",
    "            The input samples.\n",
    "            If `'no_validation'`, no validation is performed on `X`. This is\n",
    "            useful for meta-estimator which can delegate input validation to\n",
    "            their underlying estimator(s). In that case `y` must be passed and\n",
    "            the only accepted `check_params` are `multi_output` and\n",
    "            `y_numeric`.\n",
    "        y : array-like of shape (n_samples,), default='no_validation'\n",
    "            The targets.\n",
    "            - If `None`, `check_array` is called on `X`. If the estimator's\n",
    "              requires_y tag is True, then an error will be raised.\n",
    "            - If `'no_validation'`, `check_array` is called on `X` and the\n",
    "              estimator's requires_y tag is ignored. This is a default\n",
    "              placeholder and is never meant to be explicitly set. In that case\n",
    "              `X` must be passed.\n",
    "            - Otherwise, only `y` with `_check_y` or both `X` and `y` are\n",
    "              checked with either `check_array` or `check_X_y` depending on\n",
    "              `validate_separately`.\n",
    "        reset : bool, default=True\n",
    "            Whether to reset the `n_features_in_` attribute.\n",
    "            If False, the input will be checked for consistency with data\n",
    "            provided when reset was last True.\n",
    "            .. note::\n",
    "               It is recommended to call reset=True in `fit` and in the first\n",
    "               call to `partial_fit`. All other methods that validate `X`\n",
    "               should set `reset=False`.\n",
    "        validate_separately : False or tuple of dicts, default=False\n",
    "            Only used if y is not None.\n",
    "            If False, call validate_X_y(). Else, it must be a tuple of kwargs\n",
    "            to be used for calling check_array() on X and y respectively.\n",
    "            `estimator=self` is automatically added to these dicts to generate\n",
    "            more informative error message in case of invalid input data.\n",
    "        **check_params : kwargs\n",
    "            Parameters passed to :func:`sklearn.utils.check_array` or\n",
    "            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately\n",
    "            is not False.\n",
    "            `estimator=self` is automatically added to these params to generate\n",
    "            more informative error message in case of invalid input data.\n",
    "        Returns\n",
    "        -------\n",
    "        out : {ndarray, sparse matrix} or tuple of these\n",
    "            The validated input. A tuple is returned if both `X` and `y` are\n",
    "            validated.\n",
    "        \"\"\"\n",
    "        self._check_feature_names(X, reset=reset)\n",
    "\n",
    "        if y is None and self._get_tags()[\"requires_y\"]:\n",
    "            raise ValueError(\n",
    "                f\"This {self.__class__.__name__} estimator \"\n",
    "                \"requires y to be passed, but the target y is None.\"\n",
    "            )\n",
    "\n",
    "        no_val_X = isinstance(X, str) and X == \"no_validation\"\n",
    "        no_val_y = y is None or isinstance(y, str) and y == \"no_validation\"\n",
    "\n",
    "        default_check_params = {\"estimator\": self}\n",
    "        check_params = {**default_check_params, **check_params}\n",
    "\n",
    "        if no_val_X and no_val_y:\n",
    "            raise ValueError(\"Validation should be done on X, y or both.\")\n",
    "        elif not no_val_X and no_val_y:\n",
    "            X = check_array(X, input_name=\"X\", **check_params)\n",
    "            out = X\n",
    "        elif no_val_X and not no_val_y:\n",
    "            y = _check_y(y, **check_params)\n",
    "            out = y\n",
    "        else:\n",
    "            if validate_separately:\n",
    "                # We need this because some estimators validate X and y\n",
    "                # separately, and in general, separately calling check_array()\n",
    "                # on X and y isn't equivalent to just calling check_X_y()\n",
    "                # :(\n",
    "                check_X_params, check_y_params = validate_separately\n",
    "                if \"estimator\" not in check_X_params:\n",
    "                    check_X_params = {**default_check_params, **check_X_params}\n",
    "                X = check_array(X, input_name=\"X\", **check_X_params)\n",
    "                if \"estimator\" not in check_y_params:\n",
    "                    check_y_params = {**default_check_params, **check_y_params}\n",
    "                y = check_array(y, input_name=\"y\", **check_y_params)\n",
    "            else:\n",
    "                X, y = check_X_y(X, y, **check_params)\n",
    "            out = X, y\n",
    "\n",
    "        if not no_val_X and check_params.get(\"ensure_2d\", True):\n",
    "            self._check_n_features(X, reset=reset)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _validate_params(self):\n",
    "        \"\"\"Validate types and values of constructor parameters\n",
    "        The expected type and values must be defined in the `_parameter_constraints`\n",
    "        class attribute, which is a dictionary `param_name: list of constraints`. See\n",
    "        the docstring of `validate_parameter_constraints` for a description of the\n",
    "        accepted constraints.\n",
    "        \"\"\"\n",
    "        validate_parameter_constraints(\n",
    "            self._parameter_constraints,\n",
    "            self.get_params(deep=False),\n",
    "            caller_name=self.__class__.__name__,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _repr_html_(self):\n",
    "        \"\"\"HTML representation of estimator.\n",
    "        This is redundant with the logic of `_repr_mimebundle_`. The latter\n",
    "        should be favorted in the long term, `_repr_html_` is only\n",
    "        implemented for consumers who do not interpret `_repr_mimbundle_`.\n",
    "        \"\"\"\n",
    "        if get_config()[\"display\"] != \"diagram\":\n",
    "            raise AttributeError(\n",
    "                \"_repr_html_ is only defined when the \"\n",
    "                \"'display' configuration option is set to \"\n",
    "                \"'diagram'\"\n",
    "            )\n",
    "        return self._repr_html_inner\n",
    "\n",
    "    def _repr_html_inner(self):\n",
    "        \"\"\"This function is returned by the @property `_repr_html_` to make\n",
    "        `hasattr(estimator, \"_repr_html_\") return `True` or `False` depending\n",
    "        on `get_config()[\"display\"]`.\n",
    "        \"\"\"\n",
    "        return estimator_html_repr(self)\n",
    "\n",
    "    def _repr_mimebundle_(self, **kwargs):\n",
    "        \"\"\"Mime bundle used by jupyter kernels to display estimator\"\"\"\n",
    "        output = {\"text/plain\": repr(self)}\n",
    "        if get_config()[\"display\"] == \"diagram\":\n",
    "            output[\"text/html\"] = estimator_html_repr(self)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ClassifierMixin:\n",
    "    \"\"\"Mixin class for all classifiers in scikit-learn.\"\"\"\n",
    "\n",
    "    _estimator_type = \"classifier\"\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "        In multi-label classification, this is the subset accuracy\n",
    "        which is a harsh metric since you require for each sample that\n",
    "        each label set be correctly predicted.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Test samples.\n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            True labels for `X`.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights.\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics import accuracy_score\n",
    "\n",
    "        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"requires_y\": True}\n",
    "\n",
    "\n",
    "class RegressorMixin:\n",
    "    \"\"\"Mixin class for all regression estimators in scikit-learn.\"\"\"\n",
    "\n",
    "    _estimator_type = \"regressor\"\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        \"\"\"Return the coefficient of determination of the prediction.\n",
    "        The coefficient of determination :math:`R^2` is defined as\n",
    "        :math:`(1 - \\\\frac{u}{v})`, where :math:`u` is the residual\n",
    "        sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
    "        is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
    "        The best possible score is 1.0 and it can be negative (because the\n",
    "        model can be arbitrarily worse). A constant model that always predicts\n",
    "        the expected value of `y`, disregarding the input features, would get\n",
    "        a :math:`R^2` score of 0.0.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Test samples. For some estimators this may be a precomputed\n",
    "            kernel matrix or a list of generic objects instead with shape\n",
    "            ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
    "            is the number of samples used in the fitting for the estimator.\n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            True values for `X`.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights.\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
    "        Notes\n",
    "        -----\n",
    "        The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
    "        ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
    "        with default value of :func:`~sklearn.metrics.r2_score`.\n",
    "        This influences the ``score`` method of all the multioutput\n",
    "        regressors (except for\n",
    "        :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
    "        \"\"\"\n",
    "\n",
    "        from sklearn.metrics import r2_score\n",
    "\n",
    "        y_pred = self.predict(X)\n",
    "        return r2_score(y, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"requires_y\": True}\n",
    "\n",
    "\n",
    "class ClusterMixin:\n",
    "    \"\"\"Mixin class for all cluster estimators in scikit-learn.\"\"\"\n",
    "\n",
    "    _estimator_type = \"clusterer\"\n",
    "\n",
    "    def fit_predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Perform clustering on `X` and returns cluster labels.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input data.\n",
    "        y : Ignored\n",
    "            Not used, present for API consistency by convention.\n",
    "        Returns\n",
    "        -------\n",
    "        labels : ndarray of shape (n_samples,), dtype=np.int64\n",
    "            Cluster labels.\n",
    "        \"\"\"\n",
    "        # non-optimized default implementation; override when a better\n",
    "        # method is possible for a given clustering algorithm\n",
    "        self.fit(X)\n",
    "        return self.labels_\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"preserves_dtype\": []}\n",
    "\n",
    "\n",
    "class BiclusterMixin:\n",
    "    \"\"\"Mixin class for all bicluster estimators in scikit-learn.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def biclusters_(self):\n",
    "        \"\"\"Convenient way to get row and column indicators together.\n",
    "        Returns the ``rows_`` and ``columns_`` members.\n",
    "        \"\"\"\n",
    "        return self.rows_, self.columns_\n",
    "\n",
    "    def get_indices(self, i):\n",
    "        \"\"\"Row and column indices of the `i`'th bicluster.\n",
    "        Only works if ``rows_`` and ``columns_`` attributes exist.\n",
    "        Parameters\n",
    "        ----------\n",
    "        i : int\n",
    "            The index of the cluster.\n",
    "        Returns\n",
    "        -------\n",
    "        row_ind : ndarray, dtype=np.intp\n",
    "            Indices of rows in the dataset that belong to the bicluster.\n",
    "        col_ind : ndarray, dtype=np.intp\n",
    "            Indices of columns in the dataset that belong to the bicluster.\n",
    "        \"\"\"\n",
    "        rows = self.rows_[i]\n",
    "        columns = self.columns_[i]\n",
    "        return np.nonzero(rows)[0], np.nonzero(columns)[0]\n",
    "\n",
    "    def get_shape(self, i):\n",
    "        \"\"\"Shape of the `i`'th bicluster.\n",
    "        Parameters\n",
    "        ----------\n",
    "        i : int\n",
    "            The index of the cluster.\n",
    "        Returns\n",
    "        -------\n",
    "        n_rows : int\n",
    "            Number of rows in the bicluster.\n",
    "        n_cols : int\n",
    "            Number of columns in the bicluster.\n",
    "        \"\"\"\n",
    "        indices = self.get_indices(i)\n",
    "        return tuple(len(i) for i in indices)\n",
    "\n",
    "    def get_submatrix(self, i, data):\n",
    "        \"\"\"Return the submatrix corresponding to bicluster `i`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        i : int\n",
    "            The index of the cluster.\n",
    "        data : array-like of shape (n_samples, n_features)\n",
    "            The data.\n",
    "        Returns\n",
    "        -------\n",
    "        submatrix : ndarray of shape (n_rows, n_cols)\n",
    "            The submatrix corresponding to bicluster `i`.\n",
    "        Notes\n",
    "        -----\n",
    "        Works with sparse matrices. Only works if ``rows_`` and\n",
    "        ``columns_`` attributes exist.\n",
    "        \"\"\"\n",
    "        from sklearn.utils.validation import check_array\n",
    "\n",
    "        data = check_array(data, accept_sparse=\"csr\")\n",
    "        row_ind, col_ind = self.get_indices(i)\n",
    "        return data[row_ind[:, np.newaxis], col_ind]\n",
    "\n",
    "\n",
    "class TransformerMixin:\n",
    "    \"\"\"Mixin class for all transformers in scikit-learn.\"\"\"\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        Fit to data, then transform it.\n",
    "        Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
    "        and returns a transformed version of `X`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples.\n",
    "        y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \\\n",
    "                default=None\n",
    "            Target values (None for unsupervised transformations).\n",
    "        **fit_params : dict\n",
    "            Additional fit parameters.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : ndarray array of shape (n_samples, n_features_new)\n",
    "            Transformed array.\n",
    "        \"\"\"\n",
    "        # non-optimized default implementation; override when a better\n",
    "        # method is possible for a given clustering algorithm\n",
    "        if y is None:\n",
    "            # fit method of arity 1 (unsupervised transformation)\n",
    "            return self.fit(X, **fit_params).transform(X)\n",
    "        else:\n",
    "            # fit method of arity 2 (supervised transformation)\n",
    "            return self.fit(X, y, **fit_params).transform(X)\n",
    "\n",
    "\n",
    "class _OneToOneFeatureMixin:\n",
    "    \"\"\"Provides `get_feature_names_out` for simple transformers.\n",
    "    Assumes there's a 1-to-1 correspondence between input features\n",
    "    and output features.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Get output feature names for transformation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_features : array-like of str or None, default=None\n",
    "            Input features.\n",
    "            - If `input_features` is `None`, then `feature_names_in_` is\n",
    "              used as feature names in. If `feature_names_in_` is not defined,\n",
    "              then the following input feature names are generated:\n",
    "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
    "            - If `input_features` is an array-like, then `input_features` must\n",
    "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
    "        Returns\n",
    "        -------\n",
    "        feature_names_out : ndarray of str objects\n",
    "            Same as input features.\n",
    "        \"\"\"\n",
    "        return _check_feature_names_in(self, input_features)\n",
    "\n",
    "\n",
    "class _ClassNamePrefixFeaturesOutMixin:\n",
    "    \"\"\"Mixin class for transformers that generate their own names by prefixing.\n",
    "    Assumes that `_n_features_out` is defined for the estimator.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Get output feature names for transformation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_features : array-like of str or None, default=None\n",
    "            Only used to validate feature names with the names seen in :meth:`fit`.\n",
    "        Returns\n",
    "        -------\n",
    "        feature_names_out : ndarray of str objects\n",
    "            Transformed feature names.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"_n_features_out\")\n",
    "        return _generate_get_feature_names_out(\n",
    "            self, self._n_features_out, input_features=input_features\n",
    "        )\n",
    "\n",
    "\n",
    "class DensityMixin:\n",
    "    \"\"\"Mixin class for all density estimators in scikit-learn.\"\"\"\n",
    "\n",
    "    _estimator_type = \"DensityEstimator\"\n",
    "\n",
    "    def score(self, X, y=None):\n",
    "        \"\"\"Return the score of the model on the data `X`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Test samples.\n",
    "        y : Ignored\n",
    "            Not used, present for API consistency by convention.\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class OutlierMixin:\n",
    "    \"\"\"Mixin class for all outlier detection estimators in scikit-learn.\"\"\"\n",
    "\n",
    "    _estimator_type = \"outlier_detector\"\n",
    "\n",
    "    def fit_predict(self, X, y=None):\n",
    "        \"\"\"Perform fit on X and returns labels for X.\n",
    "        Returns -1 for outliers and 1 for inliers.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        y : Ignored\n",
    "            Not used, present for API consistency by convention.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            1 for inliers, -1 for outliers.\n",
    "        \"\"\"\n",
    "        # override for transductive outlier detectors like LocalOulierFactor\n",
    "        return self.fit(X).predict(X)\n",
    "\n",
    "\n",
    "class MetaEstimatorMixin:\n",
    "    _required_parameters = [\"estimator\"]\n",
    "    \"\"\"Mixin class for all meta estimators in scikit-learn.\"\"\"\n",
    "\n",
    "\n",
    "class MultiOutputMixin:\n",
    "    \"\"\"Mixin to mark estimators that support multioutput.\"\"\"\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"multioutput\": True}\n",
    "\n",
    "\n",
    "class _UnstableArchMixin:\n",
    "    \"\"\"Mark estimators that are non-determinstic on 32bit or PowerPC\"\"\"\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\n",
    "            \"non_deterministic\": (\n",
    "                _IS_32BIT or platform.machine().startswith((\"ppc\", \"powerpc\"))\n",
    "            )\n",
    "        }\n",
    "\n",
    "\n",
    "def is_classifier(estimator):\n",
    "    \"\"\"Return True if the given estimator is (probably) a classifier.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object\n",
    "        Estimator object to test.\n",
    "    Returns\n",
    "    -------\n",
    "    out : bool\n",
    "        True if estimator is a classifier and False otherwise.\n",
    "    \"\"\"\n",
    "    return getattr(estimator, \"_estimator_type\", None) == \"classifier\"\n",
    "\n",
    "\n",
    "def is_regressor(estimator):\n",
    "    \"\"\"Return True if the given estimator is (probably) a regressor.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        Estimator object to test.\n",
    "    Returns\n",
    "    -------\n",
    "    out : bool\n",
    "        True if estimator is a regressor and False otherwise.\n",
    "    \"\"\"\n",
    "    return getattr(estimator, \"_estimator_type\", None) == \"regressor\"\n",
    "\n",
    "\n",
    "def is_outlier_detector(estimator):\n",
    "    \"\"\"Return True if the given estimator is (probably) an outlier detector.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        Estimator object to test.\n",
    "    Returns\n",
    "    -------\n",
    "    out : bool\n",
    "        True if estimator is an outlier detector and False otherwise.\n",
    "    \"\"\"\n",
    "    return getattr(estimator, \"_estimator_type\", None) == \"outlier_detector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from typing import List\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "from joblib import effective_n_jobs\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.base import is_classifier, is_regressor\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import MetaEstimatorMixin\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeRegressor,\n",
    "    BaseDecisionTree,\n",
    "    DecisionTreeClassifier,\n",
    ")\n",
    "from sklearn.utils import Bunch, _print_elapsed_time, deprecated\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.metaestimators import _BaseComposition\n",
    "\n",
    "\n",
    "def _fit_single_estimator(\n",
    "    estimator, X, y, sample_weight=None, message_clsname=None, message=None\n",
    "):\n",
    "    \"\"\"Private function used to fit an estimator within a job.\"\"\"\n",
    "    if sample_weight is not None:\n",
    "        try:\n",
    "            with _print_elapsed_time(message_clsname, message):\n",
    "                estimator.fit(X, y, sample_weight=sample_weight)\n",
    "        except TypeError as exc:\n",
    "            if \"unexpected keyword argument 'sample_weight'\" in str(exc):\n",
    "                raise TypeError(\n",
    "                    \"Underlying estimator {} does not support sample weights.\".format(\n",
    "                        estimator.__class__.__name__\n",
    "                    )\n",
    "                ) from exc\n",
    "            raise\n",
    "    else:\n",
    "        with _print_elapsed_time(message_clsname, message):\n",
    "            estimator.fit(X, y)\n",
    "    return estimator\n",
    "\n",
    "\n",
    "def _set_random_states(estimator, random_state=None):\n",
    "    \"\"\"Set fixed random_state parameters for an estimator.\n",
    "    Finds all parameters ending ``random_state`` and sets them to integers\n",
    "    derived from ``random_state``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator supporting get/set_params\n",
    "        Estimator with potential randomness managed by random_state\n",
    "        parameters.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Pseudo-random number generator to control the generation of the random\n",
    "        integers. Pass an int for reproducible output across multiple function\n",
    "        calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    Notes\n",
    "    -----\n",
    "    This does not necessarily set *all* ``random_state`` attributes that\n",
    "    control an estimator's randomness, only those accessible through\n",
    "    ``estimator.get_params()``.  ``random_state``s not controlled include\n",
    "    those belonging to:\n",
    "        * cross-validation splitters\n",
    "        * ``scipy.stats`` rvs\n",
    "    \"\"\"\n",
    "    random_state = check_random_state(random_state)\n",
    "    to_set = {}\n",
    "    for key in sorted(estimator.get_params(deep=True)):\n",
    "        if key == \"random_state\" or key.endswith(\"__random_state\"):\n",
    "            to_set[key] = random_state.randint(np.iinfo(np.int32).max)\n",
    "\n",
    "    if to_set:\n",
    "        estimator.set_params(**to_set)\n",
    "\n",
    "\n",
    "class BaseEnsemble(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):\n",
    "    \"\"\"Base class for all ensemble classes.\n",
    "    Warning: This class should not be used directly. Use derived classes\n",
    "    instead.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object\n",
    "        The base estimator from which the ensemble is built.\n",
    "    n_estimators : int, default=10\n",
    "        The number of estimators in the ensemble.\n",
    "    estimator_params : list of str, default=tuple()\n",
    "        The list of attributes to use as parameters when instantiating a\n",
    "        new base estimator. If none are given, default parameters are used.\n",
    "    base_estimator : object, default=\"deprecated\"\n",
    "        Use `estimator` instead.\n",
    "        .. deprecated:: 1.2\n",
    "            `base_estimator` is deprecated and will be removed in 1.4.\n",
    "            Use `estimator` instead.\n",
    "    Attributes\n",
    "    ----------\n",
    "    estimator_ : estimator\n",
    "        The base estimator from which the ensemble is grown.\n",
    "    base_estimator_ : estimator\n",
    "        The base estimator from which the ensemble is grown.\n",
    "        .. deprecated:: 1.2\n",
    "            `base_estimator_` is deprecated and will be removed in 1.4.\n",
    "            Use `estimator_` instead.\n",
    "    estimators_ : list of estimators\n",
    "        The collection of fitted base estimators.\n",
    "    \"\"\"\n",
    "\n",
    "    # overwrite _required_parameters from MetaEstimatorMixin\n",
    "    _required_parameters: List[str] = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator=None,\n",
    "        *,\n",
    "        n_estimators=10,\n",
    "        estimator_params=tuple(),\n",
    "        base_estimator=\"deprecated\",\n",
    "    ):\n",
    "        # Set parameters\n",
    "        self.estimator = estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimator_params = estimator_params\n",
    "        self.base_estimator = base_estimator\n",
    "\n",
    "        # Don't instantiate estimators now! Parameters of base_estimator might\n",
    "        # still change. Eg., when grid-searching with the nested object syntax.\n",
    "        # self.estimators_ needs to be filled by the derived classes in fit.\n",
    "\n",
    "    def _validate_estimator(self, default=None):\n",
    "        \"\"\"Check the base estimator.\n",
    "        Sets the `estimator_` attributes.\n",
    "        \"\"\"\n",
    "        if self.estimator is not None and (\n",
    "            self.base_estimator not in [None, \"deprecated\"]\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Both `estimator` and `base_estimator` were set. Only set `estimator`.\"\n",
    "            )\n",
    "\n",
    "        if self.estimator is not None:\n",
    "            self._estimator = self.estimator\n",
    "        elif self.base_estimator not in [None, \"deprecated\"]:\n",
    "            warnings.warn(\n",
    "                \"`base_estimator` was renamed to `estimator` in version 1.2 and \"\n",
    "                \"will be removed in 1.4.\",\n",
    "                FutureWarning,\n",
    "            )\n",
    "            self._estimator = self.base_estimator\n",
    "        else:\n",
    "            self._estimator = default\n",
    "\n",
    "    # TODO(1.4): remove\n",
    "    # mypy error: Decorated property not supported\n",
    "    @deprecated(  # type: ignore\n",
    "        \"Attribute `base_estimator_` was deprecated in version 1.2 and will be removed \"\n",
    "        \"in 1.4. Use `estimator_` instead.\"\n",
    "    )\n",
    "    @property\n",
    "    def base_estimator_(self):\n",
    "        \"\"\"Estimator used to grow the ensemble.\"\"\"\n",
    "        return self._estimator\n",
    "\n",
    "    # TODO(1.4): remove\n",
    "    @property\n",
    "    def estimator_(self):\n",
    "        \"\"\"Estimator used to grow the ensemble.\"\"\"\n",
    "        return self._estimator\n",
    "\n",
    "    def _make_estimator(self, append=True, random_state=None):\n",
    "        \"\"\"Make and configure a copy of the `estimator_` attribute.\n",
    "        Warning: This method should be used to properly instantiate new\n",
    "        sub-estimators.\n",
    "        \"\"\"\n",
    "        estimator = clone(self.estimator_)\n",
    "        estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})\n",
    "\n",
    "        # TODO(1.3): Remove\n",
    "        # max_features = 'auto' would cause warnings in every call to\n",
    "        # Tree.fit(..)\n",
    "        if isinstance(estimator, BaseDecisionTree):\n",
    "            if getattr(estimator, \"max_features\", None) == \"auto\":\n",
    "                if isinstance(estimator, DecisionTreeClassifier):\n",
    "                    estimator.set_params(max_features=\"sqrt\")\n",
    "                elif isinstance(estimator, DecisionTreeRegressor):\n",
    "                    estimator.set_params(max_features=1.0)\n",
    "\n",
    "        if random_state is not None:\n",
    "            _set_random_states(estimator, random_state)\n",
    "\n",
    "        if append:\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "        return estimator\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of estimators in the ensemble.\"\"\"\n",
    "        return len(self.estimators_)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return the index'th estimator in the ensemble.\"\"\"\n",
    "        return self.estimators_[index]\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Return iterator over estimators in the ensemble.\"\"\"\n",
    "        return iter(self.estimators_)\n",
    "\n",
    "\n",
    "def _partition_estimators(n_estimators, n_jobs):\n",
    "    \"\"\"Private function used to partition estimators between jobs.\"\"\"\n",
    "    # Compute the number of jobs\n",
    "    n_jobs = min(effective_n_jobs(n_jobs), n_estimators)\n",
    "\n",
    "    # Partition estimators between jobs\n",
    "    n_estimators_per_job = np.full(n_jobs, n_estimators // n_jobs, dtype=int)\n",
    "    n_estimators_per_job[: n_estimators % n_jobs] += 1\n",
    "    starts = np.cumsum(n_estimators_per_job)\n",
    "\n",
    "    return n_jobs, n_estimators_per_job.tolist(), [0] + starts.tolist()\n",
    "\n",
    "\n",
    "class _BaseHeterogeneousEnsemble(\n",
    "    MetaEstimatorMixin, _BaseComposition, metaclass=ABCMeta\n",
    "):\n",
    "    \"\"\"Base class for heterogeneous ensemble of learners.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimators : list of (str, estimator) tuples\n",
    "        The ensemble of estimators to use in the ensemble. Each element of the\n",
    "        list is defined as a tuple of string (i.e. name of the estimator) and\n",
    "        an estimator instance. An estimator can be set to `'drop'` using\n",
    "        `set_params`.\n",
    "    Attributes\n",
    "    ----------\n",
    "    estimators_ : list of estimators\n",
    "        The elements of the estimators parameter, having been fitted on the\n",
    "        training data. If an estimator has been set to `'drop'`, it will not\n",
    "        appear in `estimators_`.\n",
    "    \"\"\"\n",
    "\n",
    "    _required_parameters = [\"estimators\"]\n",
    "\n",
    "    @property\n",
    "    def named_estimators(self):\n",
    "        \"\"\"Dictionary to access any fitted sub-estimators by name.\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`~sklearn.utils.Bunch`\n",
    "        \"\"\"\n",
    "        return Bunch(**dict(self.estimators))\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, estimators):\n",
    "        self.estimators = estimators\n",
    "\n",
    "    def _validate_estimators(self):\n",
    "        if len(self.estimators) == 0:\n",
    "            raise ValueError(\n",
    "                \"Invalid 'estimators' attribute, 'estimators' should be a \"\n",
    "                \"non-empty list of (string, estimator) tuples.\"\n",
    "            )\n",
    "        names, estimators = zip(*self.estimators)\n",
    "        # defined by MetaEstimatorMixin\n",
    "        self._validate_names(names)\n",
    "\n",
    "        has_estimator = any(est != \"drop\" for est in estimators)\n",
    "        if not has_estimator:\n",
    "            raise ValueError(\n",
    "                \"All estimators are dropped. At least one is required \"\n",
    "                \"to be an estimator.\"\n",
    "            )\n",
    "\n",
    "        is_estimator_type = is_classifier if is_classifier(self) else is_regressor\n",
    "\n",
    "        for est in estimators:\n",
    "            if est != \"drop\" and not is_estimator_type(est):\n",
    "                raise ValueError(\n",
    "                    \"The estimator {} should be a {}.\".format(\n",
    "                        est.__class__.__name__, is_estimator_type.__name__[3:]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return names, estimators\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"\n",
    "        Set the parameters of an estimator from the ensemble.\n",
    "        Valid parameter keys can be listed with `get_params()`. Note that you\n",
    "        can directly set the parameters of the estimators contained in\n",
    "        `estimators`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        **params : keyword arguments\n",
    "            Specific parameters using e.g.\n",
    "            `set_params(parameter_name=new_value)`. In addition, to setting the\n",
    "            parameters of the estimator, the individual estimator of the\n",
    "            estimators can also be set, or can be removed by setting them to\n",
    "            'drop'.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Estimator instance.\n",
    "        \"\"\"\n",
    "        super()._set_params(\"estimators\", **params)\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"\n",
    "        Get the parameters of an estimator from the ensemble.\n",
    "        Returns the parameters given in the constructor as well as the\n",
    "        estimators contained within the `estimators` parameter.\n",
    "        Parameters\n",
    "        ----------\n",
    "        deep : bool, default=True\n",
    "            Setting it to True gets the various estimators and the parameters\n",
    "            of the estimators as well.\n",
    "        Returns\n",
    "        -------\n",
    "        params : dict\n",
    "            Parameter and estimator names mapped to their values or parameter\n",
    "            names mapped to their values.\n",
    "        \"\"\"\n",
    "        return super()._get_params(\"estimators\", deep=deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Authors: Andreas Mueller\n",
    "#          Manoj Kumar\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def compute_class_weight(class_weight, *, classes, y):\n",
    "    \"\"\"Estimate class weights for unbalanced datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    class_weight : dict, 'balanced' or None\n",
    "        If 'balanced', class weights will be given by\n",
    "        ``n_samples / (n_classes * np.bincount(y))``.\n",
    "        If a dictionary is given, keys are classes and values\n",
    "        are corresponding class weights.\n",
    "        If None is given, the class weights will be uniform.\n",
    "\n",
    "    classes : ndarray\n",
    "        Array of the classes occurring in the data, as given by\n",
    "        ``np.unique(y_org)`` with ``y_org`` the original class labels.\n",
    "\n",
    "    y : array-like of shape (n_samples,)\n",
    "        Array of original class labels per sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    class_weight_vect : ndarray of shape (n_classes,)\n",
    "        Array with class_weight_vect[i] the weight for i-th class.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    The \"balanced\" heuristic is inspired by\n",
    "    Logistic Regression in Rare Events Data, King, Zen, 2001.\n",
    "    \"\"\"\n",
    "    # Import error caused by circular imports.\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    if set(y) - set(classes):\n",
    "        raise ValueError(\"classes should include all valid labels that can be in y\")\n",
    "    if class_weight is None or len(class_weight) == 0:\n",
    "        # uniform class weights\n",
    "        weight = np.ones(classes.shape[0], dtype=np.float64, order=\"C\")\n",
    "    elif class_weight == \"balanced\":\n",
    "        # Find the weight of each class as present in y.\n",
    "        le = LabelEncoder()\n",
    "        y_ind = le.fit_transform(y)\n",
    "        if not all(np.in1d(classes, le.classes_)):\n",
    "            raise ValueError(\"classes should have valid labels that are in y\")\n",
    "\n",
    "        recip_freq = len(y) / (len(le.classes_) * np.bincount(y_ind).astype(np.float64))\n",
    "        weight = recip_freq[le.transform(classes)]\n",
    "    else:\n",
    "        # user-defined dictionary\n",
    "        weight = np.ones(classes.shape[0], dtype=np.float64, order=\"C\")\n",
    "        if not isinstance(class_weight, dict):\n",
    "            raise ValueError(\n",
    "                \"class_weight must be dict, 'balanced', or None, got: %r\" % class_weight\n",
    "            )\n",
    "        unweighted_classes = []\n",
    "        for i, c in enumerate(classes):\n",
    "            if c in class_weight:\n",
    "                weight[i] = class_weight[c]\n",
    "            else:\n",
    "                unweighted_classes.append(c)\n",
    "\n",
    "        n_weighted_classes = len(classes) - len(unweighted_classes)\n",
    "        if unweighted_classes and n_weighted_classes != len(class_weight):\n",
    "            raise ValueError(\n",
    "                f\"The classes, {unweighted_classes}, are not in class_weight\"\n",
    "            )\n",
    "\n",
    "    return weight\n",
    "\n",
    "\n",
    "def compute_sample_weight(class_weight, y, *, indices=None):\n",
    "    \"\"\"Estimate sample weights by class for unbalanced datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    class_weight : dict, list of dicts, \"balanced\", or None\n",
    "        Weights associated with classes in the form ``{class_label: weight}``.\n",
    "        If not given, all classes are supposed to have weight one. For\n",
    "        multi-output problems, a list of dicts can be provided in the same\n",
    "        order as the columns of y.\n",
    "\n",
    "        Note that for multioutput (including multilabel) weights should be\n",
    "        defined for each class of every column in its own dict. For example,\n",
    "        for four-class multilabel classification weights should be\n",
    "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
    "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
    "\n",
    "        The \"balanced\" mode uses the values of y to automatically adjust\n",
    "        weights inversely proportional to class frequencies in the input data:\n",
    "        ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "        For multi-output, the weights of each column of y will be multiplied.\n",
    "\n",
    "    y : {array-like, sparse matrix} of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Array of original class labels per sample.\n",
    "\n",
    "    indices : array-like of shape (n_subsample,), default=None\n",
    "        Array of indices to be used in a subsample. Can be of length less than\n",
    "        n_samples in the case of a subsample, or equal to n_samples in the\n",
    "        case of a bootstrap subsample with repeated indices. If None, the\n",
    "        sample weight will be calculated over the full sample. Only \"balanced\"\n",
    "        is supported for class_weight if this is provided.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sample_weight_vect : ndarray of shape (n_samples,)\n",
    "        Array with sample weights as applied to the original y.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure y is 2D. Sparse matrices are already 2D.\n",
    "    if not sparse.issparse(y):\n",
    "        y = np.atleast_1d(y)\n",
    "        if y.ndim == 1:\n",
    "            y = np.reshape(y, (-1, 1))\n",
    "    n_outputs = y.shape[1]\n",
    "\n",
    "    if isinstance(class_weight, str):\n",
    "        if class_weight not in [\"balanced\"]:\n",
    "            raise ValueError(\n",
    "                'The only valid preset for class_weight is \"balanced\". Given \"%s\".'\n",
    "                % class_weight\n",
    "            )\n",
    "    elif indices is not None and not isinstance(class_weight, str):\n",
    "        raise ValueError(\n",
    "            'The only valid class_weight for subsampling is \"balanced\". Given \"%s\".'\n",
    "            % class_weight\n",
    "        )\n",
    "    elif n_outputs > 1:\n",
    "        if not hasattr(class_weight, \"__iter__\") or isinstance(class_weight, dict):\n",
    "            raise ValueError(\n",
    "                \"For multi-output, class_weight should be a \"\n",
    "                \"list of dicts, or a valid string.\"\n",
    "            )\n",
    "        if len(class_weight) != n_outputs:\n",
    "            raise ValueError(\n",
    "                \"For multi-output, number of elements in \"\n",
    "                \"class_weight should match number of outputs.\"\n",
    "            )\n",
    "\n",
    "    expanded_class_weight = []\n",
    "    for k in range(n_outputs):\n",
    "\n",
    "        y_full = y[:, k]\n",
    "        if sparse.issparse(y_full):\n",
    "            # Ok to densify a single column at a time\n",
    "            y_full = y_full.toarray().flatten()\n",
    "        classes_full = np.unique(y_full)\n",
    "        classes_missing = None\n",
    "\n",
    "        if class_weight == \"balanced\" or n_outputs == 1:\n",
    "            class_weight_k = class_weight\n",
    "        else:\n",
    "            class_weight_k = class_weight[k]\n",
    "\n",
    "        if indices is not None:\n",
    "            # Get class weights for the subsample, covering all classes in\n",
    "            # case some labels that were present in the original data are\n",
    "            # missing from the sample.\n",
    "            y_subsample = y_full[indices]\n",
    "            classes_subsample = np.unique(y_subsample)\n",
    "\n",
    "            weight_k = np.take(\n",
    "                compute_class_weight(\n",
    "                    class_weight_k, classes=classes_subsample, y=y_subsample\n",
    "                ),\n",
    "                np.searchsorted(classes_subsample, classes_full),\n",
    "                mode=\"clip\",\n",
    "            )\n",
    "\n",
    "            classes_missing = set(classes_full) - set(classes_subsample)\n",
    "        else:\n",
    "            weight_k = compute_class_weight(\n",
    "                class_weight_k, classes=classes_full, y=y_full\n",
    "            )\n",
    "\n",
    "        weight_k = weight_k[np.searchsorted(classes_full, y_full)]\n",
    "\n",
    "        if classes_missing:\n",
    "            # Make missing classes' weight zero\n",
    "            weight_k[np.in1d(y_full, list(classes_missing))] = 0.0\n",
    "\n",
    "        expanded_class_weight.append(weight_k)\n",
    "\n",
    "    expanded_class_weight = np.prod(expanded_class_weight, axis=0, dtype=np.float64)\n",
    "\n",
    "    return expanded_class_weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "075024aca70acbaef7a590c66e41b716ad6737fc45064b7ec5fe8fdf30d044e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
