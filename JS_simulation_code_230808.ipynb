{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSK2022/RandomForest/blob/main/JS_simulation_code_230808.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GThv_VDEEWx0"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-survival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcoUGQnCutgO"
      },
      "outputs": [],
      "source": [
        "from math import ceil\n",
        "from numbers import Integral, Real\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse import issparse\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.tree import _tree\n",
        "from sklearn.tree._classes import DENSE_SPLITTERS, SPARSE_SPLITTERS\n",
        "from sklearn.tree._splitter import Splitter\n",
        "from sklearn.tree._tree import BestFirstTreeBuilder, DepthFirstTreeBuilder, Tree\n",
        "from sklearn.utils._param_validation import Interval, StrOptions\n",
        "from sklearn.utils.validation import check_is_fitted, check_random_state\n",
        "\n",
        "import sksurv\n",
        "from sksurv.base import SurvivalAnalysisMixin\n",
        "from sksurv.functions import StepFunction\n",
        "from sksurv.util import check_array_survival\n",
        "from sksurv.tree._criterion import LogrankCriterion, get_unique_times\n",
        "\n",
        "__all__ = [\"SurvivalTree\"]\n",
        "\n",
        "DTYPE = _tree.DTYPE\n",
        "\n",
        "\n",
        "def _array_to_step_function(x, array):\n",
        "    n_samples = array.shape[0]\n",
        "    funcs = np.empty(n_samples, dtype=np.object_)\n",
        "    for i in range(n_samples):\n",
        "        funcs[i] = StepFunction(x=x, y=array[i])\n",
        "    return funcs\n",
        "\n",
        "\n",
        "class SurvivalTree(BaseEstimator, SurvivalAnalysisMixin):\n",
        "\n",
        "    _parameter_constraints = {\n",
        "        \"splitter\": [StrOptions({\"best\", \"random\"})],\n",
        "        \"max_depth\": [Interval(Integral, 1, None, closed=\"left\"), None],\n",
        "        \"min_samples_split\": [\n",
        "            Interval(Integral, 2, None, closed=\"left\"),\n",
        "            Interval(Real, 0.0, 1.0, closed=\"neither\"),\n",
        "        ],\n",
        "        \"min_samples_leaf\": [\n",
        "            Interval(Integral, 1, None, closed=\"left\"),\n",
        "            Interval(Real, 0.0, 0.5, closed=\"right\"),\n",
        "        ],\n",
        "        \"min_weight_fraction_leaf\": [Interval(Real, 0.0, 0.5, closed=\"both\")],\n",
        "        \"max_features\": [\n",
        "            Interval(Integral, 1, None, closed=\"left\"),\n",
        "            Interval(Real, 0.0, 1.0, closed=\"right\"),\n",
        "            StrOptions({\"auto\", \"sqrt\", \"log2\"}, deprecated={\"auto\"}),\n",
        "            None,\n",
        "        ],\n",
        "        \"random_state\": [\"random_state\"],\n",
        "        \"max_leaf_nodes\": [Interval(Integral, 2, None, closed=\"left\"), None],\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        splitter=\"best\",\n",
        "        max_depth=None,\n",
        "        min_samples_split=6,\n",
        "        min_samples_leaf=3,\n",
        "        min_weight_fraction_leaf=0.0,\n",
        "        max_features=None,\n",
        "        random_state=None,\n",
        "        max_leaf_nodes=None,\n",
        "    ):\n",
        "        self.splitter = splitter\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.max_leaf_nodes = max_leaf_nodes\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None, check_input=True):\n",
        "\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if check_input:\n",
        "            X = self._validate_data(X, ensure_min_samples=2, accept_sparse=\"csc\")\n",
        "            event, time = check_array_survival(X, y)\n",
        "            time = time.astype(np.float64)\n",
        "            self.unique_times_, self.is_event_time_ = get_unique_times(time, event)\n",
        "            if issparse(X):\n",
        "                X.sort_indices()\n",
        "\n",
        "            y_numeric = np.empty((X.shape[0], 2), dtype=np.float64)\n",
        "            y_numeric[:, 0] = time\n",
        "            y_numeric[:, 1] = event.astype(np.float64)\n",
        "        else:\n",
        "            y_numeric, self.unique_times_, self.is_event_time_ = y\n",
        "\n",
        "        n_samples, self.n_features_in_ = X.shape\n",
        "        params = self._check_params(n_samples)\n",
        "\n",
        "        self.n_outputs_ = self.unique_times_.shape[0]\n",
        "        # one \"class\" for CHF, one for survival function\n",
        "        self.n_classes_ = np.ones(self.n_outputs_, dtype=np.intp) * 2\n",
        "\n",
        "        # Build tree\n",
        "        self.criterion = \"logrank\"\n",
        "        criterion = LogrankCriterion(self.n_outputs_, n_samples, self.unique_times_)\n",
        "\n",
        "        SPLITTERS = SPARSE_SPLITTERS if issparse(X) else DENSE_SPLITTERS\n",
        "\n",
        "        splitter = self.splitter\n",
        "        if not isinstance(self.splitter, Splitter):\n",
        "            splitter = SPLITTERS[self.splitter](\n",
        "                criterion, self.max_features_, params[\"min_samples_leaf\"], params[\"min_weight_leaf\"], random_state\n",
        "            )\n",
        "\n",
        "        self.tree_ = Tree(self.n_features_in_, self.n_classes_, self.n_outputs_)\n",
        "\n",
        "        # Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise\n",
        "        if params[\"max_leaf_nodes\"] < 0:\n",
        "            builder = DepthFirstTreeBuilder(\n",
        "                splitter,\n",
        "                params[\"min_samples_split\"],\n",
        "                params[\"min_samples_leaf\"],\n",
        "                params[\"min_weight_leaf\"],\n",
        "                params[\"max_depth\"],\n",
        "                0.0,  # min_impurity_decrease\n",
        "            )\n",
        "        else:\n",
        "            builder = BestFirstTreeBuilder(\n",
        "                splitter,\n",
        "                params[\"min_samples_split\"],\n",
        "                params[\"min_samples_leaf\"],\n",
        "                params[\"min_weight_leaf\"],\n",
        "                params[\"max_depth\"],\n",
        "                params[\"max_leaf_nodes\"],\n",
        "                0.0,  # min_impurity_decrease\n",
        "            )\n",
        "\n",
        "        builder.build(self.tree_, X, y_numeric, sample_weight)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _check_params(self, n_samples):\n",
        "        self._validate_params()\n",
        "\n",
        "        # Check parameters\n",
        "        max_depth = (2**31) - 1 if self.max_depth is None else self.max_depth\n",
        "\n",
        "        max_leaf_nodes = -1 if self.max_leaf_nodes is None else self.max_leaf_nodes\n",
        "\n",
        "        if isinstance(self.min_samples_leaf, (Integral, np.integer)):\n",
        "            min_samples_leaf = self.min_samples_leaf\n",
        "        else:  # float\n",
        "            min_samples_leaf = int(ceil(self.min_samples_leaf * n_samples))\n",
        "\n",
        "        if isinstance(self.min_samples_split, Integral):\n",
        "            min_samples_split = self.min_samples_split\n",
        "        else:  # float\n",
        "            min_samples_split = int(ceil(self.min_samples_split * n_samples))\n",
        "            min_samples_split = max(2, min_samples_split)\n",
        "\n",
        "        min_samples_split = max(min_samples_split, 2 * min_samples_leaf)\n",
        "\n",
        "        self._check_max_features()\n",
        "\n",
        "        if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n",
        "            raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n",
        "\n",
        "        min_weight_leaf = self.min_weight_fraction_leaf * n_samples\n",
        "\n",
        "        return {\n",
        "            \"max_depth\": max_depth,\n",
        "            \"max_leaf_nodes\": max_leaf_nodes,\n",
        "            \"min_samples_leaf\": min_samples_leaf,\n",
        "            \"min_samples_split\": min_samples_split,\n",
        "            \"min_weight_leaf\": min_weight_leaf,\n",
        "        }\n",
        "\n",
        "    def _check_max_features(self):\n",
        "        if isinstance(self.max_features, str):\n",
        "            if self.max_features in (\"auto\", \"sqrt\"):\n",
        "                max_features = max(1, int(np.sqrt(self.n_features_in_)))\n",
        "            elif self.max_features == \"log2\":\n",
        "                max_features = max(1, int(np.log2(self.n_features_in_)))\n",
        "\n",
        "        elif self.max_features is None:\n",
        "            max_features = self.n_features_in_\n",
        "        elif isinstance(self.max_features, (Integral, np.integer)):\n",
        "            max_features = self.max_features\n",
        "        else:  # float\n",
        "            if self.max_features > 0.0:\n",
        "                max_features = max(1, int(self.max_features * self.n_features_in_))\n",
        "            else:\n",
        "                max_features = 0\n",
        "\n",
        "        if not 0 < max_features <= self.n_features_in_:\n",
        "            raise ValueError(\"max_features must be in (0, n_features]\")\n",
        "\n",
        "        self.max_features_ = max_features\n",
        "\n",
        "    def _validate_X_predict(self, X, check_input, accept_sparse=\"csr\"):\n",
        "        \"\"\"Validate X whenever one tries to predict\"\"\"\n",
        "        if check_input:\n",
        "            X = self._validate_data(X, dtype=DTYPE, accept_sparse=accept_sparse, reset=False)\n",
        "        else:\n",
        "            # The number of features is checked regardless of `check_input`\n",
        "            self._check_n_features(X, reset=False)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def predict(self, X, check_input=True):\n",
        "\n",
        "        chf = self.predict_cumulative_hazard_function(X, check_input, return_array=True)\n",
        "        return chf[:, self.is_event_time_].sum(1)\n",
        "\n",
        "    def predict_cumulative_hazard_function(self, X, check_input=True, return_array=False):\n",
        "\n",
        "        check_is_fitted(self, \"tree_\")\n",
        "        X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n",
        "\n",
        "        pred = self.tree_.predict(X)\n",
        "        arr = pred[..., 0]\n",
        "        if return_array:\n",
        "            return arr\n",
        "        return _array_to_step_function(self.unique_times_, arr)\n",
        "\n",
        "    def predict_survival_function(self, X, check_input=True, return_array=False):\n",
        "\n",
        "        check_is_fitted(self, \"tree_\")\n",
        "        X = self._validate_X_predict(X, check_input, accept_sparse=\"csr\")\n",
        "\n",
        "        pred = self.tree_.predict(X)\n",
        "        arr = pred[..., 1]\n",
        "        if return_array:\n",
        "            return arr\n",
        "        return _array_to_step_function(self.unique_times_, arr)\n",
        "\n",
        "    def apply(self, X, check_input=True):\n",
        "\n",
        "        check_is_fitted(self, \"tree_\")\n",
        "        self._validate_X_predict(X, check_input)\n",
        "        return self.tree_.apply(X)\n",
        "\n",
        "    def decision_path(self, X, check_input=True):\n",
        "\n",
        "        X = self._validate_X_predict(X, check_input)\n",
        "        return self.tree_.decision_path(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taOa-fgqu4U-"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "def get_unique_times_by_id(ids, time_start, time_stop, event):\n",
        "    unique_times_by_id = defaultdict(list)\n",
        "    has_event_by_id = defaultdict(list)\n",
        "\n",
        "    for id_, t_start, t_stop, e in zip(ids, time_start, time_stop, event):\n",
        "        if not unique_times_by_id[id_] or t_stop != unique_times_by_id[id_][-1]:\n",
        "            unique_times_by_id[id_].append(t_stop)\n",
        "            has_event_by_id[id_].append(e)\n",
        "        elif e == 1:\n",
        "            has_event_by_id[id_][-1] = True\n",
        "\n",
        "    for id_ in unique_times_by_id.keys():\n",
        "        unique_times_by_id[id_] = np.asarray(unique_times_by_id[id_])\n",
        "        has_event_by_id[id_] = np.asarray(has_event_by_id[id_], dtype=np.bool_)\n",
        "\n",
        "    return unique_times_by_id, has_event_by_id\n",
        "\n",
        "class RisksetCounter:\n",
        "    def __init__(self, ids, time_start, time_stop, event):\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "\n",
        "        self.all_unique_times = np.unique(time_stop)\n",
        "        self.n_unique_times = len(self.all_unique_times)\n",
        "\n",
        "        self.n_at_risk = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.n_events = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "\n",
        "        self.unique_times_by_id, self.has_event_by_id = get_unique_times_by_id(ids, time_start, time_stop, event)\n",
        "        self.set_data()\n",
        "\n",
        "    def reset(self):\n",
        "        self.n_at_risk.fill(0)\n",
        "        self.n_events.fill(0)\n",
        "        self.set_data()\n",
        "\n",
        "    def set_data(self):\n",
        "        for id_, times in self.unique_times_by_id.items():\n",
        "            events = self.has_event_by_id[id_]\n",
        "            for t, e in zip(times, events):\n",
        "                idx = np.searchsorted(self.all_unique_times, t)\n",
        "                self.n_at_risk[idx:] += 1\n",
        "                self.n_events[idx] += e\n",
        "\n",
        "    def update(self, ids, time_start, time_stop, event):\n",
        "        new_unique_times_by_id, new_has_event_by_id = get_unique_times_by_id(ids, time_start, time_stop, event)\n",
        "\n",
        "        for id_, times in new_unique_times_by_id.items():\n",
        "            events = new_has_event_by_id[id_]\n",
        "            for t, e in zip(times, events):\n",
        "                idx = np.searchsorted(self.all_unique_times, t)\n",
        "                self.n_at_risk[idx:] -= 1\n",
        "                self.n_events[idx] -= e\n",
        "\n",
        "# Now, let's proceed with the updated class and demonstration\n",
        "class RisksetCounterUpdated(RisksetCounter):\n",
        "    def __init__(self, ids, time_start, time_stop, event):\n",
        "        super().__init__(ids, time_start, time_stop, event)\n",
        "\n",
        "    def at_id_time(self, id_, t_idx):\n",
        "        at_risk = 0\n",
        "        events = 0\n",
        "        return at_risk, events\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GH0_TXEu4U_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "ids = np.array([\n",
        "    1, 2, 3, 4, 5, 5, 6, 7, 8, 8, 9, 9, 9, 10, 11, 11, 11, 12, 12, 12\n",
        "])\n",
        "\n",
        "data = np.array([\n",
        "    [0, 1, 0], [0, 4, 0], [0, 7, 0], [0, 10, 0], [0, 6, 1], [6, 10, 0], [0, 14, 0], [0, 18, 0], [0, 5, 1],\n",
        "    [5, 18, 0], [0, 12, 1], [12, 16, 1], [16, 18, 0], [0, 23, 0], [0, 10, 1], [10, 15, 1], [15, 23, 0],\n",
        "    [0, 3, 1], [3, 16, 1], [16, 23, 1]\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dummy = np.array([1, 1, 2, 2])\n",
        "time_start_dummy = np.array([0, 1, 0, 2])\n",
        "time_stop_dummy = np.array([1, 2, 2, 3])\n",
        "event_dummy = np.array([0, 1, 0, 1])\n",
        "\n",
        "# Proceeding with the demonstration using the dummy data\n",
        "riskset_counter = RisksetCounterUpdated(ids_dummy, time_start_dummy, time_stop_dummy, event_dummy)\n",
        "unique_times_dummy = np.unique(time_stop_dummy)\n",
        "\n",
        "results = []\n",
        "for id_ in np.unique(ids_dummy):\n",
        "    for t_idx, t in enumerate(unique_times_dummy):\n",
        "        at_risk, events = riskset_counter.at_id_time(id_, t_idx)\n",
        "        results.append((id_, t, at_risk, events))\n",
        "results"
      ],
      "metadata": {
        "id": "liJ0OI9PwB5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Uaj3w2u4VA"
      },
      "source": [
        "주요 변경 사항:\n",
        "\n",
        "1. 데이터셋은 ID와 시간으로 정렬되어 있다고 가정하였습니다.\n",
        "2. 이전 ID 값을 추적하여 현재 ID가 변경되면 위험 집합을 업데이트하는 시간을 재설정합니다. 이렇게 하면 동일한 ID에 대해 동일한 시간에 여러 번 위험 집합을 업데이트하지 않습니다.\n",
        "3. 이벤트 발생 여부와 관계없이 위험 집합을 업데이트하며, 이벤트가 발생한 경우에만 이벤트 카운터를 증가시킵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AEBAF1au4VD"
      },
      "outputs": [],
      "source": [
        "def argbinsearch(arr, key_val):\n",
        "    arr_len = len(arr)\n",
        "    min_idx = 0\n",
        "    max_idx = arr_len\n",
        "\n",
        "    while min_idx < max_idx:\n",
        "        mid_idx = min_idx + ((max_idx - min_idx) // 2)\n",
        "\n",
        "        if mid_idx < 0 or mid_idx >= arr_len:\n",
        "            return -1\n",
        "\n",
        "        mid_val = arr[mid_idx]\n",
        "        if mid_val <= key_val:  # Change the condition to <=\n",
        "            min_idx = mid_idx + 1\n",
        "        else:\n",
        "            max_idx = mid_idx\n",
        "\n",
        "    return min_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9gICigNu4VE"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import check_random_state\n",
        "\n",
        "class PseudoScoreCriterion:\n",
        "    def __init__(self, n_outputs, n_samples, unique_times, x, ids, time_start, time_stop, event, random_state=None):\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_samples = n_samples\n",
        "        self.n_unique_times = len(unique_times)\n",
        "\n",
        "        self.x = x\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "        self.unique_times = unique_times\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "        # For interval censoring\n",
        "        self.n_node_samples = 0\n",
        "        self.weighted_n_node_samples = 0.0\n",
        "        self.weighted_n_left = 0.0\n",
        "        self.weighted_n_right = 0.0\n",
        "\n",
        "        # Initialize the risk set counter without unique_times\n",
        "        self.riskset_total = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        self.delta_n_at_risk_left = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.n_events_left = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.delta_n_at_risk_right = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.n_events_right = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "\n",
        "        # For reset\n",
        "        self.start = 0\n",
        "        self.pos = 0\n",
        "        self.end = 0\n",
        "        self.samples = None  # Initialize the samples attribute\n",
        "\n",
        "        # For children impurity computation\n",
        "        self.left_impurity = np.empty(self.n_unique_times, dtype=np.float64)\n",
        "        self.right_impurity = np.empty(self.n_unique_times, dtype=np.float64)\n",
        "\n",
        "        # To store the unique time index for each sample\n",
        "        self.samples_time_idx = np.zeros(n_samples, dtype=np.int64)\n",
        "        for i in range(n_samples):\n",
        "            self.samples_time_idx[i] = np.searchsorted(unique_times, time_stop[i])\n",
        "\n",
        "    def init(self, y, sample_weight, n_samples, samples, start, end):\n",
        "        self.n_node_samples = end - start\n",
        "        self.weighted_n_node_samples = 0.0\n",
        "\n",
        "        start_times = y[:, 0]\n",
        "        stop_times = y[:, 1]\n",
        "        event = y[:, 2]\n",
        "        self.samples = samples  # Storing the samples for this node\n",
        "\n",
        "        for idx in samples[start:end]:\n",
        "            self.riskset_total.update([self.ids[idx]], [start_times[idx]], [stop_times[idx]], [event[idx]])\n",
        "            w = sample_weight[idx] if sample_weight is not None else 1.0\n",
        "            self.weighted_n_node_samples += w\n",
        "\n",
        "    def update(self, new_pos, split_feature, split_threshold):\n",
        "        pos = self.start  # always start from the beginning\n",
        "\n",
        "        # Initialize the statistics for each side of the split\n",
        "        self.Y_left = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.Y_right = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.dN_left = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.dN_right = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "\n",
        "        self.n_samples_left = new_pos - pos\n",
        "        self.delta_n_at_risk_left[:] = 0\n",
        "        self.n_events_left[:] = 0\n",
        "\n",
        "        # Update statistics up to new_pos\n",
        "        self.weighted_n_left = 0.0\n",
        "        for i in range(pos, new_pos):\n",
        "            idx = self.samples[i]\n",
        "            event = self.event[idx]  # Modified this line\n",
        "            time_idx = self.samples_time_idx[idx]\n",
        "\n",
        "            # Decide which side of the split the sample falls on\n",
        "            is_left = self.x[idx, split_feature] <= split_threshold\n",
        "\n",
        "            if is_left:\n",
        "                self.Y_left[time_idx] += 1\n",
        "                self.dN_left[time_idx] += event\n",
        "            else:\n",
        "                self.Y_right[time_idx] += 1\n",
        "                self.dN_right[time_idx] += event\n",
        "\n",
        "            w = 1.0  # Since we aren't using sample weights in this implementation\n",
        "            self.weighted_n_left += w * is_left\n",
        "\n",
        "        self.weighted_n_left = (self.weighted_n_node_samples - self.weighted_n_left)\n",
        "        self.pos = new_pos\n",
        "\n",
        "    def proxy_impurity_improvement(self):\n",
        "        # Compute w(t|x_j,c)\n",
        "        w = (self.Y_left * self.Y_right) / (self.Y_left + self.Y_right + 1e-7)\n",
        "\n",
        "        # Compute the numerator for the standardized pseudo-score test statistics\n",
        "        numer = np.sum(w * (self.dN_left / (self.Y_left + 1e-7) - self.dN_right / (self.Y_right + 1e-7)))\n",
        "\n",
        "        # Compute the variance estimate for the standardized pseudo-score test statistics\n",
        "        var_estimate = 0.0\n",
        "        for t in range(self.n_unique_times):\n",
        "            for Y, dN, total_Y, total_dN in [(self.Y_left, self.dN_left, self.Y_left + self.Y_right, self.dN_left + self.dN_right), (self.Y_right, self.dN_right, self.Y_left + self.Y_right, self.dN_left + self.dN_right)]:\n",
        "                if Y[t] == 0:\n",
        "                    continue\n",
        "                term = w[t] * Y[t] / total_Y[t] * (dN[t] - total_dN[t] / total_Y[t])\n",
        "                var_estimate += term ** 2\n",
        "\n",
        "        # Compute the standardized pseudo-score test statistic\n",
        "        return numer / np.sqrt(var_estimate + 1e-7) if var_estimate != 0.0 else -np.inf\n",
        "\n",
        "\n",
        "    def node_value(self):\n",
        "        # Extract the necessary statistics from the risk set\n",
        "        Y = self.riskset_total.n_at_risk\n",
        "        dN_bar = self.riskset_total.n_events\n",
        "\n",
        "        dest = np.zeros(2 * self.n_unique_times)\n",
        "\n",
        "        for t in range(self.n_unique_times):\n",
        "            if Y[t] == 0:\n",
        "                break\n",
        "\n",
        "            # Nelson-Aalen estimator for the mean function\n",
        "            dest[2 * t] = np.sum(dN_bar[:t + 1] / (Y[:t + 1] + 1e-7))\n",
        "            dest[2 * t + 1] = dN_bar[t] / (Y[t] + 1e-7) if Y[t] != 0 else 0\n",
        "\n",
        "        return dest\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset the risk set counter for the total node\n",
        "        self.riskset_total.reset()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-survival"
      ],
      "metadata": {
        "id": "Kj_g36gjxMj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnU7d9Fbu4VF"
      },
      "outputs": [],
      "source": [
        "from math import ceil\n",
        "from numbers import Integral, Real\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse import issparse\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.tree import _tree\n",
        "from sklearn.tree._classes import DENSE_SPLITTERS, SPARSE_SPLITTERS\n",
        "from sklearn.tree._splitter import Splitter\n",
        "from sklearn.tree._tree import BestFirstTreeBuilder, DepthFirstTreeBuilder, Tree\n",
        "from sklearn.utils._param_validation import Interval, StrOptions\n",
        "from sklearn.utils.validation import check_is_fitted, check_random_state\n",
        "\n",
        "import sksurv\n",
        "from sksurv.base import SurvivalAnalysisMixin\n",
        "from sksurv.functions import StepFunction\n",
        "from sksurv.util import check_array_survival\n",
        "from sksurv.tree._criterion import LogrankCriterion, get_unique_times\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "from queue import LifoQueue\n",
        "import pandas as pd\n",
        "class PseudoScoreTreeBuilder:\n",
        "    TREE_UNDEFINED = -np.inf\n",
        "\n",
        "    def __init__(self, max_depth=4, min_leaf=20, random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_leaf = min_leaf\n",
        "        self.random_state = random_state\n",
        "        self.random_state_ = check_random_state(self.random_state)\n",
        "\n",
        "    def build(self, X, ids, time_start, time_stop, event):\n",
        "        n_samples, n_features = X.shape\n",
        "        data = np.column_stack((time_start, time_stop, event))\n",
        "\n",
        "        # Building the tree using the specified hyperparameters\n",
        "        criterion = PseudoScoreCriterion(n_outputs=n_features, n_samples=n_samples, unique_times=np.unique(time_stop), x=X, ids=ids, time_start=time_start, time_stop=time_stop, event=event, random_state=self.random_state_)\n",
        "        criterion.init(data, None, n_samples, np.arange(n_samples), 0, n_samples, 0, n_samples)\n",
        "        val, feat, stat = self._get_best_split(X, data, criterion)\n",
        "        splits = LifoQueue()\n",
        "        splits.put((val, feat, stat, 0, np.arange(X.shape[0])))\n",
        "\n",
        "        node_stats = []\n",
        "        while splits.qsize() > 0:\n",
        "            val, feat, stat, lvl, idx = splits.get()\n",
        "            s = {\"feature\": feat, \"threshold\": val, \"n_node_samples\": idx.shape[0], \"statistic\": stat, \"depth\": lvl}\n",
        "            node_stats.append(s)\n",
        "\n",
        "            if val == self.TREE_UNDEFINED:\n",
        "                continue\n",
        "\n",
        "            left = X[idx, feat] <= val\n",
        "            right = idx[~left]\n",
        "            left = idx[left]\n",
        "\n",
        "            if lvl == self.max_depth - 1:\n",
        "                splits.put([self.TREE_UNDEFINED, self.TREE_UNDEFINED, [-np.inf], lvl + 1, right])\n",
        "                splits.put([self.TREE_UNDEFINED, self.TREE_UNDEFINED, [-np.inf], lvl + 1, left])\n",
        "                continue\n",
        "\n",
        "            X_right = X[right, :]\n",
        "            data_right = data[right, :]\n",
        "            s_right = self._get_best_split(X_right, data_right, criterion)\n",
        "            splits.put(list(s_right) + [lvl + 1, right])\n",
        "\n",
        "            X_left = X[left, :]\n",
        "            data_left = data[left, :]\n",
        "            s_left = self._get_best_split(X_left, data_left, criterion)\n",
        "            splits.put(list(s_left) + [lvl + 1, left])\n",
        "\n",
        "        return pd.DataFrame.from_dict(dict(zip(range(len(node_stats)), node_stats)), orient=\"index\")\n",
        "\n",
        "\n",
        "    def _get_best_split(self, X, y, criterion):\n",
        "        n_samples, n_features = X.shape\n",
        "        min_leaf = self.min_leaf\n",
        "        best_val = self.TREE_UNDEFINED\n",
        "        best_feat = self.TREE_UNDEFINED\n",
        "        best_stat = [-np.inf]\n",
        "\n",
        "        # Initialize the criterion for the whole node\n",
        "        criterion.init(y, None, n_samples, np.arange(n_samples), 0, n_samples)\n",
        "\n",
        "        for feat in range(n_features):\n",
        "            if np.unique(X[:, feat]).shape[0] < 2:\n",
        "                continue\n",
        "\n",
        "            for val in np.unique(X[:, feat]):\n",
        "                # Reset criterion for each potential split\n",
        "                criterion.reset()\n",
        "                left = X[:, feat] <= val\n",
        "\n",
        "                if np.sum(left) < min_leaf or np.sum(~left) < min_leaf:\n",
        "                    continue\n",
        "\n",
        "                # Update the criterion with the current potential split\n",
        "                criterion.update(np.sum(left), feat, val, np.where(left)[0])\n",
        "                stat = criterion.proxy_impurity_improvement()\n",
        "\n",
        "                if stat > best_stat[0]:\n",
        "                    best_feat = feat\n",
        "                    best_val = val\n",
        "                    best_stat = [stat]\n",
        "\n",
        "        return best_val, best_feat, best_stat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOOiDO-ju4VI"
      },
      "outputs": [],
      "source": [
        "class RecurrentTree:\n",
        "    def __init__(self, max_depth=4, min_leaf=20, random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_leaf = min_leaf\n",
        "        self.random_state = random_state\n",
        "        self.tree_ = None\n",
        "\n",
        "    def fit(self, X, ids, time_start, time_stop, event):\n",
        "        # Ensure input is in the expected format\n",
        "        X = np.array(X)\n",
        "        ids = np.array(ids)\n",
        "        time_start = np.array(time_start)\n",
        "        time_stop = np.array(time_stop)\n",
        "        event = np.array(event)\n",
        "\n",
        "        # Use the PseudoScoreTreeBuilder to build the tree\n",
        "        builder = PseudoScoreTreeBuilder(max_depth=self.max_depth, min_leaf=self.min_leaf, random_state=self.random_state)\n",
        "        self.tree_ = builder.build(X, ids=ids, time_start=time_start, time_stop=time_stop, event=event)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_tree(self):\n",
        "        \"\"\"\n",
        "        Return the tree as a pandas DataFrame.\n",
        "        Each row represents a node with its associated statistics.\n",
        "        \"\"\"\n",
        "        return self.tree_\n",
        "\n",
        "    def _traverse_tree(self, x, node_idx):\n",
        "        \"\"\"Traverse the tree to find the terminal node for a given sample.\"\"\"\n",
        "        node = self.tree_.loc[node_idx]\n",
        "\n",
        "        # Check if it's a terminal node\n",
        "        if node[\"threshold\"] == PseudoScoreTreeBuilder.TREE_UNDEFINED:\n",
        "            return node_idx\n",
        "\n",
        "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "            return self._traverse_tree(x, 2 * node_idx + 1)  # Assume left child at 2*idx + 1\n",
        "        else:\n",
        "            return self._traverse_tree(x, 2 * node_idx + 2)  # Assume right child at 2*idx + 2\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Ensure input is in the expected format\n",
        "        X = np.array(X)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        terminal_node_indices = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            # Traverse the tree to find the terminal node for the current sample\n",
        "            terminal_node_idx = self._traverse_tree(X[i], 0)\n",
        "            terminal_node_indices[i] = terminal_node_idx\n",
        "\n",
        "        return terminal_node_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrLZShNMu4VI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate synthetic recurrent event data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "n_features = 5\n",
        "\n",
        "# Feature data\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "\n",
        "# Generate some sample IDs\n",
        "ids = np.array([f\"id_{i}\" for i in range(1, n_samples + 1)])\n",
        "\n",
        "# Generate synthetic time start, time stop, and event data\n",
        "time_start = np.random.randint(0, 10, size=n_samples)\n",
        "time_stop = time_start + np.random.randint(1, 5, size=n_samples)\n",
        "event = np.random.randint(0, 2, size=n_samples)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, ids_train, ids_test, time_start_train, time_start_test, time_stop_train, time_stop_test, event_train, event_test = train_test_split(X, ids, time_start, time_stop, event, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now, let's try training the tree again\n",
        "tree = RecurrentTree(max_depth=3, min_leaf=5)\n",
        "tree.fit(X=X_train, ids=ids_train, time_start=time_start_train, time_stop=time_stop_train, event=event_train)\n",
        "\n",
        "# Predict using the RecurrentTree on the test set\n",
        "predictions = tree.predict(X_test)\n",
        "\n",
        "# Print a summary of the built tree and the first few predictions\n",
        "tree_summary = tree.get_tree()\n",
        "tree_summary, predictions[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6C0ZZw7u4VM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numbers import Integral, Real\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "def _get_n_samples_bootstrap(n_ids, max_samples):\n",
        "    \"\"\"\n",
        "    Modified for recurrent events. Get the number of IDs in a bootstrap sample.\n",
        "    \"\"\"\n",
        "    if max_samples is None:\n",
        "        return n_ids\n",
        "\n",
        "    if isinstance(max_samples, Integral):\n",
        "        if max_samples > n_ids:\n",
        "            msg = \"`max_samples` must be <= n_ids={} but got value {}\"\n",
        "            raise ValueError(msg.format(n_ids, max_samples))\n",
        "        return max_samples\n",
        "\n",
        "    if isinstance(max_samples, Real):\n",
        "        return max(round(n_ids * max_samples), 1)\n",
        "\n",
        "def _generate_sample_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Sample unique IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    random_instance = check_random_state(random_state)\n",
        "    sampled_ids = np.random.choice(ids, n_ids_bootstrap, replace=True)\n",
        "    return sampled_ids\n",
        "\n",
        "def _generate_unsampled_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Determine unsampled IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    sampled_ids = _generate_sample_indices(random_state, ids, n_ids_bootstrap)\n",
        "    unsampled_ids = np.setdiff1d(ids, sampled_ids)\n",
        "\n",
        "    # Expand these unsampled IDs to include all their associated events.\n",
        "    # Again, this will depend on your data structure.\n",
        "    # As an example:\n",
        "    # unsampled_indices = np.concatenate([events_by_id[id] for id in unsampled_ids])\n",
        "\n",
        "    return unsampled_ids  # or return unsampled_indices based on your data structure\n",
        "\n",
        "\n",
        "\n",
        "from warnings import catch_warnings, simplefilter\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "def _parallel_build_trees(\n",
        "    tree,\n",
        "    bootstrap,\n",
        "    X,\n",
        "    y,\n",
        "    ids,  # New parameter: a list/array of IDs corresponding to each event in X and y\n",
        "    sample_weight,\n",
        "    tree_idx,\n",
        "    n_trees,\n",
        "    verbose=0,\n",
        "    class_weight=None,\n",
        "    n_ids_bootstrap=None,  # Instead of n_samples_bootstrap\n",
        "):\n",
        "    \"\"\"\n",
        "    Private function used to fit a single tree in parallel for recurrent events.\"\"\"\n",
        "    if verbose > 1:\n",
        "        print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n",
        "\n",
        "    if bootstrap:\n",
        "        unique_ids = np.unique(ids)\n",
        "        n_ids = len(unique_ids)\n",
        "\n",
        "        # Generate bootstrap samples using IDs\n",
        "        sampled_ids = _generate_sample_indices(\n",
        "            tree.random_state, unique_ids, n_ids_bootstrap\n",
        "        )\n",
        "\n",
        "        # Expand sampled IDs to all their associated events\n",
        "        indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "\n",
        "        if sample_weight is None:\n",
        "            curr_sample_weight = np.ones((X.shape[0],), dtype=np.float64)\n",
        "        else:\n",
        "            curr_sample_weight = sample_weight.copy()\n",
        "\n",
        "        # Adjust the sample weight based on how many times each ID was sampled\n",
        "        sample_counts_for_ids = np.bincount(np.searchsorted(unique_ids, sampled_ids), minlength=n_ids)\n",
        "        curr_sample_weight *= sample_counts_for_ids[np.searchsorted(unique_ids, ids)]\n",
        "\n",
        "        if class_weight == \"subsample\":\n",
        "            with catch_warnings():\n",
        "                simplefilter(\"ignore\", DeprecationWarning)\n",
        "                curr_sample_weight *= compute_sample_weight(\"auto\", y, indices=indices)\n",
        "        elif class_weight == \"balanced_subsample\":\n",
        "            curr_sample_weight *= compute_sample_weight(\"balanced\", y, indices=indices)\n",
        "\n",
        "        tree.fit(X[indices], y[indices], sample_weight=curr_sample_weight[indices], check_input=False)\n",
        "    else:\n",
        "        tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
        "\n",
        "    return tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsg41jCiu4VN"
      },
      "outputs": [],
      "source": [
        "def compute_C_index(mu_OOB_i, mu_OOB_iprime, N_i, N_iprime):\n",
        "    \"\"\"\n",
        "    Computes the concordance index for recurrent events.\n",
        "\n",
        "    Parameters:\n",
        "    - mu_OOB_i: List of cumulative hazards/risk scores for individuals i.\n",
        "    - mu_OOB_iprime: List of cumulative hazards/risk scores for individuals i'.\n",
        "    - N_i: List of number of events experienced by individuals i up to a given time.\n",
        "    - N_iprime: List of number of events experienced by individuals i' up to a given time.\n",
        "\n",
        "    Returns:\n",
        "    - C_index: The computed concordance index.\n",
        "    \"\"\"\n",
        "\n",
        "    m = len(mu_OOB_i)\n",
        "    concordant_pairs = 0\n",
        "    valid_pairs = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        for iprime in range(m):\n",
        "            if N_i[i] > N_iprime[iprime]:\n",
        "                valid_pairs += 1\n",
        "                if mu_OOB_i[i] > mu_OOB_iprime[iprime]:\n",
        "                    concordant_pairs += 1\n",
        "\n",
        "    C_index = concordant_pairs / valid_pairs if valid_pairs != 0 else 0\n",
        "    return C_index\n",
        "\n",
        "def prediction_error_rate(C_index):\n",
        "    \"\"\"Computes the prediction error rate given the concordance index.\"\"\"\n",
        "    return 1 - C_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_vpL8Iju4VN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Number of unique IDs\n",
        "n_ids = 5\n",
        "\n",
        "# Generate a random number of events for each ID\n",
        "events_per_id = np.random.randint(1, 6, size=n_ids)\n",
        "\n",
        "# Generate the synthetic dataset\n",
        "X = []\n",
        "y = []\n",
        "ids = []\n",
        "\n",
        "for id_num, n_events in enumerate(events_per_id, 1):\n",
        "    X.extend(np.random.rand(n_events, 3))  # 3 features for simplicity\n",
        "    y.extend(np.random.randint(0, 2, n_events))  # Binary target values\n",
        "    ids.extend([id_num] * n_events)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "ids = np.array(ids)\n",
        "\n",
        "X.shape, y.shape, ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI2IeLTsu4VN"
      },
      "outputs": [],
      "source": [
        "# Generate bootstrap sample indices\n",
        "sampled_indices = _generate_sample_indices(np.random.RandomState(None), np.unique(ids), 3)  # Sampling 3 unique IDs\n",
        "\n",
        "# Expand to get the indices of all events associated with the sampled IDs\n",
        "bootstrap_indices = np.where(np.isin(ids, sampled_indices))[0]\n",
        "\n",
        "# Display the IDs for the bootstrapped events\n",
        "bootstrapped_ids = ids[bootstrap_indices]\n",
        "\n",
        "bootstrapped_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWhXItdlu4VN"
      },
      "outputs": [],
      "source": [
        "from warnings import catch_warnings, simplefilter\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "\n",
        "def _parallel_build_trees(\n",
        "    tree,\n",
        "    bootstrap,\n",
        "    X,\n",
        "    y,\n",
        "    ids,  # New parameter: a list/array of IDs corresponding to each event in X and y\n",
        "    sample_weight,\n",
        "    tree_idx,\n",
        "    n_trees,\n",
        "    verbose=0,\n",
        "    class_weight=None,\n",
        "    n_ids_bootstrap=None,  # Instead of n_samples_bootstrap\n",
        "):\n",
        "    \"\"\"\n",
        "    Private function used to fit a single tree in parallel for recurrent events.\"\"\"\n",
        "    if verbose > 1:\n",
        "        print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n",
        "\n",
        "    if bootstrap:\n",
        "        unique_ids = np.unique(ids)\n",
        "        n_ids = len(unique_ids)\n",
        "\n",
        "        # Generate bootstrap samples using IDs\n",
        "        sampled_ids = _generate_sample_indices(\n",
        "            tree.random_state, unique_ids, n_ids_bootstrap\n",
        "        )\n",
        "\n",
        "        # Expand sampled IDs to all their associated events\n",
        "        indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "\n",
        "        if sample_weight is None:\n",
        "            curr_sample_weight = np.ones((X.shape[0],), dtype=np.float64)\n",
        "        else:\n",
        "            curr_sample_weight = sample_weight.copy()\n",
        "\n",
        "        # Adjust the sample weight based on how many times each ID was sampled\n",
        "        sample_counts_for_ids = np.bincount(np.searchsorted(unique_ids, sampled_ids), minlength=n_ids)\n",
        "        curr_sample_weight *= sample_counts_for_ids[np.searchsorted(unique_ids, ids)]\n",
        "\n",
        "        if class_weight == \"subsample\":\n",
        "            with catch_warnings():\n",
        "                simplefilter(\"ignore\", DeprecationWarning)\n",
        "                curr_sample_weight *= compute_sample_weight(\"auto\", y, indices=indices)\n",
        "        elif class_weight == \"balanced_subsample\":\n",
        "            curr_sample_weight *= compute_sample_weight(\"balanced\", y, indices=indices)\n",
        "\n",
        "        tree.fit(X[indices], y[indices], sample_weight=curr_sample_weight[indices], check_input=False)\n",
        "    else:\n",
        "        tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
        "\n",
        "    return tree\n",
        "\n",
        "# Let's fit the tree again\n",
        "_parallel_build_trees(\n",
        "    tree,\n",
        "    bootstrap=True,\n",
        "    X=X,\n",
        "    y=y,\n",
        "    ids=ids,\n",
        "    sample_weight=None,\n",
        "    tree_idx=0,\n",
        "    n_trees=1,\n",
        "    verbose=1,\n",
        "    class_weight=None,\n",
        "    n_ids_bootstrap=3\n",
        ")\n",
        "\n",
        "# Plot the fitted tree\n",
        "plt.figure(figsize=(15, 10))\n",
        "plot_tree(tree, filled=True, feature_names=['Feature 1', 'Feature 2', 'Feature 3'], class_names=['Class 0', 'Class 1'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQz1ECh0u4VO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}