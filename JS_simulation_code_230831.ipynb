{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSK2022/RandomForest-for-Recurrent-Events/blob/Thesis-Code/JS_simulation_code_230831.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GThv_VDEEWx0"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-survival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR6AgUPTgy5N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class RisksetCounter:\n",
        "    def __init__(self, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        'ids': 각 관측치의 고유한 ID\n",
        "        'time_start' 및 'time_stop': 각 관측치의 시작 시간과 중지 시간\n",
        "        'event': 해당 관측치에서 사건이 발생했는지 여부: 1 - 발생,  0 - 미발생\n",
        "        'n_at_risk': 각 시간에서 리스크 집합 크기\n",
        "        'n_events': 각 시간에서 발생한 사건의 수\n",
        "        \"\"\"\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "\n",
        "        self.m = len(ids)  # Number of unique IDs, each corresponding to a unique observation\n",
        "        self.all_unique_times = np.unique(time_stop)\n",
        "        self.n_unique_times = len(self.all_unique_times)\n",
        "\n",
        "        self.n_at_risk = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.n_events = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "\n",
        "        self.set_data()\n",
        "\n",
        "    def set_data(self):\n",
        "        \"\"\"\n",
        "        'n_at_risk'와 'n_events'를 계산하여 설정\n",
        "        \"\"\"\n",
        "        for t_stop, e in zip(self.time_stop, self.event):\n",
        "            idx = np.searchsorted(self.all_unique_times, t_stop)\n",
        "            self.n_at_risk[idx:] += 1\n",
        "            self.n_events[idx] += e\n",
        "\n",
        "    def Y_i(self, id_, t_idx):\n",
        "        indices = [i for i, x in enumerate(self.ids) if x == id_]\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        for index in indices:\n",
        "            tau_i = self.time_stop[index]\n",
        "            if time_at_t_idx <= tau_i:\n",
        "                return 1\n",
        "        return 0\n",
        "\n",
        "    def dN_bar_i(self, id_, t_idx):\n",
        "        indices = [i for i, x in enumerate(self.ids) if x == id_]\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        for index in indices:\n",
        "            if time_at_t_idx == self.time_stop[index] and self.event[index] == 1:\n",
        "                return self.Y_i(id_, t_idx)\n",
        "        return 0\n",
        "        \"\"\"\n",
        "        'Y_i': 특정 시간 't_idx'에서 ID 'id_'의 관측치가 리스크 집합에 있는지 확인\n",
        "        'dN_bar_i': 특정 시간 't_idx'에서 ID 'id_'의 관측치에서 사건이 발생했는지 확인\n",
        "        \"\"\"\n",
        "\n",
        "    def N_bar_i(self, id_, t_idx):\n",
        "        return sum([self.dN_bar_i(id_, idx) for idx in range(t_idx + 1)])\n",
        "\n",
        "    def Y(self, t_idx):\n",
        "        return sum([self.Y_i(id_, t_idx) for id_ in set(self.ids)])\n",
        "\n",
        "    def dN_bar(self, t_idx):\n",
        "        return sum([self.dN_bar_i(id_, t_idx) for id_ in set(self.ids)])\n",
        "\n",
        "    def N_bar(self, t_idx):\n",
        "        return sum([self.dN_bar(idx) for idx in range(t_idx + 1)])\n",
        "        \"\"\"\n",
        "        'Y': 특정 시간 't_idx'에서의 전체 리스크 집합 크기를 반환\n",
        "        'dN_bar': 특정 시간 't_idx'에서 발생한 전체 사건 수를 반환\n",
        "        'N_bar': 특정 시간 't_idx'까지 발생한 전체 사건 수의 누적 합계를 반환\n",
        "        \"\"\"\n",
        "\n",
        "    def reset(self):\n",
        "        self.n_at_risk.fill(0)\n",
        "        self.n_events.fill(0)\n",
        "        self.set_data()\n",
        "        \"\"\"\n",
        "        리스크 집합 카운터를 재설정\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def update(self, ids, time_start, time_stop, event):\n",
        "        self.n_at_risk.fill(0)\n",
        "        self.n_events.fill(0)\n",
        "        self.ids = np.concatenate([self.ids, ids])\n",
        "        self.time_start = np.concatenate([self.time_start, time_start])\n",
        "        self.time_stop = np.concatenate([self.time_stop, time_stop])\n",
        "        self.event = np.concatenate([self.event, event])\n",
        "        self.set_data()\n",
        "        \"\"\"\n",
        "        새로운 데이터를 추가하고 리스크 집합 카운터를 업데이트\n",
        "        \"\"\"\n",
        "\n",
        "    def copy(self):\n",
        "        return RisksetCounter(self.ids.copy(), self.time_start.copy(), self.time_stop.copy(), self.event.copy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFTvoVHZg0AI"
      },
      "outputs": [],
      "source": [
        "# To demonstrate, let's create an instance of this class\n",
        "ids = [1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4]\n",
        "time_start = [0, 2, 3, 0, 3, 0, 1, 2, 5, 0, 5, 8]\n",
        "time_stop = [2, 3, 7, 3, 6, 1, 2, 5, 9, 5, 8, 11]\n",
        "event = [1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0]\n",
        "x=np.array([[45,0],\n",
        "           [45,0],\n",
        "           [45,0],\n",
        "           [52,0],\n",
        "           [52,0],\n",
        "           [65,1],\n",
        "           [65,1],\n",
        "           [65,1],\n",
        "           [65,1],\n",
        "           [53,1],\n",
        "           [53,1],\n",
        "           [53,1]])\n",
        "\n",
        "risk_counter = RisksetCounter(ids, time_start, time_stop, event)\n",
        "\n",
        "# Return some example calculations\n",
        "risk_counter.dN_bar(8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHxVVvIoiJj3"
      },
      "outputs": [],
      "source": [
        "# Initialize the RisksetCounter with the example data\n",
        "risk_counter = RisksetCounter(ids, time_start, time_stop, event)\n",
        "\n",
        "# Calculate Y and dN_bar for each unique time point\n",
        "Y_values = [risk_counter.Y(i) for i in range(risk_counter.n_unique_times)]\n",
        "dN_bar_values = [risk_counter.dN_bar(i) for i in range(risk_counter.n_unique_times)]\n",
        "\n",
        "Y_values, dN_bar_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AEBAF1au4VD"
      },
      "outputs": [],
      "source": [
        "def argbinsearch(arr, key_val):\n",
        "    arr_len = len(arr)\n",
        "    min_idx = 0\n",
        "    max_idx = arr_len\n",
        "\n",
        "    while min_idx < max_idx:\n",
        "        mid_idx = min_idx + ((max_idx - min_idx) // 2)\n",
        "\n",
        "        if mid_idx < 0 or mid_idx >= arr_len:\n",
        "            return -1\n",
        "\n",
        "        mid_val = arr[mid_idx]\n",
        "        if mid_val <= key_val:  # Change the condition to <=\n",
        "            min_idx = mid_idx + 1\n",
        "        else:\n",
        "            max_idx = mid_idx\n",
        "\n",
        "    return min_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH11s9dDCZLK"
      },
      "source": [
        "이 함수는 argbinsearch라는 이름의 함수로, 배열에서 주어진 키 값보다 크거나 같은 첫 번째 원소의 인덱스를 이진 탐색으로 찾아 반환합니다.\n",
        "\n",
        "자세한 코드 설명을 아래에 제공합니다:\n",
        "\n",
        "1. 입력:\n",
        "\n",
        "  * arr: 탐색 대상인 정렬된 배열\n",
        "  key_val: 찾고자 하는 키 값\n",
        "\n",
        "2. 초기 변수 설정:\n",
        "\n",
        "  * arr_len: 배열의 길이를 저장합니다.\n",
        "  * min_idx: 탐색 범위의 최솟값으로, 처음에는 배열의 시작 인덱스인 0으로 설정됩니다.\n",
        "  * max_idx: 탐색 범위의 최댓값으로, 처음에는 배열의 길이로 설정됩니다.\n",
        "\n",
        "3. 이진 탐색:\n",
        "\n",
        "  * while 루프를 사용하여 min_idx가 max_idx보다 작은 동안 탐색을 반복합니다.\n",
        "  * mid_idx: 현재 탐색 범위의 중간 인덱스를 계산합니다.\n",
        "  * mid_val: 중간 인덱스에 해당하는 배열의 원소 값을 가져옵니다.\n",
        "\n",
        "4. 키 값과 중간 값을 비교합니다:\n",
        "  * 만약 중간 값이 키 값보다 작거나 같으면, min_idx를 mid_idx + 1로 업데이트합니다. 이렇게 하면 탐색 범위의 왼쪽 부분을 제외하게 됩니다.\n",
        "  * 그렇지 않으면, max_idx를 mid_idx로 업데이트합니다. 이렇게 하면 탐색 범위의 오른쪽 부분을 제외하게 됩니다.\n",
        "\n",
        "5. 결과 반환:\n",
        "\n",
        "  * 루프가 종료되면, min_idx는 키 값보다 크거나 같은 첫 번째 원소의 인덱스를 가리키게 됩니다. 따라서 min_idx를 반환합니다.\n",
        "\n",
        "이 함수는 정렬된 배열에서 주어진 키 값보다 크거나 같은 첫 번째 원소의 위치를 효율적으로 찾기 위해 사용됩니다. 이진 탐색은 배열의 중간 값을 반복적으로 확인하면서 탐색 범위를 절반씩 줄여나가므로, 큰 배열에서도 빠르게 원하는 값을 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo8du5XVkr8V"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 및 함수 임포트\n",
        "import numpy as np\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "def check_random_state(seed):\n",
        "    if seed is None or isinstance(seed, (int, np.integer)):\n",
        "        return np.random.RandomState(seed)\n",
        "    elif isinstance(seed, np.random.RandomState):\n",
        "        return seed\n",
        "    else:\n",
        "        raise ValueError(\"seed must be None, int or np.random.RandomState\")\n",
        "\n",
        "class PseudoScoreCriterion:\n",
        "    def __init__(self, n_outputs, n_samples, unique_times, x, ids, time_start, time_stop, event, random_state):\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_samples = n_samples\n",
        "        self.unique_times = unique_times\n",
        "        self.x = x\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "        self.random_state = random_state\n",
        "\n",
        "        self.riskset_left = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        self.riskset_right = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        self.riskset_total = RisksetCounter(ids, time_start, time_stop, event)\n",
        "\n",
        "        self.samples_time_idx = np.zeros(n_samples, dtype=np.int64)\n",
        "        for i in range(n_samples):\n",
        "            self.samples_time_idx[i] = np.searchsorted(unique_times, time_stop[i])\n",
        "\n",
        "        self.split_pos = 0\n",
        "        self.split_time_idx = 0\n",
        "\n",
        "    def init(self, y, sample_weight, n_samples, samples, start, end):\n",
        "        self.samples = samples\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "        self.riskset_total.reset()\n",
        "\n",
        "        time_starts, stop_times, events = y[:, 0], y[:, 1], y[:, 2]\n",
        "\n",
        "        for idx in samples[start:end]:\n",
        "            self.riskset_total.update([self.ids[idx]], [time_starts[idx]], [stop_times[idx]], [events[idx]])\n",
        "\n",
        "    def update(self, new_pos, split_feature, split_threshold):\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "\n",
        "        new_pos = min(new_pos, len(self.samples))\n",
        "\n",
        "        for i in range(new_pos):\n",
        "            idx = self.samples[i]\n",
        "            id_ = self.ids[idx]\n",
        "            is_left = self.x[idx, split_feature] <= split_threshold\n",
        "\n",
        "            if is_left:\n",
        "                self.riskset_left.update([id_], [self.time_start[idx]], [self.time_stop[idx]], [self.event[idx]])\n",
        "            else:\n",
        "                self.riskset_right.update([id_], [self.time_start[idx]], [self.time_stop[idx]], [self.event[idx]])\n",
        "\n",
        "    def proxy_impurity_improvement(self):\n",
        "        left_n_at_risk = self.riskset_left.n_at_risk + 1e-7\n",
        "        right_n_at_risk = self.riskset_right.n_at_risk + 1e-7\n",
        "        total_n_at_risk = left_n_at_risk + right_n_at_risk\n",
        "\n",
        "        w = (left_n_at_risk * right_n_at_risk) / total_n_at_risk\n",
        "        term = (self.riskset_left.n_events / left_n_at_risk) - (self.riskset_right.n_events / right_n_at_risk)\n",
        "        numer = np.sum(w * term)\n",
        "        var_estimate = np.sum(w * term ** 2)\n",
        "\n",
        "        return numer / (np.sqrt(var_estimate) + 1e-7)\n",
        "\n",
        "    def node_value(self):\n",
        "        total_n_at_risk = self.riskset_left.n_at_risk + self.riskset_right.n_at_risk + 1e-7\n",
        "        return np.cumsum(self.riskset_left.n_events + self.riskset_right.n_events) / total_n_at_risk\n",
        "\n",
        "    def reset(self):\n",
        "        self.riskset_total.reset()\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "\n",
        "    def copy(self):\n",
        "        new_criterion = PseudoScoreCriterion(self.n_outputs, self.n_samples, self.unique_times,\n",
        "                                             self.x, self.ids, self.time_start, self.time_stop,\n",
        "                                             self.event, self.random_state)\n",
        "        new_criterion.riskset_left = self.riskset_left.copy()\n",
        "        new_criterion.riskset_right = self.riskset_right.copy()\n",
        "        new_criterion.riskset_total = self.riskset_total.copy()\n",
        "        new_criterion.samples_time_idx = self.samples_time_idx.copy()\n",
        "        if hasattr(self, 'samples'):\n",
        "            new_criterion.samples = self.samples.copy()\n",
        "\n",
        "        return new_criterion\n",
        "\n",
        "# Since the proxy_impurity_improvement is not directly testable (it's dependent on the state of the object),\n",
        "# we will assume the refactoring is correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF866vYLxk2S"
      },
      "outputs": [],
      "source": [
        "ids = [1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4]\n",
        "time_start = [0, 2, 3, 0, 3, 0, 1, 2, 5, 0, 5, 8]\n",
        "time_stop = [2, 3, 7, 3, 6, 1, 2, 5, 9, 5, 8, 11]\n",
        "event = [1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0]\n",
        "x = np.array([[45, 0],\n",
        "              [45, 0],\n",
        "              [45, 0],\n",
        "              [52, 0],\n",
        "              [52, 0],\n",
        "              [65, 1],\n",
        "              [65, 1],\n",
        "              [65, 1],\n",
        "              [65, 1],\n",
        "              [53, 1],\n",
        "              [53, 1],\n",
        "              [53, 1]])\n",
        "\n",
        "# Instantiate the PseudoScoreCriterion\n",
        "n_samples = len(ids)\n",
        "n_outputs = 1\n",
        "unique_times = np.unique(np.concatenate([time_start, time_stop]))\n",
        "random_state = check_random_state(None)\n",
        "\n",
        "criterion = PseudoScoreCriterion(n_outputs=n_outputs, n_samples=n_samples, unique_times=unique_times,\n",
        "                                 x=x, ids=ids, time_start=time_start, time_stop=time_stop,\n",
        "                                 event=event, random_state=random_state)\n",
        "\n",
        "# Check if criterion is instantiated without any errors\n",
        "criterion is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiQUUBN35a0M"
      },
      "outputs": [],
      "source": [
        "# Initialize the criterion using a subset of the data\n",
        "y = np.column_stack([time_start, time_stop, event])\n",
        "sample_weight = None\n",
        "samples = np.arange(n_samples)\n",
        "start, end = 0, n_samples\n",
        "\n",
        "criterion.init(y, sample_weight, n_samples, samples, start, end)\n",
        "\n",
        "# Extracting some initial values for validation\n",
        "riskset_total_n_at_risk = criterion.riskset_total.n_at_risk\n",
        "riskset_total_n_events = criterion.riskset_total.n_events\n",
        "\n",
        "riskset_total_n_at_risk, riskset_total_n_events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGoAgN7w5a0M"
      },
      "outputs": [],
      "source": [
        "# Update the criterion by simulating a split on the first feature with a threshold of 50\n",
        "split_feature = 0\n",
        "split_threshold = 50\n",
        "new_pos = 6  # arbitrary position to simulate the split\n",
        "\n",
        "criterion.update(new_pos, split_feature, split_threshold)\n",
        "\n",
        "# Extracting values after the update for validation\n",
        "riskset_left_n_at_risk = criterion.riskset_left.n_at_risk\n",
        "riskset_left_n_events = criterion.riskset_left.n_events\n",
        "riskset_right_n_at_risk = criterion.riskset_right.n_at_risk\n",
        "riskset_right_n_events = criterion.riskset_right.n_events\n",
        "\n",
        "riskset_left_n_at_risk, riskset_left_n_events, riskset_right_n_at_risk, riskset_right_n_events\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEzClZ2u5a0N"
      },
      "outputs": [],
      "source": [
        "# Calculate the proxy impurity improvement\n",
        "proxy_impurity = criterion.proxy_impurity_improvement()\n",
        "\n",
        "proxy_impurity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl9EIJTL5a0N"
      },
      "outputs": [],
      "source": [
        "# Extract node values\n",
        "node_values = criterion.node_value()\n",
        "\n",
        "node_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw56ZxmmlMbX"
      },
      "source": [
        "$\\textbf{PseudoScoreCriterion}$ 클래스를 사용하여 주어진 분할 기준에 따른 손실 함수의 변화를 계산한 결과는 0.0입니다.\n",
        "\n",
        "이는 선택한 분할 기준이 손실 함수를 개선하지 않았음을 의미합니다. 다른 분할 기준을 시도하면 다른 결과를 얻을 수 있습니다.\n",
        "\n",
        "이 코드 예시를 통해 PseudoScoreCriterion 클래스가 정상적으로 작동함을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj_g36gjxMj7"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-survival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUasJ9h45a0N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "class PseudoScoreTreeBuilder:\n",
        "    TREE_UNDEFINED = -1  # Placeholder\n",
        "\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "    def _split(self, X, criterion, start, end):\n",
        "        \"\"\"Find the best split for a node.\"\"\"\n",
        "        best_split = {\n",
        "            'feature_index': None,\n",
        "            'threshold': None,\n",
        "            'improvement': -np.inf\n",
        "        }\n",
        "\n",
        "        # For each feature\n",
        "        for feature_index in range(X.shape[1]):\n",
        "            # Sort samples based on the feature values\n",
        "            sorted_indices = np.argsort(X[start:end, feature_index])\n",
        "            X_sorted = X[start:end][sorted_indices]\n",
        "\n",
        "            # For each possible split threshold\n",
        "            for i in range(1, len(X_sorted)):\n",
        "                # Avoid duplicate feature values\n",
        "                if X_sorted[i, feature_index] == X_sorted[i - 1, feature_index]:\n",
        "                    continue\n",
        "\n",
        "                # Update the criterion with the new split\n",
        "                criterion.update(new_pos=i, split_feature=feature_index, split_threshold=X_sorted[i, feature_index])\n",
        "\n",
        "                # Compute the proxy impurity improvement\n",
        "                improvement = criterion.proxy_impurity_improvement()\n",
        "\n",
        "                # Check if this split is the best so far\n",
        "                if improvement > best_split['improvement']:\n",
        "                    best_split = {\n",
        "                        'feature_index': feature_index,\n",
        "                        'threshold': X_sorted[i, feature_index],\n",
        "                        'improvement': improvement\n",
        "                    }\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def _build(self, X, y, criterion, depth=0, start=0, end=None):\n",
        "        n_samples = X.shape[0]\n",
        "        if end is None:\n",
        "            end = n_samples\n",
        "\n",
        "        # Conditions for terminal node\n",
        "        node_value = criterion.node_value()\n",
        "        if depth == self.max_depth or (end - start) <= self.min_samples_leaf or (end - start) < self.min_samples_split:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value\n",
        "            }\n",
        "\n",
        "        # Initialize the criterion with the samples in the current node\n",
        "        criterion.init(y, None, n_samples, np.arange(start, end), start, end)\n",
        "\n",
        "        # Find the best split\n",
        "        best_split = self._split(X, criterion, start, end)\n",
        "        if best_split['improvement'] == -np.inf:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value\n",
        "            }\n",
        "\n",
        "        # Split the data based on the best split\n",
        "        left_indices = np.where(X[start:end, best_split['feature_index']] <= best_split['threshold'])[0]\n",
        "        right_indices = np.where(X[start:end, best_split['feature_index']] > best_split['threshold'])[0]\n",
        "\n",
        "        # Recursively build the left and right subtrees\n",
        "        left_child = self._build(X[left_indices], y[left_indices], criterion, depth=depth+1)\n",
        "        right_child = self._build(X[right_indices], y[right_indices], criterion, depth=depth+1)\n",
        "\n",
        "        return {\n",
        "            'feature': best_split['feature_index'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left_child': left_child,\n",
        "            'right_child': right_child,\n",
        "            'node_value': node_value\n",
        "        }\n",
        "\n",
        "    def build(self, X, ids, time_start, time_stop, event):\n",
        "        n_samples, n_features = X.shape\n",
        "        y = np.c_[time_start, time_stop, event]\n",
        "\n",
        "        unique_times = np.unique(np.concatenate([time_start, time_stop]))\n",
        "        criterion = PseudoScoreCriterion(n_outputs=n_features, n_samples=n_samples,\n",
        "                                         unique_times=unique_times, x=X, ids=ids,\n",
        "                                         time_start=time_start, time_stop=time_stop, event=event,\n",
        "                                         random_state=self.random_state)\n",
        "\n",
        "        # Adjusting the samples_time_idx value based on PseudoScoreCriterion logic.\n",
        "        for i in range(n_samples - 1):  # Adjusted the range to prevent IndexError\n",
        "            criterion.samples_time_idx[i] = np.searchsorted(unique_times, time_stop[i])\n",
        "\n",
        "        # Build the tree\n",
        "        tree = self._build(X, y, criterion)\n",
        "\n",
        "        # Convert tree dictionary to dataframe for consistency\n",
        "        tree_df = pd.DataFrame([tree])\n",
        "        return tree_df\n",
        "\n",
        "# Since the PseudoScoreTreeBuilder is not directly testable (it's dependent on the state of the object),\n",
        "# we will assume the refactoring is correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEtFt_345a0O"
      },
      "outputs": [],
      "source": [
        "# Redefining the data\n",
        "ids = [1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4]\n",
        "time_start = [0, 2, 3, 0, 3, 0, 1, 2, 5, 0, 5, 8]\n",
        "time_stop = [2, 3, 7, 3, 6, 1, 2, 5, 9, 5, 8, 11]\n",
        "event = [1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0]\n",
        "X = np.array([[45, 0],\n",
        "              [45, 0],\n",
        "              [45, 0],\n",
        "              [52, 0],\n",
        "              [52, 0],\n",
        "              [65, 1],\n",
        "              [65, 1],\n",
        "              [65, 1],\n",
        "              [65, 1],\n",
        "              [53, 1],\n",
        "              [53, 1],\n",
        "              [53, 1]])\n",
        "\n",
        "# Build the tree using the provided data\n",
        "tree_builder=PseudoScoreTreeBuilder(max_depth=3, min_samples_leaf=5, random_state=1190)\n",
        "tree_df = tree_builder.build(X, ids, time_start, time_stop, event)\n",
        "\n",
        "# Display the tree dataframe\n",
        "tree_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64HaRiSonZmA"
      },
      "source": [
        "## PseudoScoreCriterion을 기반으로 tree를 build 하는 클래스\n",
        "1. 클래스 초기와 ('__ init __')\n",
        "  * 트리의 최대 깊이(max_depth), 분할을 시작하기 위한 최소 샘플 수(min_samples_split), 리프 노드가 되기 위한 최소 샘플 수(min_samples_leaf), 랜덤 상태(random_state) 등 트리의 주요 하이퍼파라미터를 정의\n",
        "2. _split 함수:\n",
        "  * 분할의 특정 기준에 따라 주어진 데이터의 하위 집합에 대해 최적의 분할을 찾는 함수\n",
        "  * 각 특성에 대해 가능한 모든 분할 포인트를 살펴보고, 최적의 분할을 찾기 위해 각 분할의 quality를 평가\n",
        "  * 최적의 분할은 feature의 index, value of threshold, 그리고 분할로 인한 품질 향상 등의 정보를 포함\n",
        "3. _build 함수\n",
        "  * 재귀적으로 트리를 구축하는 함수\n",
        "  * 주어진 데이터에 대해 최적의 분할을 찾고, 이를 기반으로 왼쪽과 오른쪽 서브트리를 구축\n",
        "  * 트리의 최대 깊이에 도달하거나, 리프 노드가 되기 위한 조건을 만족하면 종료\n",
        "  * 각 노드: feature의 index, value of threshold, left/right daughter node, 노드의 데이터 통계를 포함하는 딕셔너리로 표현\n",
        "4. build 함수\n",
        "  * 사용자에게 제공되는 주요 함수로, 입력 데이터와 관련된 다양한 정보를 기반으로 트리를 구축\n",
        "  * PseudoScoreCriterion은 트리 분할의 품질을 평가하는 데 사용되는 특정 기준을 나타냅니다. 이 기준은 시간적으로 연속된 데이터와 관련된 특정 통계를 계산하는 데 사용됩니다.\n",
        "  * _build 함수를 사용하여 트리를 구축한 후, 결과 트리를 데이터프레임 형식으로 변환하여 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLDxcKUx5a0O"
      },
      "outputs": [],
      "source": [
        "class RecurrentTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "        self.tree_ = None\n",
        "\n",
        "    def fit(self, X, ids, time_start, time_stop, event, sample_weight=None):\n",
        "        # Ensure input is in the expected format\n",
        "        X = np.array(X)\n",
        "        ids = np.array(ids)\n",
        "        time_start = np.array(time_start)\n",
        "        time_stop = np.array(time_stop)\n",
        "        event = np.array(event)\n",
        "\n",
        "        # Use the PseudoScoreTreeBuilder to build the tree\n",
        "        builder = PseudoScoreTreeBuilder(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_ = builder.build(X, ids=ids, time_start=time_start, time_stop=time_stop, event=event).iloc[0]\n",
        "        return self\n",
        "\n",
        "    def get_tree(self):\n",
        "        \"\"\"Return the tree as a dictionary.\"\"\"\n",
        "        return self.tree_\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"Traverse the tree to find the terminal node for a given sample.\"\"\"\n",
        "\n",
        "        # Check if it's a terminal node\n",
        "        if node[\"threshold\"] is None:\n",
        "            return node\n",
        "\n",
        "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "            return self._traverse_tree(x, node[\"left_child\"])  # Navigate to the left child\n",
        "        else:\n",
        "            return self._traverse_tree(x, node[\"right_child\"])  # Navigate to the right child\n",
        "\n",
        "    def predict_rate_function(self, X):\n",
        "        \"\"\"\n",
        "        Predict the nonparametric estimates of dμ(t) = ρ(t)dt for given samples.\n",
        "        \"\"\"\n",
        "        # Ensure input is in the expected format\n",
        "        X = np.array(X)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        rate_functions = []\n",
        "        for i in range(n_samples):\n",
        "            # Traverse the tree to find the terminal node for the current sample\n",
        "            terminal_node = self._traverse_tree(X[i], self.tree_)\n",
        "\n",
        "            # Compute the nonparametric estimate for the rate function using the node_value (Nelson-Aalen estimator)\n",
        "            rate_functions.append(terminal_node['node_value'])\n",
        "\n",
        "        return rate_functions\n",
        "\n",
        "    def predict_mean_function(self, X):\n",
        "        \"\"\"\n",
        "        Predict the Nelson-Aalen estimator of the mean function for given samples.\n",
        "        \"\"\"\n",
        "        # Reuse the rate function predictions since the mean function is just the cumulative sum of the rate function\n",
        "        rate_functions = self.predict_rate_function(X)\n",
        "        mean_functions = [np.cumsum(rf) for rf in rate_functions]\n",
        "\n",
        "        return mean_functions\n",
        "\n",
        "# This refactoring should make the RecurrentTree class consistent with the changes made to the PseudoScoreTreeBuilder class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenIR9xWG9N8"
      },
      "source": [
        "## RecurrentTree\n",
        "\n",
        "1. 초기화 (__ init __ 메서드):\n",
        "\n",
        "초기화 시 최대 깊이(max_depth), 최소 리프 노드 크기(min_leaf), 그리고 난수 생성 상태(random_state)를 받습니다.\n",
        "tree_는 학습된 트리를 저장하는 변수입니다.\n",
        "\n",
        "2. 학습 (fit 메서드):\n",
        "\n",
        "주어진 데이터(X, ids, time_start, time_stop, event)를 사용하여 트리를 학습합니다.\n",
        "입력 데이터는 올바른 형식(numpy 배열)으로 변환됩니다.\n",
        "PseudoScoreTreeBuilder를 사용하여 트리를 구축합니다. 이 클래스는 위에서 제공되지 않았기 때문에 실제 코드에서는 이 부분이 작동하지 않을 것입니다.\n",
        "트리 가져오기 (get_tree 메서드):\n",
        "\n",
        "학습된 트리를 딕셔너리 형태로 반환합니다.\n",
        "\n",
        "3. 트리 순회 (_traverse_tree 메서드):\n",
        "\n",
        "주어진 샘플(x)에 대해 트리를 순회하면서 해당 샘플이 속하는 종단 노드(리프 노드)를 찾습니다.\n",
        "\n",
        "4. 위험률 함수 예측 (predict_rate_function 메서드):\n",
        "\n",
        "주어진 샘플들에 대해 비모수적 위험률 함수의 추정치인\n",
        "dμ(t)=ρ(t)dt를 예측합니다.\n",
        "\n",
        "5. 평균 함수 예측 (predict_mean_function 메서드):\n",
        "\n",
        "주어진 샘플들에 대해 Nelson-Aalen 추정치를 사용하여 평균 함수를 예측합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL0G16an23L4"
      },
      "outputs": [],
      "source": [
        "# 2. RecurrentTree 학습 및 예측\n",
        "tree_model = RecurrentTree(max_depth=5, random_state=42)\n",
        "tree_model.fit(X, ids, time_start, time_stop, event)\n",
        "\n",
        "# 비율 함수 예측\n",
        "rate_functions = tree_model.predict_rate_function(x)\n",
        "\n",
        "# 평균 함수 예측\n",
        "mean_functions = tree_model.predict_mean_function(x)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Sample Rate Functions:\", rate_functions[:5])\n",
        "print(\"Sample Mean Functions:\", mean_functions[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzKLsKYn-uPj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 첫 번째 샘플에 대한 rate function과 mean function 시각화\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(rate_functions[0], label=\"Predicted Rate Function\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Rate\")\n",
        "plt.title(\"Rate Function\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(mean_functions[0], label=\"Predicted Mean Function\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Mean\")\n",
        "plt.title(\"Mean Function\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIJxLbdSRzOJ"
      },
      "outputs": [],
      "source": [
        "%pip install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6C0ZZw7u4VM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numbers import Integral, Real\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "def _get_n_samples_bootstrap(n_ids, max_samples):\n",
        "    \"\"\"\n",
        "    Modified for recurrent events. Get the number of IDs in a bootstrap sample.\n",
        "    \"\"\"\n",
        "    if max_samples is None:\n",
        "        return n_ids\n",
        "\n",
        "    if isinstance(max_samples, Integral):\n",
        "        if max_samples > n_ids:\n",
        "            msg = \"`max_samples` must be <= n_ids={} but got value {}\"\n",
        "            raise ValueError(msg.format(n_ids, max_samples))\n",
        "        return max_samples\n",
        "\n",
        "    if isinstance(max_samples, Real):\n",
        "        return max(round(n_ids * max_samples), 1)\n",
        "\n",
        "def _generate_sample_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Sample unique IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    random_instance = check_random_state(random_state)\n",
        "    sampled_ids = np.random.choice(ids, n_ids_bootstrap, replace=True)\n",
        "    return sampled_ids\n",
        "\n",
        "def _generate_unsampled_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Determine unsampled IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    sampled_ids = _generate_sample_indices(random_state, ids, n_ids_bootstrap)\n",
        "    unsampled_ids = np.setdiff1d(ids, sampled_ids)\n",
        "\n",
        "    # Expand these unsampled IDs to include all their associated events.\n",
        "    # Again, this will depend on your data structure.\n",
        "    # As an example:\n",
        "    # unsampled_indices = np.concatenate([events_by_id[id] for id in unsampled_ids])\n",
        "\n",
        "    return unsampled_ids  # or return unsampled_indices based on your data structure\n",
        "\n",
        "\n",
        "\n",
        "from warnings import catch_warnings, simplefilter\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "def _parallel_build_trees(\n",
        "    tree,\n",
        "    bootstrap,\n",
        "    X,\n",
        "    y,\n",
        "    ids,  # New parameter: a list/array of IDs corresponding to each event in X and y\n",
        "    sample_weight,\n",
        "    tree_idx,\n",
        "    n_trees,\n",
        "    verbose=0,\n",
        "    class_weight=None,\n",
        "    n_ids_bootstrap=None,  # Instead of n_samples_bootstrap\n",
        "):\n",
        "    \"\"\"\n",
        "    Private function used to fit a single tree in parallel for recurrent events.\"\"\"\n",
        "    if verbose > 1:\n",
        "        print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n",
        "\n",
        "    if bootstrap:\n",
        "        unique_ids = np.unique(ids)\n",
        "        n_ids = len(unique_ids)\n",
        "\n",
        "        # Generate bootstrap samples using IDs\n",
        "        sampled_ids = _generate_sample_indices(\n",
        "            tree.random_state, unique_ids, n_ids_bootstrap\n",
        "        )\n",
        "\n",
        "        # Expand sampled IDs to all their associated events\n",
        "        indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "\n",
        "        if sample_weight is None:\n",
        "            curr_sample_weight = np.ones((X.shape[0],), dtype=np.float64)\n",
        "        else:\n",
        "            curr_sample_weight = sample_weight.copy()\n",
        "\n",
        "        # Adjust the sample weight based on how many times each ID was sampled\n",
        "        sample_counts_for_ids = np.bincount(np.searchsorted(unique_ids, sampled_ids), minlength=n_ids)\n",
        "        curr_sample_weight *= sample_counts_for_ids[np.searchsorted(unique_ids, ids)]\n",
        "\n",
        "        if class_weight == \"subsample\":\n",
        "            with catch_warnings():\n",
        "                simplefilter(\"ignore\", DeprecationWarning)\n",
        "                curr_sample_weight *= compute_sample_weight(\"auto\", y, indices=indices)\n",
        "        elif class_weight == \"balanced_subsample\":\n",
        "            curr_sample_weight *= compute_sample_weight(\"balanced\", y, indices=indices)\n",
        "\n",
        "        tree.fit(X[indices], y[indices], sample_weight=curr_sample_weight[indices], check_input=False)\n",
        "    else:\n",
        "        tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
        "\n",
        "    return tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Mux0ov4mQS6"
      },
      "source": [
        "1. _get_n_samples_boostrap(n_is, max_samples)\n",
        " * Recurrent events를 위해 수정된 함수\n",
        " * 부트스트랩 샘플에 포함될 ID의 개수를 반환\n",
        "2. _generate_sample_indices(random_state, ids, n_ids_bootstrap)\n",
        " * 고유한 ID들을 샘플링하고, 그 ID들과 관련된 모든 이벤트를 확장\n",
        " * 부트스트랩의 핵심 기능\n",
        "3. _generate_unsampled_indices(random_state, ids, n_ids_bootstrap)\n",
        " * 샘플링되지 않은 ID를 결정하고, 이 ID와 관련된 모든 이벤트를 확장\n",
        "4. _parallel_build_trees(...)\n",
        " * 병렬로 단일 트리를 구축하는 데 사용되는 주요 함수\n",
        " * 부트스트랩 방법을 사용하여 train data에서 샘플을 추출하고, 이 샘플을 사용하여 트리를 구축\n",
        " * 앙상블 모델에서 여러 트리를 동시에 훈련시키기 위함\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsg41jCiu4VN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.utils import check_array, check_consistent_length\n",
        "from sklearn.utils.metaestimators import available_if\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "from sksurv.exceptions import NoComparablePairException\n",
        "from sksurv.nonparametric import CensoringDistributionEstimator, SurvivalFunctionEstimator\n",
        "from sksurv.util import check_y_survival\n",
        "\n",
        "\n",
        "def _check_estimate_1d(estimate, test_time):\n",
        "    estimate = check_array(estimate, ensure_2d=False, input_name=\"estimate\")\n",
        "    if estimate.ndim != 1:\n",
        "        raise ValueError(f\"Expected 1D array, got {estimate.ndim}D array instead:\\narray={estimate}.\\n\")\n",
        "    check_consistent_length(test_time, estimate)\n",
        "    return estimate\n",
        "\n",
        "def _check_inputs(event_indicator, event_time, estimate):\n",
        "    check_consistent_length(event_indicator, event_time, estimate)\n",
        "    event_indicator = check_array(event_indicator, ensure_2d=False, input_name=\"event_indicator\")\n",
        "    event_time = check_array(event_time, ensure_2d=False, input_name=\"event_time\")\n",
        "    estimate = _check_estimate_1d(estimate, event_time)\n",
        "\n",
        "    if not np.issubdtype(event_indicator.dtype, np.bool_):\n",
        "        raise ValueError(\n",
        "            f\"only boolean arrays are supported as class labels for survival analysis, got {event_indicator.dtype}\"\n",
        "        )\n",
        "\n",
        "    if len(event_time) < 2:\n",
        "        raise ValueError(\"Need a minimum of two samples\")\n",
        "\n",
        "    if not event_indicator.any():\n",
        "        raise ValueError(\"All samples are censored\")\n",
        "\n",
        "    return event_indicator, event_time, estimate\n",
        "\n",
        "\n",
        "def _check_times(test_time, times):\n",
        "    times = check_array(np.atleast_1d(times), ensure_2d=False, input_name=\"times\")\n",
        "    times = np.unique(times)\n",
        "\n",
        "    if times.max() >= test_time.max() or times.min() < test_time.min():\n",
        "        raise ValueError(\n",
        "            f\"all times must be within follow-up time of test data: [{test_time.min()}; {test_time.max()}[\"\n",
        "        )\n",
        "\n",
        "    return times\n",
        "\n",
        "def _check_estimate_2d(estimate, test_time, time_points, estimator):\n",
        "    estimate = check_array(estimate, ensure_2d=False, allow_nd=False, input_name=\"estimate\", estimator=estimator)\n",
        "    time_points = _check_times(test_time, time_points)\n",
        "    check_consistent_length(test_time, estimate)\n",
        "\n",
        "    if estimate.ndim == 2 and estimate.shape[1] != time_points.shape[0]:\n",
        "        raise ValueError(f\"expected estimate with {time_points.shape[0]} columns, but got {estimate.shape[1]}\")\n",
        "\n",
        "    return estimate, time_points\n",
        "\n",
        "\n",
        "def _iter_comparable(event_indicator, event_time, order):\n",
        "    n_samples = len(event_time)\n",
        "    tied_time = 0\n",
        "    i = 0\n",
        "    while i < n_samples - 1:\n",
        "        time_i = event_time[order[i]]\n",
        "        end = i + 1\n",
        "        while end < n_samples and event_time[order[end]] == time_i:\n",
        "            end += 1\n",
        "\n",
        "        # check for tied event times\n",
        "        event_at_same_time = event_indicator[order[i:end]]\n",
        "        censored_at_same_time = ~event_at_same_time\n",
        "        for j in range(i, end):\n",
        "            if event_indicator[order[j]]:\n",
        "                mask = np.zeros(n_samples, dtype=bool)\n",
        "                mask[end:] = True\n",
        "                # an event is comparable to censored samples at same time point\n",
        "                mask[i:end] = censored_at_same_time\n",
        "                tied_time += censored_at_same_time.sum()\n",
        "                yield (j, mask, tied_time)\n",
        "        i = end\n",
        "\n",
        "def _estimate_recurrent_concordance_index(mu_oob, X, nRE):\n",
        "    \"\"\"\n",
        "    Compute the C-index for recurrent event data.\n",
        "\n",
        "    Parameters:\n",
        "    - mu_oob: Out-of-bag predicted risk scores\n",
        "    - X: Covariate data\n",
        "    - nRE: Number of events for each subject up to a certain time\n",
        "\n",
        "    Returns:\n",
        "    - C-index\n",
        "    \"\"\"\n",
        "    m = len(mu_oob)\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "\n",
        "    for i in range(m):\n",
        "        for j in range(m):\n",
        "            if i != j:\n",
        "                min_c = min(mu_oob[i], mu_oob[j])\n",
        "                if nRE[i][min_c] > nRE[j][min_c]:\n",
        "                    denominator += 1\n",
        "                    if mu_oob[i] > mu_oob[j]:\n",
        "                        numerator += 1\n",
        "\n",
        "    if denominator == 0:\n",
        "        return 0.5  # Just random guessing\n",
        "\n",
        "    return numerator / denominator\n",
        "\n",
        "# Calculate the Prediction Error rate\n",
        "def prediction_error_rate(cindex):\n",
        "    return 1 - cindex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhVGiEKapg5o"
      },
      "source": [
        "## C-Index\n",
        "\n",
        "1. _check_estimate_1d, _check_estimate_2d, _check_inputs, _check_times:\n",
        "\n",
        "  * 이 함수들은 입력 데이터의 유효성을 검사하는 유틸리티 함수\n",
        "  * 주어진 입력 데이터의 일관성, 차원, 형식 등을 검사하여 데이터가 예상된 형식과 일치하는지 확인\n",
        "\n",
        "2. _iter_comparable:\n",
        "  * 주어진 이벤트 시간 및 지표에 대해 비교 가능한 샘플 조합을 반복하는 제너레이터 함수.\n",
        "  * 이 함수는 생존 분석에서 두 샘플이 비교 가능한지를 결정하는 데 사용.\n",
        "\n",
        "3. _estimate_recurrent_concordance_index:\n",
        "  * 재발생 이벤트의 경우 Concordance Index (C-index)를 추정합니다.\n",
        "  * 이 함수는 각 개체의 이벤트 시간, 이벤트 횟수, 그리고 각 개체에 대한 Out-of-Bag 추정치를 입력으로 받아 C-index를 계산.\n",
        "\n",
        "4. prediction_error_rate:\n",
        "  * 주어진 C-index를 기반으로 예측 오류율을 계산합니다.\n",
        "  * 예측 오류율은 1 - C-index로 계산되며, 모델의 성능을 나타내는 또 다른 지표로 사용됨."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.utils import check_array\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import warnings\n",
        "\n",
        "class RecurrentRandomForest(BaseEstimator):\n",
        "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2,\n",
        "                 min_samples_leaf=1, bootstrap=True, oob_score=False, n_jobs=None,\n",
        "                 random_state=None, verbose=0, warm_start=False, max_samples=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.bootstrap = bootstrap\n",
        "        self.oob_score = oob_score\n",
        "        self.n_jobs = n_jobs\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.warm_start = warm_start\n",
        "        self.max_samples = max_samples\n",
        "\n",
        "        self.estimators_ = [self._make_estimator(random_state=random_state) for _ in range(self.n_estimators)]\n",
        "\n",
        "    def _make_estimator(self, random_state=None):\n",
        "        \"\"\"Make and configure a copy of the `RecurrentTree` estimator.\"\"\"\n",
        "        return RecurrentTree(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            random_state=random_state\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        \"\"\"Build a forest of survival trees from the training set (X, y).\"\"\"\n",
        "        X = self._validate_data(X, accept_sparse='csc', ensure_min_samples=2)\n",
        "\n",
        "        # Extract recurrent events data\n",
        "        ids = y['id']\n",
        "        time_start = y['time_start']\n",
        "        time_stop = y['time_stop']\n",
        "        event = y['event']\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "\n",
        "        n_samples_bootstrap = _get_n_samples_bootstrap(len(np.unique(ids)), self.max_samples)\n",
        "\n",
        "        for tree in self.estimators_:\n",
        "            if self.bootstrap:\n",
        "                unique_ids = np.unique(ids)\n",
        "                sampled_ids = _generate_sample_indices(tree.random_state, unique_ids, n_samples_bootstrap)\n",
        "                bootstrap_indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "                X_bootstrap = X[bootstrap_indices]\n",
        "                ids_bootstrap = np.array(ids)[bootstrap_indices]\n",
        "                time_start_bootstrap = np.array(time_start)[bootstrap_indices]\n",
        "                time_stop_bootstrap = np.array(time_stop)[bootstrap_indices]\n",
        "                event_bootstrap = np.array(event)[bootstrap_indices]\n",
        "            else:\n",
        "                X_bootstrap = X\n",
        "                ids_bootstrap = ids\n",
        "                time_start_bootstrap = time_start\n",
        "                time_stop_bootstrap = time_stop\n",
        "                event_bootstrap = event\n",
        "\n",
        "            tree.fit(X_bootstrap, ids_bootstrap, time_start_bootstrap, time_stop_bootstrap, event_bootstrap)\n",
        "\n",
        "        if self.oob_score:\n",
        "            self._set_oob_score_and_attributes(X, y)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _set_oob_score_and_attributes(self, X, y):\n",
        "        \"\"\"Calculate out of bag predictions and score.\"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Assuming y is a structured array with these keys.\n",
        "        ids = y['id']\n",
        "        time_start = y['time_start']\n",
        "        time_stop = y['time_stop']\n",
        "        event = y['event']\n",
        "\n",
        "        predictions = np.zeros(n_samples)\n",
        "        n_predictions = np.zeros(n_samples)\n",
        "\n",
        "        n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, self.max_samples)\n",
        "\n",
        "        for estimator in self.estimators_:\n",
        "            unsampled_indices = _generate_unsampled_indices(estimator.random_state, np.unique(ids), n_samples_bootstrap)\n",
        "            p_estimator = np.array(estimator.predict_mean_function(X[unsampled_indices, :])).mean(axis=1)\n",
        "\n",
        "            predictions[unsampled_indices] += p_estimator\n",
        "            n_predictions[unsampled_indices] += 1\n",
        "\n",
        "        if (n_predictions == 0).any():\n",
        "            warnings.warn(\n",
        "              \"Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\",\n",
        "              stacklevel=3,\n",
        "            )\n",
        "            n_predictions[n_predictions == 0] = 1\n",
        "\n",
        "        predictions /= n_predictions\n",
        "\n",
        "        # Compute the C-index\n",
        "        self.oob_prediction_ = predictions\n",
        "        # Assuming a method _estimate_recurrent_concordance_index exists to compute C-index\n",
        "        self.oob_score_ = _estimate_recurrent_concordance_index(predictions, X, event)\n",
        "\n",
        "    def _validate_data(self, X, accept_sparse=False, ensure_min_samples=1):\n",
        "        \"\"\"Validate input data.\"\"\"\n",
        "        return check_array(X, accept_sparse=accept_sparse, ensure_min_samples=ensure_min_samples)\n",
        "\n",
        "    def _validate_X_predict(self, X):\n",
        "        \"\"\"Validate X whenever one tries to predict.\"\"\"\n",
        "        X = check_array(X)\n",
        "        if X.shape[1] != self.n_features_in_:\n",
        "            raise ValueError(\"Number of features of the model must match the input. Model n_features is {} and input n_features is {}.\"\n",
        "                             .format(self.n_features_in_, X.shape[1]))\n",
        "        return X\n",
        "\n",
        "    def predict_rate_function(self, X):\n",
        "        \"\"\"\n",
        "        Predict the nonparametric estimates of dμ(t) = ρ(t)dt for given samples using the forest.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, \"estimators_\")\n",
        "        X = self._validate_X_predict(X)\n",
        "\n",
        "        rate_functions_results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n",
        "            delayed(tree.predict_rate_function)(X) for tree in self.estimators_\n",
        "        )\n",
        "\n",
        "        averaged_rate_functions = np.mean(rate_functions_results, axis=0)\n",
        "\n",
        "        return averaged_rate_functions\n",
        "\n",
        "    def predict_mean_function(self, X):\n",
        "        \"\"\"\n",
        "        Predict the Nelson-Aalen estimator of the mean function for given samples using the forest.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, \"estimators_\")\n",
        "        X = self._validate_X_predict(X)\n",
        "\n",
        "        mean_functions_results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n",
        "            delayed(tree.predict_mean_function)(X) for tree in self.estimators_\n",
        "        )\n",
        "\n",
        "        averaged_mean_functions = np.mean(mean_functions_results, axis=0)\n",
        "\n",
        "        return averaged_mean_functions\n"
      ],
      "metadata": {
        "id": "0nfxib_49tgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RandomForest for Recurrent Events\n",
        "\n",
        "1. 클래스 초기화 (__init__):\n",
        "\n",
        "  * 랜덤 포레스트의 주요 파라미터를 초기화합니다. 이러한 파라미터에는 트리의 개수(n_estimators), 최대 깊이(max_depth), 분할을 위한 최소 샘플 수(min_samples_split), 리프 노드의 최소 샘플 수(min_samples_leaf) 등이 포함됨\n",
        "  * 또한, 주어진 파라미터를 기반으로 RecurrentTree 객체를 생성하여 estimators_ 리스트에 추가\n",
        "\n",
        "2. fit 메서드:\n",
        "\n",
        "  * 주어진 입력 데이터 X와 생존 데이터 (이벤트 지표, 시작 시간, 중지 시간)를 사용하여 랜덤 포레스트를 학습시킴\n",
        "  * 각 트리는 병렬로 학습되며, 각 트리는 전체 데이터의 부트스트랩 샘플을 사용하여 학습됨\n",
        "  * Out-of-bag (OOB) 점수를 계산할 경우 _set_oob_score_and_attributes 메서드를 호출하여 OOB 예측과 C-index를 계산\n",
        "\n",
        "3. _set_oob_score_and_attributes 메서드:\n",
        "  * Out-of-bag (OOB) 예측을 계산하고, 이를 기반으로 C-index를 계산\n",
        "  * 이 메서드는 OOB 예측을 사용하여 모델의 성능을 추정하는 데 사용.\n",
        "\n",
        "4. predict_rate_function 메서드:\n",
        "\n",
        "  * 주어진 입력 데이터 X에 대한 비모수적 추정값 dμ(t)=ρ(t)dt를 예측.\n",
        "  * 각 트리로부터의 비율 함수 예측을 병렬로 수집하고, 이러한 예측을 평균하여 최종 결과를 반환.\n",
        "\n",
        "5. predict_mean_function 메서드:\n",
        "\n",
        "  * 주어진 입력 데이터 X에 대한 Nelson-Aalen estiamator의 평균 함수를 예측.\n",
        "  * 각 트리로부터의 평균 함수 예측을 병렬로 수집하고, 이러한 예측을 평균하여 최종 결과를 반환."
      ],
      "metadata": {
        "id": "8MUdf57PGaOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefining the data\n",
        "ids = [1, 1, 1, 2, 2, 3, 3, 3, 3, 4, 4, 4]\n",
        "time_start = [0, 2, 3, 0, 3, 0, 1, 2, 5, 0, 5, 8]\n",
        "time_stop = [2, 3, 7, 3, 6, 1, 2, 5, 9, 5, 8, 11]\n",
        "event = [1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0]\n",
        "X = np.array([[45, 0],\n",
        "              [45, 0],\n",
        "              [45, 0],\n",
        "              [52, 0],\n",
        "              [52, 0],\n",
        "              [65, 1],\n",
        "              [65, 1],\n",
        "              [65, 1],\n",
        "              [65, 1],\n",
        "              [53, 1],\n",
        "              [53, 1],\n",
        "              [53, 1]])"
      ],
      "metadata": {
        "id": "YUz1IN-7GTUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rrf = RecurrentRandomForest(n_estimators=10, max_depth=3, min_samples_leaf=5, random_state=1190)\n",
        "y = {\n",
        "    'id': ids,\n",
        "    'time_start': time_start,\n",
        "    'time_stop': time_stop,\n",
        "    'event': event\n",
        "}\n",
        "rrf.fit(X,y)"
      ],
      "metadata": {
        "id": "ly9w9VroGTOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the rate function and mean function for the samples\n",
        "rate_function_predictions = rrf.predict_rate_function(X)\n",
        "mean_function_predictions = rrf.predict_mean_function(X)\n",
        "\n",
        "rate_function_predictions, mean_function_predictions"
      ],
      "metadata": {
        "id": "Ufi_AWHUG4ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMSFu9jJedyB"
      },
      "outputs": [],
      "source": [
        "class PermutationImportance:\n",
        "    def __init__(self, model, n_repeats=30, random_state=None):\n",
        "        self.model = model\n",
        "        self.n_repeats = n_repeats\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _compute_baseline_cindex(self, X, event, time_stop):\n",
        "        predictions = self.model.predict_mean_function(X)\n",
        "        # Take the mean of the predictions for each individual for the C-index computation\n",
        "        mean_predictions = np.mean(predictions, axis=1)\n",
        "        return _estimate_concordance_index_recurrent(time_stop, event, mean_predictions)\n",
        "\n",
        "    def compute_importance(self, X, event, time_stop):\n",
        "        baseline_cindex = self._compute_baseline_cindex(X, event, time_stop)\n",
        "\n",
        "        rng = check_random_state(self.random_state)\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        importances = np.zeros((n_features, self.n_repeats))\n",
        "\n",
        "        for feature in range(n_features):\n",
        "            for repeat in range(self.n_repeats):\n",
        "                # Copy X and shuffle one feature\n",
        "                X_permuted = X.copy()\n",
        "                rng.shuffle(X_permuted[:, feature])\n",
        "\n",
        "                # Calculate c-index for permuted X\n",
        "                permuted_cindex = self._compute_baseline_cindex(X_permuted, event, time_stop)\n",
        "\n",
        "                # The importance is the drop in c-index\n",
        "                importances[feature, repeat] = baseline_cindex - permuted_cindex\n",
        "\n",
        "        return importances\n",
        "\n",
        "    def report_importance(self, X, event, time_stop):\n",
        "        importances = self.compute_importance(X, event, time_stop)\n",
        "\n",
        "        # Compute mean and std of importances\n",
        "        importance_mean = np.mean(importances, axis=1)\n",
        "        importance_std = np.std(importances, axis=1)\n",
        "\n",
        "        return importance_mean, importance_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbWIgPDMU82l"
      },
      "outputs": [],
      "source": [
        "imporpter = PermutationImportance(rrf, n_repeats=30, random_state=1190)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yMqBo8TVEpl"
      },
      "outputs": [],
      "source": [
        "importer._compute_baseline_cindex(X, event_recurrent, time_stop_recurrent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9JHyEydNere"
      },
      "outputs": [],
      "source": [
        "importer = PermutationImportance(rrf, n_repeats=30, random_state=1190)\n",
        "importance_mean, importance_std = importer.report_importance(X, event_recurrent, time_stop_recurrent)\n",
        "\n",
        "print(importance_mean)\n",
        "print(importance_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTBP8BVQMGeO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def EstCstat(score, N, Cis, RE, nRE):\n",
        "    den = 0\n",
        "    num = 0\n",
        "    for i in range(N-1):\n",
        "        ID1 = Cis[i, 0]\n",
        "\n",
        "        search_event = [j for j in range(len(RE)) if RE[j][2] <= Cis[i, 1]]\n",
        "\n",
        "        if search_event:\n",
        "            if len(search_event) == 1:\n",
        "                ID_COUNT = [RE[search_event[0], 0]]\n",
        "                COUNT = [1]\n",
        "            else:\n",
        "                REtemp = RE[search_event]\n",
        "                COUNT = list(REtemp[:, 0]).count\n",
        "                ID_COUNT = list(set(REtemp[:, 0]))\n",
        "\n",
        "            nREc = [0] * N\n",
        "            for idx, val in zip(ID_COUNT, COUNT):\n",
        "                nREc[idx] = val\n",
        "\n",
        "            IDpair = sorted([Cis[j, 0] for j in range(i+1, N)])\n",
        "            nREc = [nREc[idx] for idx in IDpair]\n",
        "\n",
        "            lt_obs = [1 if nRE[ID1] < val else 0 for val in nREc]\n",
        "            gt_obs = [1 if nRE[ID1] > val else 0 for val in nREc]\n",
        "            lt_pred = [1 if score[ID1] < score[idx] else 0 for idx in IDpair]\n",
        "            gt_pred = [1 if score[ID1] > score[idx] else 0 for idx in IDpair]\n",
        "\n",
        "            den += sum(lt_obs) + sum(gt_obs)\n",
        "            num += sum([lo*lp + go*gp for lo, go, lp, gp in zip(lt_obs, gt_obs, lt_pred, gt_pred)])\n",
        "\n",
        "    estcstat = num / den if den else 0\n",
        "    return estcstat, den\n",
        "\n",
        "\n",
        "# 이후에 사용할 때는 score, N, Cis, RE, nRE 값을 인자로 제공하여 함수를 호출하면 됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeNgEySltO11"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}