{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "from warnings import catch_warnings, simplefilter, warn\n",
    "import threading\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "from scipy.sparse import hstack as sparse_hstack\n",
    "from joblib import Parallel\n",
    "\n",
    "from sklearn.base import is_classifier\n",
    "from sklearn.base import ClassifierMixin, MultiOutputMixin, RegressorMixin\n",
    "\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.tree import (\n",
    "    DecisionTreeClassifier,\n",
    "    DecisionTreeRegressor,\n",
    ")\n",
    "from sklearn.tree._tree import DTYPE, DOUBLE\n",
    "from sklearn.utils import check_random_state, compute_sample_weight, deprecated\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.ensemble._base import BaseEnsemble, _partition_estimators\n",
    "from sklearn.utils.fixes import delayed\n",
    "from sklearn.utils.multiclass import check_classification_targets, type_of_target\n",
    "from sklearn.utils.validation import (\n",
    "    check_is_fitted,\n",
    "    _check_sample_weight,\n",
    ")\n",
    "from sklearn.utils.validation import _num_samples\n",
    "\n",
    "__all__ = [\n",
    "    \"RandomForestClassifier\",\n",
    "    \"RandomForestRegressor\",\n",
    "]\n",
    "\n",
    "MAX_INT = np.iinfo(np.int32).max\n",
    "\n",
    "def _get_n_samples_bootstrap(n_samples, max_samples):\n",
    "    \"\"\"\n",
    "    Get the number of samples in a bootstrap sample.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples in the dataset.\n",
    "    max_samples : int or float\n",
    "        The maximum number of samples to draw from the total available:\n",
    "            - if float, this indicates a fraction of the total and should be\n",
    "              the interval `(0.0, 1.0]`;\n",
    "            - if int, this indicates the exact number of samples;\n",
    "            - if None, this indicates the total number of samples.\n",
    "    Returns\n",
    "    -------\n",
    "    n_samples_bootstrap : int\n",
    "        The total number of samples to draw for the bootstrap sample.\n",
    "    \"\"\"\n",
    "    if max_samples is None:\n",
    "        return n_samples\n",
    "\n",
    "    if isinstance(max_samples, numbers.Integral):\n",
    "        if not (1 <= max_samples <= n_samples):\n",
    "            msg = \"`max_samples` must be in range 1 to {} but got value {}\"\n",
    "            raise ValueError(msg.format(n_samples, max_samples))\n",
    "        return max_samples\n",
    "\n",
    "    if isinstance(max_samples, numbers.Real):\n",
    "        if not (0 < max_samples <= 1):\n",
    "            msg = \"`max_samples` must be in range (0.0, 1.0] but got value {}\"\n",
    "            raise ValueError(msg.format(max_samples))\n",
    "        return round(n_samples * max_samples)\n",
    "\n",
    "    msg = \"`max_samples` should be int or float, but got type '{}'\"\n",
    "    raise TypeError(msg.format(type(max_samples)))\n",
    "\n",
    "def _generate_sample_indices(random_state, n_samples, n_samples_bootstrap):\n",
    "    \"\"\"\n",
    "    Private function used to _parallel_build_trees function.\"\"\"\n",
    "\n",
    "    random_instance = check_random_state(random_state)\n",
    "    sample_indices = random_instance.randint(0, n_samples, n_samples_bootstrap)\n",
    "\n",
    "    return sample_indices\n",
    "\n",
    "def _generate_unsampled_indices(random_state, n_samples, n_samples_bootstrap):\n",
    "    \"\"\"\n",
    "    Private function used to forest._set_oob_score function.\"\"\"\n",
    "    sample_indices = _generate_sample_indices(\n",
    "        random_state, n_samples, n_samples_bootstrap\n",
    "    )\n",
    "    sample_counts = np.bincount(sample_indices, minlength=n_samples)\n",
    "    unsampled_mask = sample_counts == 0 ##boolean --> True인 경우, unsampled_indice로 배정\n",
    "    indices_range = np.arange(n_samples)\n",
    "    unsampled_indices = indices_range[unsampled_mask]\n",
    "\n",
    "    return unsampled_indices\n",
    "\n",
    "def _parallel_build_trees(\n",
    "    tree,\n",
    "    bootstrap,\n",
    "    X,\n",
    "    y,\n",
    "    sample_weight,\n",
    "    tree_idx,\n",
    "    n_trees,\n",
    "    verbose=0,\n",
    "    class_weight=None,\n",
    "    n_samples_bootstrap=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Private function used to fit a single tree in parallel.\"\"\"\n",
    "    if verbose > 1:\n",
    "        print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n",
    "\n",
    "    if bootstrap:\n",
    "        n_samples = X.shape[0]\n",
    "        if sample_weight is None:\n",
    "            curr_sample_weight = np.ones((n_samples,), dtype=np.float64)\n",
    "        else:\n",
    "            curr_sample_weight = sample_weight.copy()\n",
    "\n",
    "        indices = _generate_sample_indices(\n",
    "            tree.random_state, n_samples, n_samples_bootstrap\n",
    "        )\n",
    "        sample_counts = np.bincount(indices, minlength=n_samples)\n",
    "        curr_sample_weight *= sample_counts\n",
    "\n",
    "        if class_weight == \"subsample\":\n",
    "            with catch_warnings():\n",
    "                simplefilter(\"ignore\", DeprecationWarning)\n",
    "                curr_sample_weight *= compute_sample_weight(\"auto\", y, indices=indices)\n",
    "        elif class_weight == \"balanced_subsample\":\n",
    "            curr_sample_weight *= compute_sample_weight(\"balanced\", y, indices=indices)\n",
    "\n",
    "        tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
    "    else:\n",
    "        tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### def _get_n_samples_bootstrap(n_samples, max_samples):\n",
    "+ sub_estimator 별 bootstrap sample의 수를 정하는 funciton\n",
    "+ max_sample로 bootstrap에 이용될 sample 수를 결정\n",
    "+ max_sample type 별 결과\n",
    "  + type: int(정수형): bootstrap sample의 수를 직접 정하는 경우\n",
    "  + type: float(실수형)\n",
    "    + 전체 sample 수 중 bootstrap sample에 사용될 sample의 fraction(비중)을 정할 때\n",
    "    + 이 때의 boostrap sample의 수는 round(bootstrap*max_samples)\n",
    "\n",
    "#### def _generate_sample_indices(random_state, n_samples, n_samples_bootstrap):\n",
    "+ sample 중 boostrap sampling에 이용될 sample의 index number\n",
    "\n",
    "#### def _generate_unsampled_indices(random_state, n_samples, n_samples_bootstrap):\n",
    "+ sample 중 bootstrap sampling에 이용되지 않는 sample의 index number\n",
    "\n",
    "#### def _parallel_build_trees(...):\n",
    "+ randomforest의 sub_estimator인 decisiontree를 병렬적으로 grow 시킬때에 이용될 함수\n",
    "  + verbose > 1: tree grow의 진행 과정을 프린트 해 줌을 의미\n",
    "  + bootstrap\n",
    "    + Case1) True인 경우(bootstrap=True): bootstrap sampling에 이용될 sample이 몇 번 replace하게 뽑혔는 지를 가중치로 두어 curr_sample_weight를 형성하고 이를 개별 tree.fit의 sample_weight로 이용한다.\n",
    "    + (Case2) False인 경우(bootstrap=False): 사전에 준비된 sample의 sample_weight를 개별 tree.fit의 sample_weight로 부여한다.\n",
    "    + classification의 경우에는 나중에..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseForest(MultiOutputMixin, BaseEnsemble, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Base class for forests of trees.\n",
    "    Warning: This class should not be used directly. Use derived classes\n",
    "    instead.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator,\n",
    "        n_estimators=100,\n",
    "        *,\n",
    "        estimator_params=tuple(),\n",
    "        bootstrap=False,\n",
    "        oob_score=False,\n",
    "        n_jobs=None,\n",
    "        random_state=None,\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        class_weight=None,\n",
    "        max_samples=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            estimator_params=estimator_params,\n",
    "        )\n",
    "\n",
    "        self.bootstrap = bootstrap\n",
    "        self.oob_score = oob_score\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_state = random_state\n",
    "        self.verbose = verbose\n",
    "        self.warm_start = warm_start\n",
    "        self.class_weight = class_weight\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "    def apply(self, X):\n",
    "        \"\"\"\n",
    "        Apply trees in the forest to X, return leaf indices.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input samples. Internally, its dtype will be converted to\n",
    "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "            converted into a sparse ``csr_matrix``.\n",
    "        Returns\n",
    "        -------\n",
    "        X_leaves : ndarray of shape (n_samples, n_estimators)\n",
    "            For each datapoint x in X and for each tree in the forest,\n",
    "            return the index of the leaf x ends up in.\n",
    "        \"\"\"\n",
    "        X = self._validate_X_predict(X)\n",
    "        results = Parallel(\n",
    "            n_jobs=self.n_jobs,\n",
    "            verbose=self.verbose,\n",
    "            prefer=\"threads\",\n",
    "        )(delayed(tree.apply)(X, check_input=False) for tree in self.estimators_)\n",
    "\n",
    "        return np.array(results).T\n",
    "\n",
    "    def decision_path(self, X):\n",
    "        \"\"\"\n",
    "        Return the decision path in the forest.\n",
    "        .. versionadded:: 0.18\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input samples. Internally, its dtype will be converted to\n",
    "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "            converted into a sparse ``csr_matrix``.\n",
    "        Returns\n",
    "        -------\n",
    "        indicator : sparse matrix of shape (n_samples, n_nodes)\n",
    "            Return a node indicator matrix where non zero elements indicates\n",
    "            that the samples goes through the nodes. The matrix is of CSR\n",
    "            format.\n",
    "        n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
    "            The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
    "            gives the indicator value for the i-th estimator.\n",
    "        \"\"\"\n",
    "        X = self._validate_X_predict(X)\n",
    "        indicators = Parallel(\n",
    "            n_jobs=self.n_jobs,\n",
    "            verbose=self.verbose,\n",
    "            prefer=\"threads\",\n",
    "        )(\n",
    "            delayed(tree.decision_path)(X, check_input=False)\n",
    "            for tree in self.estimators_\n",
    "        )\n",
    "\n",
    "        n_nodes = [0]\n",
    "        n_nodes.extend([i.shape[1] for i in indicators])\n",
    "        n_nodes_ptr = np.array(n_nodes).cumsum()\n",
    "\n",
    "        return sparse_hstack(indicators).tocsr(), n_nodes_ptr\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Build a forest of trees from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The training input samples. Internally, its dtype will be converted\n",
    "            to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "            converted into a sparse ``csc_matrix``.\n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights. If None, then samples are equally weighted. Splits\n",
    "            that would create child nodes with net zero or negative weight are\n",
    "            ignored while searching for a split in each node. In the case of\n",
    "            classification, splits are also ignored if they would result in any\n",
    "            single class carrying a negative weight in either child node.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        # Validate or convert input data\n",
    "        if issparse(y):\n",
    "            raise ValueError(\"sparse multilabel-indicator for y is not supported.\")\n",
    "        X, y = self._validate_data(\n",
    "            X, y, multi_output=True, accept_sparse=\"csc\", dtype=DTYPE\n",
    "        )\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = _check_sample_weight(sample_weight, X)\n",
    "\n",
    "        if issparse(X):\n",
    "            # Pre-sort indices to avoid that each individual tree of the\n",
    "            # ensemble sorts the indices.\n",
    "            X.sort_indices()\n",
    "\n",
    "        y = np.atleast_1d(y)\n",
    "        if y.ndim == 2 and y.shape[1] == 1:\n",
    "            warn(\n",
    "                \"A column-vector y was passed when a 1d array was\"\n",
    "                \" expected. Please change the shape of y to \"\n",
    "                \"(n_samples,), for example using ravel().\",\n",
    "                DataConversionWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "\n",
    "        if y.ndim == 1:\n",
    "            # reshape is necessary to preserve the data contiguity against vs\n",
    "            # [:, np.newaxis] that does not.\n",
    "            y = np.reshape(y, (-1, 1))\n",
    "\n",
    "        if self.criterion == \"poisson\":\n",
    "            if np.any(y < 0):\n",
    "                raise ValueError(\n",
    "                    \"Some value(s) of y are negative which is \"\n",
    "                    \"not allowed for Poisson regression.\"\n",
    "                )\n",
    "            if np.sum(y) <= 0:\n",
    "                raise ValueError(\n",
    "                    \"Sum of y is not strictly positive which \"\n",
    "                    \"is necessary for Poisson regression.\"\n",
    "                )\n",
    "\n",
    "        self.n_outputs_ = y.shape[1]\n",
    "\n",
    "        y, expanded_class_weight = self._validate_y_class_weight(y)\n",
    "\n",
    "        if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n",
    "            y = np.ascontiguousarray(y, dtype=DOUBLE)\n",
    "\n",
    "        if expanded_class_weight is not None:\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = sample_weight * expanded_class_weight\n",
    "            else:\n",
    "                sample_weight = expanded_class_weight\n",
    "\n",
    "        if not self.bootstrap and self.max_samples is not None:\n",
    "            raise ValueError(\n",
    "                \"`max_sample` cannot be set if `bootstrap=False`. \"\n",
    "                \"Either switch to `bootstrap=True` or set \"\n",
    "                \"`max_sample=None`.\"\n",
    "            )\n",
    "        elif self.bootstrap:\n",
    "            n_samples_bootstrap = _get_n_samples_bootstrap(\n",
    "                n_samples=X.shape[0], max_samples=self.max_samples\n",
    "            )\n",
    "        else:\n",
    "            n_samples_bootstrap = None\n",
    "\n",
    "        # Check parameters\n",
    "        self._validate_estimator()\n",
    "        # TODO(1.2): Remove \"mse\" and \"mae\"\n",
    "        if isinstance(self, (RandomForestRegressor)):\n",
    "            if self.criterion == \"mse\":\n",
    "                warn(\n",
    "                    \"Criterion 'mse' was deprecated in v1.0 and will be \"\n",
    "                    \"removed in version 1.2. Use `criterion='squared_error'` \"\n",
    "                    \"which is equivalent.\",\n",
    "                    FutureWarning,\n",
    "                )\n",
    "            elif self.criterion == \"mae\":\n",
    "                warn(\n",
    "                    \"Criterion 'mae' was deprecated in v1.0 and will be \"\n",
    "                    \"removed in version 1.2. Use `criterion='absolute_error'` \"\n",
    "                    \"which is equivalent.\",\n",
    "                    FutureWarning,\n",
    "                )\n",
    "\n",
    "            # TODO(1.3): Remove \"auto\"\n",
    "            if self.max_features == \"auto\":\n",
    "                warn(\n",
    "                    \"`max_features='auto'` has been deprecated in 1.1 \"\n",
    "                    \"and will be removed in 1.3. To keep the past behaviour, \"\n",
    "                    \"explicitly set `max_features=1.0` or remove this \"\n",
    "                    \"parameter as it is also the default value for \"\n",
    "                    \"RandomForestRegressors and ExtraTreesRegressors.\",\n",
    "                    FutureWarning,\n",
    "                )\n",
    "        elif isinstance(self, (RandomForestClassifier)):\n",
    "            # TODO(1.3): Remove \"auto\"\n",
    "            if self.max_features == \"auto\":\n",
    "                warn(\n",
    "                    \"`max_features='auto'` has been deprecated in 1.1 \"\n",
    "                    \"and will be removed in 1.3. To keep the past behaviour, \"\n",
    "                    \"explicitly set `max_features='sqrt'` or remove this \"\n",
    "                    \"parameter as it is also the default value for \"\n",
    "                    \"RandomForestClassifiers and ExtraTreesClassifiers.\",\n",
    "                    FutureWarning,\n",
    "                )\n",
    "\n",
    "        if not self.bootstrap and self.oob_score:\n",
    "            raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        if not self.warm_start or not hasattr(self, \"estimators_\"):\n",
    "            # Free allocated memory, if any\n",
    "            self.estimators_ = []\n",
    "\n",
    "        n_more_estimators = self.n_estimators - len(self.estimators_)\n",
    "\n",
    "        if n_more_estimators < 0:\n",
    "            raise ValueError(\n",
    "                \"n_estimators=%d must be larger or equal to \"\n",
    "                \"len(estimators_)=%d when warm_start==True\"\n",
    "                % (self.n_estimators, len(self.estimators_))\n",
    "            )\n",
    "\n",
    "        elif n_more_estimators == 0:\n",
    "            warn(\n",
    "                \"Warm-start fitting without increasing n_estimators does not \"\n",
    "                \"fit new trees.\"\n",
    "            )\n",
    "        else:\n",
    "            if self.warm_start and len(self.estimators_) > 0:\n",
    "                # We draw from the random state to get the random state we\n",
    "                # would have got if we hadn't used a warm_start.\n",
    "                random_state.randint(MAX_INT, size=len(self.estimators_))\n",
    "\n",
    "            trees = [\n",
    "                self._make_estimator(append=False, random_state=random_state)\n",
    "                for i in range(n_more_estimators)\n",
    "            ]\n",
    "\n",
    "            # Parallel loop: we prefer the threading backend as the Cython code\n",
    "            # for fitting the trees is internally releasing the Python GIL\n",
    "            # making threading more efficient than multiprocessing in\n",
    "            # that case. However, for joblib 0.12+ we respect any\n",
    "            # parallel_backend contexts set at a higher level,\n",
    "            # since correctness does not rely on using threads.\n",
    "            trees = Parallel(\n",
    "                n_jobs=self.n_jobs,\n",
    "                verbose=self.verbose,\n",
    "                prefer=\"threads\",\n",
    "            )(\n",
    "                delayed(_parallel_build_trees)(\n",
    "                    t,\n",
    "                    self.bootstrap,\n",
    "                    X,\n",
    "                    y,\n",
    "                    sample_weight,\n",
    "                    i,\n",
    "                    len(trees),\n",
    "                    verbose=self.verbose,\n",
    "                    class_weight=self.class_weight,\n",
    "                    n_samples_bootstrap=n_samples_bootstrap,\n",
    "                )\n",
    "                for i, t in enumerate(trees)\n",
    "            )\n",
    "\n",
    "            # Collect newly grown trees\n",
    "            self.estimators_.extend(trees)\n",
    "\n",
    "        if self.oob_score:\n",
    "            y_type = type_of_target(y)\n",
    "            if y_type in (\"multiclass-multioutput\", \"unknown\"):\n",
    "                # FIXME: we could consider to support multiclass-multioutput if\n",
    "                # we introduce or reuse a constructor parameter (e.g.\n",
    "                # oob_score) allowing our user to pass a callable defining the\n",
    "                # scoring strategy on OOB sample.\n",
    "                raise ValueError(\n",
    "                    \"The type of target cannot be used to compute OOB \"\n",
    "                    f\"estimates. Got {y_type} while only the following are \"\n",
    "                    \"supported: continuous, continuous-multioutput, binary, \"\n",
    "                    \"multiclass, multilabel-indicator.\"\n",
    "                )\n",
    "            self._set_oob_score_and_attributes(X, y)\n",
    "\n",
    "        # Decapsulate classes_ attributes\n",
    "        if hasattr(self, \"classes_\") and self.n_outputs_ == 1:\n",
    "            self.n_classes_ = self.n_classes_[0]\n",
    "            self.classes_ = self.classes_[0]\n",
    "\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def _set_oob_score_and_attributes(self, X, y):\n",
    "        \"\"\"Compute and set the OOB score and attributes.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The data matrix.\n",
    "        y : ndarray of shape (n_samples, n_outputs)\n",
    "            The target matrix.\n",
    "        \"\"\"\n",
    "\n",
    "    def _compute_oob_predictions(self, X, y):\n",
    "        \"\"\"Compute and set the OOB score.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The data matrix.\n",
    "        y : ndarray of shape (n_samples, n_outputs)\n",
    "            The target matrix.\n",
    "        Returns\n",
    "        -------\n",
    "        oob_pred : ndarray of shape (n_samples, n_classes, n_outputs) or \\\n",
    "                (n_samples, 1, n_outputs)\n",
    "            The OOB predictions.\n",
    "        \"\"\"\n",
    "        # Prediction requires X to be in CSR format\n",
    "        if issparse(X):\n",
    "            X = X.tocsr()\n",
    "\n",
    "        n_samples = y.shape[0]\n",
    "        n_outputs = self.n_outputs_\n",
    "        if is_classifier(self) and hasattr(self, \"n_classes_\"):\n",
    "            # n_classes_ is a ndarray at this stage\n",
    "            # all the supported type of target will have the same number of\n",
    "            # classes in all outputs\n",
    "            oob_pred_shape = (n_samples, self.n_classes_[0], n_outputs)\n",
    "        else:\n",
    "            # for regression, n_classes_ does not exist and we create an empty\n",
    "            # axis to be consistent with the classification case and make\n",
    "            # the array operations compatible with the 2 settings\n",
    "            oob_pred_shape = (n_samples, 1, n_outputs)\n",
    "\n",
    "        oob_pred = np.zeros(shape=oob_pred_shape, dtype=np.float64)\n",
    "        n_oob_pred = np.zeros((n_samples, n_outputs), dtype=np.int64)\n",
    "\n",
    "        n_samples_bootstrap = _get_n_samples_bootstrap(\n",
    "            n_samples,\n",
    "            self.max_samples,\n",
    "        )\n",
    "        for estimator in self.estimators_:\n",
    "            unsampled_indices = _generate_unsampled_indices(\n",
    "                estimator.random_state,\n",
    "                n_samples,\n",
    "                n_samples_bootstrap,\n",
    "            )\n",
    "\n",
    "            y_pred = self._get_oob_predictions(estimator, X[unsampled_indices, :])\n",
    "            oob_pred[unsampled_indices, ...] += y_pred\n",
    "            n_oob_pred[unsampled_indices, :] += 1\n",
    "\n",
    "        for k in range(n_outputs):\n",
    "            if (n_oob_pred == 0).any():\n",
    "                warn(\n",
    "                    \"Some inputs do not have OOB scores. This probably means \"\n",
    "                    \"too few trees were used to compute any reliable OOB \"\n",
    "                    \"estimates.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                n_oob_pred[n_oob_pred == 0] = 1\n",
    "            oob_pred[..., k] /= n_oob_pred[..., [k]]\n",
    "\n",
    "        return oob_pred\n",
    "\n",
    "    def _validate_y_class_weight(self, y):\n",
    "        # Default implementation\n",
    "        return y, None\n",
    "\n",
    "    def _validate_X_predict(self, X):\n",
    "        \"\"\"\n",
    "        Validate X whenever one tries to predict, apply, predict_proba.\"\"\"\n",
    "        check_is_fitted(self)\n",
    "        X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\", reset=False)\n",
    "        if issparse(X) and (X.indices.dtype != np.intc or X.indptr.dtype != np.intc):\n",
    "            raise ValueError(\"No support for np.int64 index based sparse matrices\")\n",
    "        return X\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        \"\"\"\n",
    "        The impurity-based feature importances.\n",
    "        The higher, the more important the feature.\n",
    "        The importance of a feature is computed as the (normalized)\n",
    "        total reduction of the criterion brought by that feature.  It is also\n",
    "        known as the Gini importance.\n",
    "        Warning: impurity-based feature importances can be misleading for\n",
    "        high cardinality features (many unique values). See\n",
    "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
    "        Returns\n",
    "        -------\n",
    "        feature_importances_ : ndarray of shape (n_features,)\n",
    "            The values of this array sum to 1, unless all trees are single node\n",
    "            trees consisting of only the root node, in which case it will be an\n",
    "            array of zeros.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        all_importances = Parallel(n_jobs=self.n_jobs, prefer=\"threads\")(\n",
    "            delayed(getattr)(tree, \"feature_importances_\")\n",
    "            for tree in self.estimators_\n",
    "            if tree.tree_.node_count > 1\n",
    "        )\n",
    "\n",
    "        if not all_importances:\n",
    "            return np.zeros(self.n_features_in_, dtype=np.float64)\n",
    "\n",
    "        all_importances = np.mean(all_importances, axis=0, dtype=np.float64)\n",
    "        return all_importances / np.sum(all_importances)\n",
    "\n",
    "    # TODO: Remove in 1.2\n",
    "    # mypy error: Decorated property not supported\n",
    "    @deprecated(  # type: ignore\n",
    "        \"Attribute `n_features_` was deprecated in version 1.0 and will be \"\n",
    "        \"removed in 1.2. Use `n_features_in_` instead.\"\n",
    "    )\n",
    "    @property\n",
    "    def n_features_(self):\n",
    "        \"\"\"Number of features when fitting the estimator.\"\"\"\n",
    "        return self.n_features_in_\n",
    "\n",
    "\n",
    "def _accumulate_prediction(predict, X, out, lock):\n",
    "    \"\"\"\n",
    "    This is a utility function for joblib's Parallel.\n",
    "    It can't go locally in ForestClassifier or ForestRegressor, because joblib\n",
    "    complains that it cannot pickle it when placed there.\n",
    "    \"\"\"\n",
    "    prediction = predict(X, check_input=False)\n",
    "    with lock:\n",
    "        if len(out) == 1:\n",
    "            out[0] += prediction\n",
    "        else:\n",
    "            for i in range(len(out)):\n",
    "                out[i] += prediction[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest의 attribute 함수 리뷰\n",
    "\n",
    "#### 사전 작업\n",
    "##### 1. input data의 경우 모두 dtype을 np.float32로 변환: _validate_data function\n",
    "##### 2. sparse_matrix의 경우 함수 별 matrix 타입으로 변환: csr_matrix or csc_matrix\n",
    "\n",
    "#### fit function\n",
    "+ RandomForest의 subestimator인 DecisionTree를 사전에 지정한 hyperparameter인 n_estimators(number of estimators)에 맞게 병렬적으로 grow(_parallel_build_trees가 핵심 function)\n",
    "\n",
    "+ input data and outcome\n",
    "  + input data: {array-like or sparse matrix} of shape (n_samples, n_features)\n",
    "    + 행의 수는 해당 데이터의 number of observations\n",
    "    + 열의 수는 해당 데이터의 number of features(variables)\n",
    "    + 보통의 경우 target feature(variable)은 y로 따로 빼기 때문에 사용되는 feature는 explanatory variable만 이용\n",
    "    + 한 obs index에 feature 별 element로 형성된 transposed vector의 array 결합 형태로 제시됨\n",
    "    + sparse matrix의 경우에는 element type이 np.float32(from DTYPE module)로 이루어진 csc_matrix로 변환\n",
    "  + outcome: array-like shape (n_samples, ) or (n_samples, n_outputs)\n",
    "    + target feature에 해당하는 obs가 element인 vector 형태의 array로 구성\n",
    "    + input data X와 마찬가지로 sparse matrix의 경우에는 element type이 np.float32(from DTYPE module)로 이루어진 csc_matrix로 변환\n",
    "    + y.ndim == 1이어야 함\n",
    "\n",
    "+ 그 외 구성: hyperparameter 별 warning, error의 경우 표시\n",
    "\n",
    "#### attribute funciton\n",
    "\n",
    "##### 1. apply\n",
    "+ model.fit이 이루어진 경우에만 실행 가능\n",
    "+ 열의 수: 사용된 input data의 obs의 수\n",
    "+ 행의 수: 사전에 지정된 n_estimators\n",
    "+ input data의 형태와 유사하게 도출(column이 n_estimators인 것만 다름)\n",
    "+ 각 열 별 element의 의미: DecisionTree에 이용된 sample의 index를 의미\n",
    "  \n",
    "##### 2. decision_path\n",
    "+ model.fit이 이루어진 경우에만 실행 가능\n",
    "+ indicators: sub_estimator(DecisionTree)의 index 번호를 의미\n",
    "+ n_nodes_ptr: sample_index 별로 배정된 leaf_node\n",
    "\n",
    "##### 3. oob_score \n",
    "+ model.fit이 이루어진 경우에 실행 가능\n",
    "+ bootstrap = True 일 경우에만 도출 가능\n",
    "+ get_oob_prediction을 통해 각 DecisionTree별로 y_hat[out-of-bag prediction]을 도출(sample의 element 별로 n_estimators 만큼 도출)\n",
    "+ compute_oob_prediction function을 통해 obs별로 각각의 y_hat을 산술 평균\n",
    "+ set_oob_score_and_attributes function을 통해 oob_score 도출 가능\n",
    "+ oob_prediction_: input 데이터의 Out-of-Bag 예측치(prediction) 산출\n",
    "+ oob_score: y의 true value와 Out-of-Bag prediction 간의 r2_score(결정계수?)를 산출\n",
    "  \n",
    "\n",
    "##### 4. _validate_X_predict\n",
    "+ model.fit이 이루어진 경우에 실행 가능\n",
    "+ fitting된 model을 predict할 때 이용\n",
    "+ _validate_data function을 이용하여 element의 dtype을 np.float32로 변환\n",
    "\n",
    "##### 5. feature_importance_\n",
    "+ model.fit이 이루어진 경우에 실행 가능\n",
    "+ 사전에 지정된 criterion(mse, mae, poisson deviance) 타입에 따라, 해당 feature가 빠졌을 때의 mse/mae/poisson_deviance 양의 크기에 따라서 feature_importance로 지정\n",
    "+ 총 feature_importance의 합계 대비 해당 feature의 feature_importance의 비중 만큼을 feature_importance로 지정\n",
    "+ 각 feature_importance를 element로 하는 vector 도출\n",
    "+ 참고: 논문에서 지정한 feature_importance는 permutation importance(sklearn.inspection module)\n",
    "\n",
    "#### accumulate_prediction\n",
    "+ model.predict를 진행할 때 이용하는 함수: predicted value를 추가하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForestRegressor(RegressorMixin, BaseForest, metaclass=ABCMeta):\n",
    "    \"\"\"\n",
    "    Base class for forest of trees-based regressors.\n",
    "    Warning: This class should not be used directly. Use derived classes\n",
    "    instead.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_estimator,\n",
    "        n_estimators=100,\n",
    "        *,\n",
    "        estimator_params=tuple(),\n",
    "        bootstrap=False,\n",
    "        oob_score=False,\n",
    "        n_jobs=None,\n",
    "        random_state=None,\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        max_samples=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            estimator_params=estimator_params,\n",
    "            bootstrap=bootstrap,\n",
    "            oob_score=oob_score,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose,\n",
    "            warm_start=warm_start,\n",
    "            max_samples=max_samples,\n",
    "        )\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict regression target for X.\n",
    "        The predicted regression target of an input sample is computed as the\n",
    "        mean predicted regression targets of the trees in the forest.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The input samples. Internally, its dtype will be converted to\n",
    "            ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "            converted into a sparse ``csr_matrix``.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The predicted values.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        # Check data\n",
    "        X = self._validate_X_predict(X)\n",
    "\n",
    "        # Assign chunk of trees to jobs\n",
    "        n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)\n",
    "\n",
    "        # avoid storing the output of every estimator by summing them here\n",
    "        if self.n_outputs_ > 1:\n",
    "            y_hat = np.zeros((X.shape[0], self.n_outputs_), dtype=np.float64)\n",
    "        else:\n",
    "            y_hat = np.zeros((X.shape[0]), dtype=np.float64)\n",
    "\n",
    "        # Parallel loop\n",
    "        lock = threading.Lock()\n",
    "        Parallel(n_jobs=n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n",
    "            delayed(_accumulate_prediction)(e.predict, X, [y_hat], lock)\n",
    "            for e in self.estimators_\n",
    "        )\n",
    "\n",
    "        y_hat /= len(self.estimators_)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_oob_predictions(tree, X):\n",
    "        \"\"\"Compute the OOB predictions for an individual tree.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : DecisionTreeRegressor object\n",
    "            A single decision tree regressor.\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            The OOB samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray of shape (n_samples, 1, n_outputs)\n",
    "            The OOB associated predictions.\n",
    "        \"\"\"\n",
    "        y_pred = tree.predict(X, check_input=False)\n",
    "        if y_pred.ndim == 1:\n",
    "            # single output regression\n",
    "            y_pred = y_pred[:, np.newaxis, np.newaxis]\n",
    "        else:\n",
    "            # multioutput regression\n",
    "            y_pred = y_pred[:, np.newaxis, :]\n",
    "        return y_pred\n",
    "\n",
    "    def _set_oob_score_and_attributes(self, X, y):\n",
    "        \"\"\"Compute and set the OOB score and attributes.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The data matrix.\n",
    "        y : ndarray of shape (n_samples, n_outputs)\n",
    "            The target matrix.\n",
    "        \"\"\"\n",
    "        self.oob_prediction_ = super()._compute_oob_predictions(X, y).squeeze(axis=1)\n",
    "        if self.oob_prediction_.shape[-1] == 1:\n",
    "            # drop the n_outputs axis if there is a single output\n",
    "            self.oob_prediction_ = self.oob_prediction_.squeeze(axis=-1)\n",
    "        self.oob_score_ = r2_score(y, self.oob_prediction_)\n",
    "\n",
    "    def _compute_partial_dependence_recursion(self, grid, target_features):\n",
    "        \"\"\"Fast partial dependence computation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        grid : ndarray of shape (n_samples, n_target_features)\n",
    "            The grid points on which the partial dependence should be\n",
    "            evaluated.\n",
    "        target_features : ndarray of shape (n_target_features)\n",
    "            The set of target features for which the partial dependence\n",
    "            should be evaluated.\n",
    "        Returns\n",
    "        -------\n",
    "        averaged_predictions : ndarray of shape (n_samples,)\n",
    "            The value of the partial dependence function on each grid point.\n",
    "        \"\"\"\n",
    "        grid = np.asarray(grid, dtype=DTYPE, order=\"C\")\n",
    "        averaged_predictions = np.zeros(\n",
    "            shape=grid.shape[0], dtype=np.float64, order=\"C\"\n",
    "        )\n",
    "\n",
    "        for tree in self.estimators_:\n",
    "            # Note: we don't sum in parallel because the GIL isn't released in\n",
    "            # the fast method.\n",
    "            tree.tree_.compute_partial_dependence(\n",
    "                grid, target_features, averaged_predictions\n",
    "            )\n",
    "        # Average over the forest\n",
    "        averaged_predictions /= len(self.estimators_)\n",
    "\n",
    "        return averaged_predictions\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"multilabel\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. predict\n",
    "+ model.fit을 실행하였을 경우에 실행 가능\n",
    "+ input_data(주로 test data)를 투입하였을 때, predict value를 도출하는 함수\n",
    "+ 이를 기반으로 mse, accuracy 등의 모듈을 이용하여 모델 성능을 평가할 수 있음\n",
    "+ fit function과 같이 병렬적으로 수행\n",
    "+ _accumulate_prediction 함수를 통해 작업이 진행될 때 마다 predicted value를 추가\n",
    "+ predicted value type: np.float64\n",
    "\n",
    "#### 5. compute_partial_dependence_recursion: 나중에 보충해서 설명하겠습니다.\n",
    "\n",
    "#### 6. _more_tags(): 나중에 보충해서 설명하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressor(ForestRegressor):\n",
    "    \"\"\"\n",
    "    A random forest regressor.\n",
    "    A random forest is a meta estimator that fits a number of classifying\n",
    "    decision trees on various sub-samples of the dataset and uses averaging\n",
    "    to improve the predictive accuracy and control over-fitting.\n",
    "    The sub-sample size is controlled with the `max_samples` parameter if\n",
    "    `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
    "    each tree.\n",
    "    Read more in the :ref:`User Guide <forest>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators : int, default=100\n",
    "        The number of trees in the forest.\n",
    "        .. versionchanged:: 0.22\n",
    "           The default value of ``n_estimators`` changed from 10 to 100\n",
    "           in 0.22.\n",
    "    criterion : {\"squared_error\", \"absolute_error\", \"poisson\"}, \\\n",
    "            default=\"squared_error\"\n",
    "        The function to measure the quality of a split. Supported criteria\n",
    "        are \"squared_error\" for the mean squared error, which is equal to\n",
    "        variance reduction as feature selection criterion, \"absolute_error\"\n",
    "        for the mean absolute error, and \"poisson\" which uses reduction in\n",
    "        Poisson deviance to find splits.\n",
    "        Training using \"absolute_error\" is significantly slower\n",
    "        than when using \"squared_error\".\n",
    "        .. versionadded:: 0.18\n",
    "           Mean Absolute Error (MAE) criterion.\n",
    "        .. versionadded:: 1.0\n",
    "           Poisson criterion.\n",
    "        .. deprecated:: 1.0\n",
    "            Criterion \"mse\" was deprecated in v1.0 and will be removed in\n",
    "            version 1.2. Use `criterion=\"squared_error\"` which is equivalent.\n",
    "        .. deprecated:: 1.0\n",
    "            Criterion \"mae\" was deprecated in v1.0 and will be removed in\n",
    "            version 1.2. Use `criterion=\"absolute_error\"` which is equivalent.\n",
    "    max_depth : int, default=None\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples.\n",
    "    min_samples_split : int or float, default=2\n",
    "        The minimum number of samples required to split an internal node:\n",
    "        - If int, then consider `min_samples_split` as the minimum number.\n",
    "        - If float, then `min_samples_split` is a fraction and\n",
    "          `ceil(min_samples_split * n_samples)` are the minimum\n",
    "          number of samples for each split.\n",
    "        .. versionchanged:: 0.18\n",
    "           Added float values for fractions.\n",
    "    min_samples_leaf : int or float, default=1\n",
    "        The minimum number of samples required to be at a leaf node.\n",
    "        A split point at any depth will only be considered if it leaves at\n",
    "        least ``min_samples_leaf`` training samples in each of the left and\n",
    "        right branches.  This may have the effect of smoothing the model,\n",
    "        especially in regression.\n",
    "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "        - If float, then `min_samples_leaf` is a fraction and\n",
    "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "          number of samples for each node.\n",
    "        .. versionchanged:: 0.18\n",
    "           Added float values for fractions.\n",
    "    min_weight_fraction_leaf : float, default=0.0\n",
    "        The minimum weighted fraction of the sum total of weights (of all\n",
    "        the input samples) required to be at a leaf node. Samples have\n",
    "        equal weight when sample_weight is not provided.\n",
    "    max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
    "        The number of features to consider when looking for the best split:\n",
    "        - If int, then consider `max_features` features at each split.\n",
    "        - If float, then `max_features` is a fraction and\n",
    "          `max(1, int(max_features * n_features_in_))` features are considered at each\n",
    "          split.\n",
    "        - If \"auto\", then `max_features=n_features`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None or 1.0, then `max_features=n_features`.\n",
    "        .. note::\n",
    "            The default of 1.0 is equivalent to bagged trees and more\n",
    "            randomness can be achieved by setting smaller values, e.g. 0.3.\n",
    "        .. versionchanged:: 1.1\n",
    "            The default of `max_features` changed from `\"auto\"` to 1.0.\n",
    "        .. deprecated:: 1.1\n",
    "            The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
    "            in 1.3.\n",
    "        Note: the search for a split does not stop until at least one\n",
    "        valid partition of the node samples is found, even if it requires to\n",
    "        effectively inspect more than ``max_features`` features.\n",
    "    max_leaf_nodes : int, default=None\n",
    "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
    "        Best nodes are defined as relative reduction in impurity.\n",
    "        If None then unlimited number of leaf nodes.\n",
    "    min_impurity_decrease : float, default=0.0\n",
    "        A node will be split if this split induces a decrease of the impurity\n",
    "        greater than or equal to this value.\n",
    "        The weighted impurity decrease equation is the following::\n",
    "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "        if ``sample_weight`` is passed.\n",
    "        .. versionadded:: 0.19\n",
    "    bootstrap : bool, default=True\n",
    "        Whether bootstrap samples are used when building trees. If False, the\n",
    "        whole dataset is used to build each tree.\n",
    "    oob_score : bool, default=False\n",
    "        Whether to use out-of-bag samples to estimate the generalization score.\n",
    "        Only available if bootstrap=True.\n",
    "    n_jobs : int, default=None\n",
    "        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
    "        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
    "        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
    "        context. ``-1`` means using all processors. See :term:`Glossary\n",
    "        <n_jobs>` for more details.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Controls both the randomness of the bootstrapping of the samples used\n",
    "        when building trees (if ``bootstrap=True``) and the sampling of the\n",
    "        features to consider when looking for the best split at each node\n",
    "        (if ``max_features < n_features``).\n",
    "        See :term:`Glossary <random_state>` for details.\n",
    "    verbose : int, default=0\n",
    "        Controls the verbosity when fitting and predicting.\n",
    "    warm_start : bool, default=False\n",
    "        When set to ``True``, reuse the solution of the previous call to fit\n",
    "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
    "        new forest. See :term:`the Glossary <warm_start>`.\n",
    "    ccp_alpha : non-negative float, default=0.0\n",
    "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
    "        subtree with the largest cost complexity that is smaller than\n",
    "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
    "        :ref:`minimal_cost_complexity_pruning` for details.\n",
    "        .. versionadded:: 0.22\n",
    "    max_samples : int or float, default=None\n",
    "        If bootstrap is True, the number of samples to draw from X\n",
    "        to train each base estimator.\n",
    "        - If None (default), then draw `X.shape[0]` samples.\n",
    "        - If int, then draw `max_samples` samples.\n",
    "        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
    "          `max_samples` should be in the interval `(0.0, 1.0]`.\n",
    "        .. versionadded:: 0.22\n",
    "    Attributes\n",
    "    ----------\n",
    "    base_estimator_ : DecisionTreeRegressor\n",
    "        The child estimator template used to create the collection of fitted\n",
    "        sub-estimators.\n",
    "    estimators_ : list of DecisionTreeRegressor\n",
    "        The collection of fitted sub-estimators.\n",
    "    feature_importances_ : ndarray of shape (n_features,)\n",
    "        The impurity-based feature importances.\n",
    "        The higher, the more important the feature.\n",
    "        The importance of a feature is computed as the (normalized)\n",
    "        total reduction of the criterion brought by that feature.  It is also\n",
    "        known as the Gini importance.\n",
    "        Warning: impurity-based feature importances can be misleading for\n",
    "        high cardinality features (many unique values). See\n",
    "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
    "    n_features_ : int\n",
    "        The number of features when ``fit`` is performed.\n",
    "        .. deprecated:: 1.0\n",
    "            Attribute `n_features_` was deprecated in version 1.0 and will be\n",
    "            removed in 1.2. Use `n_features_in_` instead.\n",
    "    n_features_in_ : int\n",
    "        Number of features seen during :term:`fit`.\n",
    "        .. versionadded:: 0.24\n",
    "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
    "        Names of features seen during :term:`fit`. Defined only when `X`\n",
    "        has feature names that are all strings.\n",
    "        .. versionadded:: 1.0\n",
    "    n_outputs_ : int\n",
    "        The number of outputs when ``fit`` is performed.\n",
    "    oob_score_ : float\n",
    "        Score of the training dataset obtained using an out-of-bag estimate.\n",
    "        This attribute exists only when ``oob_score`` is True.\n",
    "    oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Prediction computed with out-of-bag estimate on the training set.\n",
    "        This attribute exists only when ``oob_score`` is True.\n",
    "    See Also\n",
    "    --------\n",
    "    sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
    "    sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
    "        tree regressors.\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for the parameters controlling the size of the trees\n",
    "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
    "    unpruned trees which can potentially be very large on some data sets. To\n",
    "    reduce memory consumption, the complexity and size of the trees should be\n",
    "    controlled by setting those parameter values.\n",
    "    The features are always randomly permuted at each split. Therefore,\n",
    "    the best found split may vary, even with the same training data,\n",
    "    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
    "    of the criterion is identical for several splits enumerated during the\n",
    "    search of the best split. To obtain a deterministic behaviour during\n",
    "    fitting, ``random_state`` has to be fixed.\n",
    "    The default value ``max_features=\"auto\"`` uses ``n_features``\n",
    "    rather than ``n_features / 3``. The latter was originally suggested in\n",
    "    [1], whereas the former was more recently justified empirically in [2].\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
    "    .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
    "           trees\", Machine Learning, 63(1), 3-42, 2006.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.ensemble import RandomForestRegressor\n",
    "    >>> from sklearn.datasets import make_regression\n",
    "    >>> X, y = make_regression(n_features=4, n_informative=2,\n",
    "    ...                        random_state=0, shuffle=False)\n",
    "    >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "    >>> regr.fit(X, y)\n",
    "    RandomForestRegressor(...)\n",
    "    >>> print(regr.predict([[0, 0, 0, 0]]))\n",
    "    [-8.32987858]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        *,\n",
    "        criterion=\"squared_error\",\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        max_features=1.0,\n",
    "        max_leaf_nodes=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        bootstrap=True,\n",
    "        oob_score=False,\n",
    "        n_jobs=None,\n",
    "        random_state=None,\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        ccp_alpha=0.0,\n",
    "        max_samples=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            base_estimator=DecisionTreeRegressor(),\n",
    "            n_estimators=n_estimators,\n",
    "            estimator_params=(\n",
    "                \"criterion\",\n",
    "                \"max_depth\",\n",
    "                \"min_samples_split\",\n",
    "                \"min_samples_leaf\",\n",
    "                \"min_weight_fraction_leaf\",\n",
    "                \"max_features\",\n",
    "                \"max_leaf_nodes\",\n",
    "                \"min_impurity_decrease\",\n",
    "                \"random_state\",\n",
    "                \"ccp_alpha\",\n",
    "            ),\n",
    "            bootstrap=bootstrap,\n",
    "            oob_score=oob_score,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose,\n",
    "            warm_start=warm_start,\n",
    "            max_samples=max_samples,\n",
    "        )\n",
    "\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.ccp_alpha = ccp_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor Class\n",
    "\n",
    "#### hyperparameter 구분\n",
    "\n",
    "##### 1. Forest 형성에 이용되는 hyperparameter\n",
    "+ n_estimators: int, 사용할 sub_estimators의 수(여기서는 DecisionTreeRegressor: Regression의 경우)\n",
    "+ bootstrap: boolean, resampling 시 bootstrap sampling 사용 여부\n",
    "+ oob_score: boolean, bootstrap sampling 시 모델의 Out-of-Bag Score(r2_score) 도출, bootstrap=True일 경우에만 이용 가능\n",
    "+ n_jobs: 병렬 처리 시 사용할 cpu 코어의 개수(-1일 경우 전체 코어를 모두 사용한다))\n",
    "+ random_state: int, DecisionTreeRegressor에 지정될 seed value 설정(randint를 통해 n_estimators 만큼 난수 생성되어 각 Tree에 지정)\n",
    "+ verbose: int, tree grow 진행 상황을 보여줌(verbose>1)일 때\n",
    "+ warm_start: boolean, 아직 사용할 수 있는 estimator 수가 남아있을 때, 지속적으로 fitting을 시도하게 하는 함수(조금 더 봐야 할 것 같습니다.)\n",
    "##### 2. 각 DecisionTree를 grow 할 때 이용되는 hyperparameter(주로 이용되는 parameter 위주로)\n",
    "+ criterion: split을 할 때 이용되는 기준\n",
    "  + squared_error: mse\n",
    "  + absolute_error: mae\n",
    "  + poisson: poisson deviance\n",
    "+ max_depth: tree의 최대 깊이 지정: None일 때에는 pruning 없이 계속 grow 시도\n",
    "+ min_samples_split: 스플릿을 시도할 때 최소 sample의 개수, default = 2\n",
    "+ min_samples_leaf: 각 leaf_node 별 최소 sample의 수, default = 1\n",
    "+ max_features: 개별 decisiontree를 grow할 때 사용될 feature의 수\n",
    "  + sqrt: 전체 feature의 제곱근에 해당하는 만큼(소수점일 경우 버림) 이용\n",
    "  + log2: 전체 feature의 log2에 해당하는 만큼(소수점일 경우 버림) 이용\n",
    "  + 1.0: 전체 feature를 모두 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/my_evn/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/my_evn/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import math\n",
    "import numbers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.tree import DecisionTreeRegressor as dtr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "data = load_boston()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df.head()\n",
    "\n",
    "random_state=1190\n",
    "boston = load_boston()\n",
    "medv = boston.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, train_size=0.7, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.42500e-02, 0.00000e+00, 4.05000e+00, ..., 1.66000e+01,\n",
       "        3.95600e+02, 6.29000e+00],\n",
       "       [9.06500e-02, 2.00000e+01, 6.96000e+00, ..., 1.86000e+01,\n",
       "        3.91340e+02, 1.36500e+01],\n",
       "       [9.84900e-02, 0.00000e+00, 2.56500e+01, ..., 1.91000e+01,\n",
       "        3.79380e+02, 1.75800e+01],\n",
       "       ...,\n",
       "       [1.50234e+01, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        3.49480e+02, 2.49100e+01],\n",
       "       [4.66883e+00, 0.00000e+00, 1.81000e+01, ..., 2.02000e+01,\n",
       "        1.04800e+01, 1.90100e+01],\n",
       "       [2.06080e-01, 2.20000e+01, 5.86000e+00, ..., 1.91000e+01,\n",
       "        3.72490e+02, 1.25000e+01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._coo.coo_matrix'>\n",
      "  (0, 0)\t3\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t2\n",
      "<class 'numpy.ndarray'> \n",
      " [[3 0 1]\n",
      " [0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
    "\n",
    "print(type(sparse_coo))\n",
    "print(sparse_coo)\n",
    "dense01 = sparse_coo.toarray()\n",
    "print(type(dense01),\"\\n\",dense01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이썬 scipy sparse matrix\n",
    "COO matrix: 파이썬의 scipy에서 sparse_matrix 포현을 지원.\n",
    "$$ \\left[\n",
    "    \\begin{matrix}\n",
    "        2 & 4 & 2 & 0 \\\\\n",
    "        0 & 0 & 1 & 0 \\\\\n",
    "        0 & 0 & 0 & 5 \\\\\n",
    "    \\end{matrix}\n",
    "    \\right] $$\n",
    "\n",
    "\n",
    "+ 위 행렬: 5개 원소 제외하고는 모두 값이 0인 행렬.\n",
    "+ 0이 아닌 원소에만 주목, 각각의 원소를 3개의 리스트로 간단히 표현할 수 있다.\n",
    "+ 각 원소의 행 인덱스를 담은 리스트, 각 원소의 열 인덱스를 담은 리스트, 원소의 값을 담은 리스트 $\\rightarrow$ 온전히 행렬을 표현할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x4 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "row = [0, 0, 0, 1, 2] # 행 인덱스를 담은 리스트\n",
    "col = [0, 1, 2, 2, 3] # 열 인덱스를 담은 리스트\n",
    "data = [2, 4, 2, 1, 5] # 원소 값을 담은 리스트\n",
    "\n",
    "m = coo_matrix((data, (row, col)))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 2, 1, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 2], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSR Matrix\n",
    "\n",
    "+ coo 방식에서 행 인덱스를 나타내는 리스트는 [0, 0, 0, 1, 2]\n",
    "+ 행렬의 첫 번째 행에 0이 아닌 원소가 3개 있다는 의미\n",
    "+ 첫 번째 원소가 3개 있다는 것을 알기 위해 굳이 0을 세 번 반복할 필요는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x4 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "indices = [0, 1, 2, 2, 3]\n",
    "indptr = [0, 3, 4, 5]\n",
    "data = [2, 4, 2, 1, 5]\n",
    "m = csr_matrix((data, indices, indptr))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 2, 1, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 3], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 4, 5], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "075024aca70acbaef7a590c66e41b716ad6737fc45064b7ec5fe8fdf30d044e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
