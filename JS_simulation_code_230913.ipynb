{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSK2022/RandomForest-for-Recurrent-Events/blob/Thesis-Code/JS_simulation_code_230913.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GThv_VDEEWx0"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-survival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLhebrJVjncb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "path=\"/Users/jeongsookim/Downloads\"\n",
        "data = pd.read_csv(f\"{path}/simuDat.csv\")\n",
        "\n",
        "ids = data['ID'].values\n",
        "time_start = data['start'].values\n",
        "time_stop = data['stop'].values\n",
        "event = data['event'].values\n",
        "x = data[['group','x1','gender']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-GYzNPujncb"
      },
      "outputs": [],
      "source": [
        "# Update the RisksetCounter class again with the above mentioned changes\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "#from functools import lru_cache\n",
        "\n",
        "class RisksetCounter:\n",
        "    def __init__(self, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        클래스 초기화\n",
        "        중복 없는 고유한 time_stop 값을 정렬, all_unique_times에 저장, all_unique_times의 길이를 n_unique_times에 저장\n",
        "        n_at_risk, n_events를 0으로 초기화, set_data 호출하여 data 설정\n",
        "        \"\"\"\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "\n",
        "        self.all_unique_times = np.unique(time_stop)\n",
        "        self.n_unique_times = len(self.all_unique_times)\n",
        "\n",
        "        self.n_at_risk = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.n_events = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.set_data()\n",
        "\n",
        "    def set_data(self):\n",
        "        \"\"\"\n",
        "        all_unique_times에 있는 각 시간에 대한 Riskset과 number of events 계산\n",
        "        \"\"\"\n",
        "        self.all_unique_times = np.unique(self.time_stop)  # Update unique times based on current data\n",
        "        self.n_unique_times = len(self.all_unique_times)\n",
        "        for t_idx, t in enumerate(self.all_unique_times):\n",
        "            self.n_at_risk[t_idx] = sum([self.Y_i(id_, t_idx) for id_ in set(self.ids)])\n",
        "            self.n_events[t_idx] = sum([self.dN_bar_i(id_, t_idx) for id_ in set(self.ids)])\n",
        "\n",
        "    def Y_i(self, id_, t_idx):\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        indices = (self.ids == id_) & (time_at_t_idx <= self.time_stop)\n",
        "        return np.any(indices)\n",
        "\n",
        "    def dN_bar_i(self, id_, t_idx):\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        indices = (self.ids == id_) & (time_at_t_idx == self.time_stop) & (self.event == 1)\n",
        "        return np.any(indices)\n",
        "\n",
        "    def update(self, ids, time_start, time_stop, event):\n",
        "        # Initialize with the new values\n",
        "        self.ids = np.array(ids)\n",
        "        self.time_start = np.array(time_start)\n",
        "        self.time_stop = np.array(time_stop)\n",
        "        self.event = np.array(event)\n",
        "\n",
        "        # Reset the n_at_risk and n_events arrays to zeros\n",
        "        self.n_at_risk = np.zeros_like(self.all_unique_times)\n",
        "        self.n_events = np.zeros_like(self.all_unique_times)\n",
        "\n",
        "        # Update the n_at_risk and n_events arrays\n",
        "        for t_idx, t in enumerate(self.all_unique_times):\n",
        "            self.n_at_risk[t_idx] = sum([self.Y_i(id_, t_idx) for id_ in set(self.ids)])\n",
        "            self.n_events[t_idx] = sum([self.dN_bar_i(id_, t_idx) for id_ in set(self.ids)])\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        n_at_risk, n_events 데이터 구조를 재설정\n",
        "        \"\"\"\n",
        "        # Reset the data structures and clear the cache\n",
        "        self.n_at_risk.fill(0)\n",
        "        self.n_events.fill(0)\n",
        "        #self.Y_i.cache_clear()\n",
        "        #self.dN_bar_i.cache_clear()\n",
        "\n",
        "    def copy(self):\n",
        "        \"\"\"\n",
        "        현재 객체 복사\n",
        "        \"\"\"\n",
        "        return RisksetCounter(self.ids.copy(), self.time_start.copy(), self.time_stop.copy(), self.event.copy())\n",
        "\n",
        "    def __reduce__(self):\n",
        "        \"\"\"\n",
        "        객체의 생성자와 생성자의 인수 반환\n",
        "        \"\"\"\n",
        "        # Return a tuple of class constructor and its arguments to bypass caching\n",
        "        return (self.__class__, (self.ids, self.time_start, self.time_stop, self.event))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJzwJvqrjncc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "path=\"/Users/jeongsookim/Downloads\"\n",
        "data = pd.read_csv(f\"{path}/simuDat.csv\")\n",
        "\n",
        "ids = data['ID'].values\n",
        "time_start = data['start'].values\n",
        "time_stop = data['stop'].values\n",
        "event = data['event'].values\n",
        "x = data[['group','x1','gender']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks2iFVE1jncc"
      },
      "outputs": [],
      "source": [
        "counter = RisksetCounter(ids, time_start, time_stop, event)\n",
        "initial_n_at_risk = counter.n_at_risk.copy()\n",
        "initial_n_events = counter.n_events.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja4Pvqp3jncc"
      },
      "outputs": [],
      "source": [
        "initial_n_at_risk ##위험 집합의 수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS6OmH04jncd"
      },
      "outputs": [],
      "source": [
        "initial_n_events ## 각 위험 집합에 따른 사건 발생 횟수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AEBAF1au4VD"
      },
      "outputs": [],
      "source": [
        "def argbinsearch(arr, key_val):\n",
        "    arr_len = len(arr)\n",
        "    min_idx = 0\n",
        "    max_idx = arr_len\n",
        "\n",
        "    while min_idx < max_idx:\n",
        "        mid_idx = min_idx + ((max_idx - min_idx) // 2)\n",
        "\n",
        "        if mid_idx < 0 or mid_idx >= arr_len:\n",
        "            return -1\n",
        "\n",
        "        mid_val = arr[mid_idx]\n",
        "        if mid_val <= key_val:  # Change the condition to <=\n",
        "            min_idx = mid_idx + 1\n",
        "        else:\n",
        "            max_idx = mid_idx\n",
        "\n",
        "    return min_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH11s9dDCZLK"
      },
      "source": [
        "이 함수는 argbinsearch라는 이름의 함수로, 배열에서 주어진 키 값보다 크거나 같은 첫 번째 원소의 인덱스를 이진 탐색으로 찾아 반환합니다.\n",
        "\n",
        "자세한 코드 설명을 아래에 제공합니다:\n",
        "\n",
        "1. 입력:\n",
        "\n",
        "  * arr: 탐색 대상인 정렬된 배열\n",
        "  key_val: 찾고자 하는 키 값\n",
        "\n",
        "2. 초기 변수 설정:\n",
        "\n",
        "  * arr_len: 배열의 길이를 저장합니다.\n",
        "  * min_idx: 탐색 범위의 최솟값으로, 처음에는 배열의 시작 인덱스인 0으로 설정됩니다.\n",
        "  * max_idx: 탐색 범위의 최댓값으로, 처음에는 배열의 길이로 설정됩니다.\n",
        "\n",
        "3. 이진 탐색:\n",
        "\n",
        "  * while 루프를 사용하여 min_idx가 max_idx보다 작은 동안 탐색을 반복합니다.\n",
        "  * mid_idx: 현재 탐색 범위의 중간 인덱스를 계산합니다.\n",
        "  * mid_val: 중간 인덱스에 해당하는 배열의 원소 값을 가져옵니다.\n",
        "\n",
        "4. 키 값과 중간 값을 비교합니다:\n",
        "  * 만약 중간 값이 키 값보다 작거나 같으면, min_idx를 mid_idx + 1로 업데이트합니다. 이렇게 하면 탐색 범위의 왼쪽 부분을 제외하게 됩니다.\n",
        "  * 그렇지 않으면, max_idx를 mid_idx로 업데이트합니다. 이렇게 하면 탐색 범위의 오른쪽 부분을 제외하게 됩니다.\n",
        "\n",
        "5. 결과 반환:\n",
        "\n",
        "  * 루프가 종료되면, min_idx는 키 값보다 크거나 같은 첫 번째 원소의 인덱스를 가리키게 됩니다. 따라서 min_idx를 반환합니다.\n",
        "\n",
        "이 함수는 정렬된 배열에서 주어진 키 값보다 크거나 같은 첫 번째 원소의 위치를 효율적으로 찾기 위해 사용됩니다. 이진 탐색은 배열의 중간 값을 반복적으로 확인하면서 탐색 범위를 절반씩 줄여나가므로, 큰 배열에서도 빠르게 원하는 값을 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sn_JZBhjnce"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 및 함수 임포트\n",
        "import numpy as np\n",
        "\n",
        "class PseudoScoreCriterion:\n",
        "    def __init__(self, n_outputs, n_samples, unique_times, x, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        Initialize instance variables using the provided input parameters\n",
        "        Objects 'riskset_left', 'riskset_right', and 'riskset_total' are initialized using the 'RisksetCounter' class\n",
        "        \"\"\"\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_samples = n_samples\n",
        "        self.unique_times = unique_times\n",
        "        self.x = x\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "\n",
        "        self.unique_ids = set(self.ids)  # Store unique ids for later use\n",
        "        self.unique_times = unique_times\n",
        "\n",
        "\n",
        "        self.riskset_left = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        self.riskset_right = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        self.riskset_total = RisksetCounter(ids, time_start, time_stop, event)\n",
        "\n",
        "        self.samples_time_idx = np.searchsorted(unique_times, time_stop)\n",
        "\n",
        "        self.split_pos = 0\n",
        "        self.split_time_idx = 0\n",
        "\n",
        "    def init(self, y, sample_weight, n_samples, samples, start, end):\n",
        "        \"\"\"\n",
        "        Initialization function\n",
        "        Reset the risk set counters ('riskset_left','riskset_right','riskset_total') and updates 'riskset_total' with new data\n",
        "        \"\"\"\n",
        "        self.samples = samples\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "        self.riskset_total.reset()\n",
        "\n",
        "        time_starts, stop_times, events = y[:, 0], y[:, 1], y[:, 2]\n",
        "        ids_for_update = [self.ids[idx] for idx in samples[start:end]]\n",
        "        time_starts_for_update = [time_starts[idx] for idx in samples[start:end]]\n",
        "        stop_times_for_update = [stop_times[idx] for idx in samples[start:end]]\n",
        "        events_for_update = [events[idx] for idx in samples[start:end]]\n",
        "\n",
        "        self.riskset_total.update(ids_for_update, time_starts_for_update, stop_times_for_update, events_for_update)\n",
        "\n",
        "    def set_unique_times(self, unique_times):\n",
        "        \"\"\"Sets the unique times for the current node.\"\"\"\n",
        "        self.unique_times = unique_times\n",
        "\n",
        "    def update(self, split_count):\n",
        "        \"\"\"\n",
        "        Update the criterion based on a specified sample count.\n",
        "        This will split the data into left and right nodes based on the provided split count.\n",
        "\n",
        "        Parameters:\n",
        "            - split_count: The number of samples to be allocated to the left node.\n",
        "        \"\"\"\n",
        "        # Reset the riskset counters for the left and right nodes\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "\n",
        "        # Create empty lists to store the ids, start times, stop times, and events for both left and right splits\n",
        "        ids_left, start_left, stop_left, event_left = [], [], [], []\n",
        "        ids_right, start_right, stop_right, event_right = [], [], [], []\n",
        "\n",
        "        # Create a set to keep track of IDs that are already assigned (to ensure an ID is only in one node)\n",
        "        assigned_ids = set()\n",
        "\n",
        "        # For each unique ID, decide whether to assign it to the left or right node based on split_count\n",
        "        for id_ in self.unique_ids:\n",
        "            id_indices = np.where(self.ids == id_)[0]  # Get all indices for this ID\n",
        "\n",
        "            if len(ids_left) < split_count:\n",
        "                ids_left.extend([self.ids[i] for i in id_indices])\n",
        "                start_left.extend([self.time_start[i] for i in id_indices])\n",
        "                stop_left.extend([self.time_stop[i] for i in id_indices])\n",
        "                event_left.extend([self.event[i] for i in id_indices])\n",
        "            else:\n",
        "                ids_right.extend([self.ids[i] for i in id_indices])\n",
        "                start_right.extend([self.time_start[i] for i in id_indices])\n",
        "                stop_right.extend([self.time_stop[i] for i in id_indices])\n",
        "                event_right.extend([self.event[i] for i in id_indices])\n",
        "\n",
        "        # Update the risk sets for the left and right nodes\n",
        "        self.riskset_left.update(ids_left, start_left, stop_left, event_left)\n",
        "        self.riskset_right.update(ids_right, start_right, stop_right, event_right)\n",
        "\n",
        "\n",
        "    #@lru_cache(maxsize=None)\n",
        "        \"\"\"\n",
        "        Functions returning the risk set value and event value for the given ID and time index from the respective risk set (left or right)\n",
        "        \"\"\"\n",
        "    def Y_left_value(self, id_, t):\n",
        "        return self.riskset_left.Y_i(id_, t)\n",
        "\n",
        "    #@lru_cache(maxsize=None)\n",
        "    def Y_right_value(self, id_, t):\n",
        "        return self.riskset_right.Y_i(id_, t)\n",
        "\n",
        "    #@lru_cache(maxsize=None)\n",
        "    def dN_bar_left_value(self, id_, t):\n",
        "        return self.riskset_left.dN_bar_i(id_, t)\n",
        "\n",
        "    #@lru_cache(maxsize=None)\n",
        "    def dN_bar_right_value(self, id_, t):\n",
        "        return self.riskset_right.dN_bar_i(id_, t)\n",
        "\n",
        "    def calculate_variance_estimate(self):\n",
        "        \"\"\"\n",
        "        Functions to compute the variance estimate for the split\n",
        "        \"\"\"\n",
        "        left_n_at_risk = self.riskset_left.n_at_risk + 1e-7\n",
        "        right_n_at_risk = self.riskset_right.n_at_risk + 1e-7\n",
        "\n",
        "        w = (left_n_at_risk * right_n_at_risk) / (left_n_at_risk + right_n_at_risk)\n",
        "\n",
        "        # Expand w and n_at_risk arrays to match the size of Y_left and Y_right\n",
        "        w_expanded = np.tile(w, len(self.unique_ids))\n",
        "        left_n_at_risk_expanded = np.tile(left_n_at_risk, len(self.unique_ids))\n",
        "        right_n_at_risk_expanded = np.tile(right_n_at_risk, len(self.unique_ids))\n",
        "\n",
        "        Y_left, Y_right, term_left, term_right = [], [], [], []\n",
        "\n",
        "        for id_ in self.unique_ids:\n",
        "            for t in range(self.riskset_left.n_unique_times):\n",
        "                Y_left_val = self.Y_left_value(id_, t)\n",
        "                Y_right_val = self.Y_right_value(id_, t)\n",
        "\n",
        "                dN_bar_left_val = self.dN_bar_left_value(id_, t)\n",
        "                dN_bar_right_val = self.dN_bar_right_value(id_, t)\n",
        "\n",
        "                term_left_val = (dN_bar_left_val - (self.riskset_left.n_events[t] / left_n_at_risk[t])) ** 2\n",
        "                term_right_val = (dN_bar_right_val - (self.riskset_right.n_events[t] / right_n_at_risk[t])) ** 2\n",
        "\n",
        "                Y_left.append(Y_left_val)\n",
        "                Y_right.append(Y_right_val)\n",
        "                term_left.append(term_left_val)\n",
        "                term_right.append(term_right_val)\n",
        "\n",
        "        Y_left = np.array(Y_left)\n",
        "        Y_right = np.array(Y_right)\n",
        "        term_left = np.array(term_left)\n",
        "        term_right = np.array(term_right)\n",
        "\n",
        "        var_estimate_L = np.sum(w_expanded * (Y_left / left_n_at_risk_expanded) * term_left)\n",
        "        var_estimate_R = np.sum(w_expanded * (Y_right / right_n_at_risk_expanded) * term_right)\n",
        "\n",
        "        return var_estimate_L + var_estimate_R\n",
        "\n",
        "\n",
        "    def proxy_impurity_improvement(self):\n",
        "        \"\"\"\n",
        "        Functions that calculates the pseudo impurity improvement of the split\n",
        "        This value represents the reduction in pseudo impurity in the risk sets after the split\n",
        "        \"\"\"\n",
        "        left_n_at_risk = self.riskset_left.n_at_risk + 1e-7\n",
        "        right_n_at_risk = self.riskset_right.n_at_risk + 1e-7\n",
        "\n",
        "        w = (left_n_at_risk * right_n_at_risk) / (left_n_at_risk + right_n_at_risk)\n",
        "        term = (self.riskset_left.n_events / left_n_at_risk) - (self.riskset_right.n_events / right_n_at_risk)\n",
        "        numer = np.sum(w * term)\n",
        "        var_estimate = self.calculate_variance_estimate()\n",
        "\n",
        "        return numer / (np.sqrt(var_estimate) + 1e-7)\n",
        "\n",
        "    def node_value(self):\n",
        "        \"\"\"\n",
        "        Returns the expected risk value of the node\n",
        "        \"\"\"\n",
        "        total_n_at_risk = self.riskset_left.n_at_risk + self.riskset_right.n_at_risk + 1e-7\n",
        "        return np.cumsum(self.riskset_left.n_events + self.riskset_right.n_events) / total_n_at_risk\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Functions to reset all risk set counters\n",
        "        \"\"\"\n",
        "        self.riskset_total.reset()\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "        #self.Y_left_value.cache_clear()\n",
        "        #self.Y_right_value.cache_clear()\n",
        "        #self.dN_bar_left_value.cache_clear()\n",
        "        #self.dN_bar_right_value.cache_clear()\n",
        "\n",
        "    def copy(self):\n",
        "        \"\"\"\n",
        "        Creates and returns a copy of the current object.\n",
        "        \"\"\"\n",
        "        new_criterion = PseudoScoreCriterion(self.n_outputs, self.n_samples, self.unique_times,\n",
        "                                                     self.x, self.ids, self.time_start, self.time_stop,\n",
        "                                                     self.event)\n",
        "        new_criterion.riskset_left = self.riskset_left.copy()\n",
        "        new_criterion.riskset_right = self.riskset_right.copy()\n",
        "        new_criterion.riskset_total = self.riskset_total.copy()\n",
        "        new_criterion.samples_time_idx = self.samples_time_idx.copy()\n",
        "        if hasattr(self, 'samples'):\n",
        "            new_criterion.samples = self.samples.copy()\n",
        "\n",
        "        return new_criterion\n",
        "\n",
        "# 주어진 코드를 기반으로 수정된 PseudoScoreCriterion 클래스를 정의하였습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWr0BWuCjnce"
      },
      "outputs": [],
      "source": [
        "ids_test = data['ID'].values\n",
        "time_start_test = data['start'].values\n",
        "time_stop_test = data['stop'].values\n",
        "event_test = data['event'].values\n",
        "x_test = data['gender'].values\n",
        "n_samples_test = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_sok43ejncf"
      },
      "outputs": [],
      "source": [
        "# 2. PseudoScoreCriterion 객체 초기화 (수정된 인터페이스 사용)\n",
        "criterion_test = PseudoScoreCriterion(n_outputs=1, n_samples=n_samples_test, x=x_test,\n",
        "                                      unique_times=np.unique(time_stop_test),\n",
        "                                      ids=ids_test, time_start=time_start_test,\n",
        "                                      time_stop=time_stop_test, event=event_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYeH05kNjncf"
      },
      "outputs": [],
      "source": [
        "y_test = np.column_stack((time_start_test, time_stop_test, event_test))\n",
        "sample_weight_test = np.ones(n_samples_test)\n",
        "sample_indices_test = np.arange(n_samples_test)\n",
        "weighted_n_samples_test = sum(sample_weight_test)\n",
        "criterion_test.init(y_test, sample_weight_test, weighted_n_samples_test, sample_indices_test, 0, n_samples_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gpe2vBujncf"
      },
      "outputs": [],
      "source": [
        "split_pos_test = 62\n",
        "criterion_test.update(split_pos_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw8HIrC0jncf"
      },
      "outputs": [],
      "source": [
        "len_left = criterion_test.riskset_left\n",
        "len_right = criterion_test.riskset_right\n",
        "\n",
        "len_left, len_right"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3ds8glBjncf"
      },
      "outputs": [],
      "source": [
        "criterion_test.riskset_left.n_events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj4cF3UNjncf"
      },
      "outputs": [],
      "source": [
        "criterion_test.riskset_right.n_events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohYA5faSjncf"
      },
      "outputs": [],
      "source": [
        "len(criterion_test.riskset_left.n_at_risk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2w3o8Vsjncf"
      },
      "outputs": [],
      "source": [
        "len(criterion_test.riskset_right.n_at_risk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VndyB9zJjncf"
      },
      "outputs": [],
      "source": [
        "criterion_test.riskset_left.all_unique_times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkppHIcQjncf"
      },
      "outputs": [],
      "source": [
        "criterion_test.riskset_left.n_at_risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ_D4qmmjncf"
      },
      "outputs": [],
      "source": [
        "criterion_test.riskset_right.n_at_risk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wjpAZayjncf"
      },
      "outputs": [],
      "source": [
        "criterion_test.proxy_impurity_improvement()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1ky4EX5jncf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "def check_random_state(seed):\n",
        "    \"\"\"\n",
        "    Ensures a consistent random state based on the given 'seed'\n",
        "    If the seed is 'None', an integer, or an instance of 'np.integer', a new random state is created.\n",
        "    If the seed is an instance of 'np.random.RandomState', it's returned as is.\n",
        "    Otherwise, a 'ValueError' is raised.\n",
        "    \"\"\"\n",
        "    if seed is None or isinstance(seed, (int, np.integer)):\n",
        "        return np.random.RandomState(seed)\n",
        "    elif isinstance(seed, np.random.RandomState):\n",
        "        return seed\n",
        "    else:\n",
        "        raise ValueError(\"seed must be None, int or np.random.RandomState\")\n",
        "\n",
        "class PseudoScoreTreeBuilder:\n",
        "    \"\"\"\n",
        "    Class designed to build a decision tree based on the pseudo-score test statistics criterion, typically used in recurrent events data analysis.\n",
        "    \"\"\"\n",
        "    TREE_UNDEFINED = -1  # Placeholder\n",
        "\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
        "                 max_features=None, max_thresholds=None, min_impurity_decrease=0,\n",
        "                 random_state=None):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        Initializes the hyperparameters and settings of the tree, such as 'max_depth','min_samples_split','max_features', and the others.\n",
        "        The 'random_state' is checked and stored.\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "    def split_indices(self, X_column, threshold):\n",
        "        \"\"\"Efficiently splits the data based on the given threshold for a specific feature column (X_column).\"\"\"\n",
        "        return np.where(X_column <= threshold)[0], np.where(X_column > threshold)[0]\n",
        "\n",
        "    def _split(self, X, criterion, start, end):\n",
        "        \"\"\"\n",
        "        Finds the best feature and threshold to split on for the data in the node defined by the range [start, end].\n",
        "        Iterates over features and possible thresholds to determine the best split based on the pseudo-score test statistics criterion.\n",
        "        Returns the feature index, threshold, and improvement of the best split.\n",
        "        \"\"\"\n",
        "        best_split = {\n",
        "            'feature_index': None,\n",
        "            'threshold': None,\n",
        "            'improvement': -np.inf\n",
        "        }\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        if self.max_features is None:\n",
        "            features_to_consider = np.arange(n_features)\n",
        "        else:\n",
        "            features_to_consider = self.random_state.choice(n_features, self.max_features, replace=False)\n",
        "\n",
        "        for feature_index in features_to_consider:\n",
        "            sorted_indices = np.argsort(X[start:end, feature_index])\n",
        "            X_sorted = X[start:end][sorted_indices]\n",
        "            unique_thresholds = np.unique(X_sorted[:, feature_index])\n",
        "\n",
        "            # Modify the handling for max_thresholds\n",
        "            if self.max_thresholds is not None:\n",
        "                n_thresholds = len(unique_thresholds)\n",
        "                if isinstance(self.max_thresholds, float):\n",
        "                    n_sample_thresholds = int(n_thresholds * self.max_thresholds)\n",
        "                else:\n",
        "                    n_sample_thresholds = self.max_thresholds\n",
        "\n",
        "                if n_thresholds > n_sample_thresholds:\n",
        "                    unique_thresholds = self.random_state.choice(unique_thresholds, n_sample_thresholds, replace=False)\n",
        "\n",
        "            for threshold in unique_thresholds:\n",
        "                new_pos = np.searchsorted(X_sorted[:, feature_index], threshold, side='right')\n",
        "                criterion.update(new_pos)  # <-- 여기를 수정함\n",
        "                improvement = criterion.proxy_impurity_improvement()\n",
        "\n",
        "                if improvement > best_split['improvement']:\n",
        "                    best_split = {\n",
        "                        'feature_index': feature_index,\n",
        "                        'threshold': threshold,\n",
        "                        'improvement': improvement\n",
        "                    }\n",
        "\n",
        "                if improvement < self.min_impurity_decrease:\n",
        "                    break\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def _build(self, X, y, criterion, depth=0, start=0, end=None):\n",
        "        \"\"\"\n",
        "        Recursively builds the decision tree.\n",
        "        If the current node meets the termination criteria (e.g., maximum depth, minimum samples in the node), it returns a terminal node.\n",
        "        Otherwise, it finds the best split for the current node, splits the data accordingly, and recursively constructs the left and right subtrees.\n",
        "        Returns a dictionary representing the node and its children.\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        if end is None:\n",
        "            end = n_samples\n",
        "\n",
        "        # Conditions for terminal node\n",
        "        node_time_start = y[start:end, 0]\n",
        "        node_time_stop = y[start:end, 1]\n",
        "        unique_times = np.unique(np.concatenate([node_time_start, node_time_stop]))\n",
        "        criterion.set_unique_times(unique_times)  # Update the criterion with the unique times of this node\n",
        "\n",
        "        node_value = criterion.node_value()\n",
        "\n",
        "        if depth == self.max_depth or (end - start) <= self.min_samples_leaf or (end - start) < self.min_samples_split:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value,\n",
        "                'unique_times': unique_times.tolist()  # Store the unique times of the node\n",
        "           }\n",
        "\n",
        "        # Initialize the criterion with the samples in the current node\n",
        "        criterion.init(y, None, n_samples, np.arange(start, end), start, end)\n",
        "\n",
        "        # Find the best split\n",
        "        best_split = self._split(X, criterion, start, end)\n",
        "        if best_split['improvement'] == -np.inf:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value,\n",
        "                'unique_times': unique_times.tolist()  # Store the unique times of the node\n",
        "            }\n",
        "\n",
        "        # Split the data based on the best split\n",
        "        left_indices = np.where(X[start:end, best_split['feature_index']] <= best_split['threshold'])[0]\n",
        "        right_indices = np.where(X[start:end, best_split['feature_index']] > best_split['threshold'])[0]\n",
        "\n",
        "        # Recursively build the left and right subtrees\n",
        "        left_child = self._build(X[left_indices], y[left_indices], criterion, depth=depth+1)\n",
        "        right_child = self._build(X[right_indices], y[right_indices], criterion, depth=depth+1)\n",
        "\n",
        "        return {\n",
        "            'feature': best_split['feature_index'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left_child': left_child,\n",
        "            'right_child': right_child,\n",
        "            'node_value': node_value,\n",
        "            'unique_times': unique_times.tolist()  # Store the unique times of the node\n",
        "        }\n",
        "\n",
        "    def build(self, X, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        The main method to invoke the tree building process.\n",
        "        Initializes the pseudo-likelihood criterion using the input data and constructs the tree using the _build method.\n",
        "        Finally, converts the resulting tree dictionary into a pandas DataFrame and returns it.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        y = np.c_[time_start, time_stop, event]\n",
        "\n",
        "        unique_times = np.unique(np.concatenate([time_start, time_stop]))\n",
        "        criterion = PseudoScoreCriterion(n_outputs=n_features, n_samples=n_samples,\n",
        "                                         unique_times=unique_times, x=X, ids=ids,\n",
        "                                         time_start=time_start, time_stop=time_stop, event=event)\n",
        "\n",
        "        # Adjusting the samples_time_idx value based on PseudoScoreCriterion logic.\n",
        "        for i in range(n_samples - 1):  # Adjusted the range to prevent IndexError\n",
        "            criterion.samples_time_idx[i] = np.searchsorted(unique_times, time_stop[i])\n",
        "\n",
        "        # Build the tree\n",
        "        tree = self._build(X, y, criterion)\n",
        "\n",
        "        # Convert tree dictionary to dataframe for consistency\n",
        "        tree_df = pd.DataFrame([tree])\n",
        "        return tree_df\n",
        "# Since the PseudoScoreTreeBuilder is not directly testable (it's dependent on the state of the object),\n",
        "# we will assume the refactoring is correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th4yLwvjjncg"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kURdORXXjncg"
      },
      "outputs": [],
      "source": [
        "ids = data['ID'].values\n",
        "time_start = data['start'].values\n",
        "time_stop = data['stop'].values\n",
        "event = data['event'].values\n",
        "x = data[['group','x1','gender']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEtFt_345a0O"
      },
      "outputs": [],
      "source": [
        "tree_builder=PseudoScoreTreeBuilder(max_depth=3, min_samples_leaf=5, max_thresholds=0.5, min_impurity_decrease=0.5, random_state=1190)\n",
        "tree_df = tree_builder.build(x, ids, time_start, time_stop, event)\n",
        "\n",
        "# Display the tree dataframe\n",
        "tree_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2I_svaojncg"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "class RecurrentTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
        "                 max_features=None, max_thresholds=None, min_impurity_decrease=0,\n",
        "                 random_state=None):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        Initializes the tree's hyperparameters and settings\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.random_state = random_state\n",
        "        self.tree_ = None\n",
        "\n",
        "    def fit(self, X, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        Trains the recurrent tree using the input data\n",
        "        \"\"\"\n",
        "        # Ensure input is in the expected format\n",
        "        X = np.array(X)\n",
        "        ids = np.array(ids)\n",
        "        time_start = np.array(time_start)\n",
        "        time_stop = np.array(time_stop)\n",
        "        event = np.array(event)\n",
        "\n",
        "        # Use the PseudoScoreTreeBuilder to build the tree\n",
        "        builder = PseudoScoreTreeBuilder(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            max_features=self.max_features,\n",
        "            max_thresholds=self.max_thresholds,\n",
        "            min_impurity_decrease=self.min_impurity_decrease,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_ = builder.build(X, ids=ids, time_start=time_start, time_stop=time_stop, event=event).iloc[0]\n",
        "        return self\n",
        "\n",
        "    def get_tree(self):\n",
        "        \"\"\"Return the tree as a dictionary.\"\"\"\n",
        "        return self.tree_\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"Traverse the tree to find the terminal node for a given sample.\"\"\"\n",
        "        if node[\"threshold\"] is None:\n",
        "            return node\n",
        "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "            return self._traverse_tree(x, node[\"left_child\"])\n",
        "        else:\n",
        "            return self._traverse_tree(x, node[\"right_child\"])\n",
        "\n",
        "    def _predict_for_group(self, X_group):\n",
        "        \"\"\"Make predictions for a group of rows representing a single ID.\"\"\"\n",
        "        # Assume all rows in X_group represent the same individual and take the first row to traverse the tree\n",
        "        x = X_group[0]\n",
        "        terminal_node = self._traverse_tree(x, self.tree_)\n",
        "\n",
        "        # Extract the cumulative hazard (mean function) for the terminal node\n",
        "        mean_function = terminal_node.get('node_value', np.array([]))\n",
        "        if not mean_function.size:\n",
        "            warnings.warn(\"No function found. Using a zero array as a placeholder.\")\n",
        "            mean_function = np.zeros_like(self.tree_['node_value'])\n",
        "\n",
        "        # Compute the rate function as the difference in consecutive values of the cumulative hazard\n",
        "        rate_function = np.diff(mean_function, prepend=0)\n",
        "\n",
        "        return rate_function, mean_function\n",
        "\n",
        "    def predict_rate_function(self, X, ids):\n",
        "        \"\"\"\n",
        "        Predict the nonparametric estimates of dμ(t) = ρ(t)dt for given samples.\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        ids = np.array(ids)\n",
        "\n",
        "        unique_ids = np.unique(ids)\n",
        "        rate_functions = []\n",
        "\n",
        "        used_indices = []\n",
        "\n",
        "        for uid in unique_ids:\n",
        "            idx = np.where(ids == uid)\n",
        "            X_group = X[idx]\n",
        "            rate_function, _ = self._predict_for_group(X_group)\n",
        "            rate_functions.append(rate_function)\n",
        "            used_indices.extend(idx)\n",
        "\n",
        "        return rate_functions, used_indices\n",
        "\n",
        "    def predict_mean_function(self, X, ids):\n",
        "        \"\"\"\n",
        "        Predict the Nelson-Aalen estimator of the mean function for given samples.\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        ids = np.array(ids)\n",
        "\n",
        "        unique_ids = np.unique(ids)\n",
        "        mean_functions = []\n",
        "\n",
        "        used_indices = []\n",
        "\n",
        "        for uid in unique_ids:\n",
        "            idx = np.where(ids == uid)\n",
        "            X_group = X[idx]\n",
        "            _, mean_function = self._predict_for_group(X_group)\n",
        "            mean_functions.append(mean_function)\n",
        "            used_indices.extend(idx)\n",
        "\n",
        "        return mean_functions, used_indices\n",
        "\n",
        "    def apply(self, X, check_input=True):\n",
        "        \"\"\"Return the index of the leaf that each sample is predicted as.\"\"\"\n",
        "        if check_input:\n",
        "            X = np.array(X, dtype=np.float32)\n",
        "        return np.array([self._get_leaf_index(x, self.tree_) for x in X])\n",
        "\n",
        "    def _get_leaf_index(self, x, node, current_index=0):\n",
        "        \"\"\"Traverse the tree to find the leaf index for a given sample.\"\"\"\n",
        "        if node[\"threshold\"] is None:  # This is a leaf node\n",
        "            return current_index\n",
        "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "            return self._get_leaf_index(x, node[\"left_child\"], current_index*2 + 1)\n",
        "        else:\n",
        "            return self._get_leaf_index(x, node[\"right_child\"], current_index*2 + 2)\n",
        "\n",
        "    def _get_decision_path(self, x, node, path):\n",
        "        \"\"\"Recursively build the decision path for a sample x.\"\"\"\n",
        "        path.append(node[\"id\"])\n",
        "        if node[\"threshold\"] is None:\n",
        "            return path\n",
        "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "            return self._get_decision_path(x, node[\"left_child\"], path)\n",
        "        else:\n",
        "            return self._get_decision_path(x, node[\"right_child\"], path)\n",
        "\n",
        "    def decision_path(self, X, check_input=True):\n",
        "        \"\"\"Return the decision path in the tree.\"\"\"\n",
        "        if check_input:\n",
        "            X = np.array(X, dtype=np.float32)\n",
        "\n",
        "        n_samples, n_nodes = len(X), self.tree_[\"node_count\"]\n",
        "        data, indices, indptr = [], [], [0]\n",
        "\n",
        "        for x in X:\n",
        "            path = self._get_decision_path(x, self.tree_, [])\n",
        "            data.extend([1] * len(path))\n",
        "            indices.extend(path)\n",
        "            indptr.append(len(indices))\n",
        "\n",
        "        return csr_matrix((data, indices, indptr), shape=(n_samples, n_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL0G16an23L4"
      },
      "outputs": [],
      "source": [
        "# 2. RecurrentTree 학습 및 예측\n",
        "tree_model = RecurrentTree(max_depth=5, min_samples_leaf=15, max_thresholds=0.5, min_impurity_decrease=0.5, random_state=1190)\n",
        "tree_model.fit(x, ids, time_start, time_stop, event)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIJxLbdSRzOJ"
      },
      "outputs": [],
      "source": [
        "%pip install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmRkknjQjnch"
      },
      "outputs": [],
      "source": [
        "tree_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSibIJ0Fjnch"
      },
      "outputs": [],
      "source": [
        "tree_model.apply(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJc9nHJ0jnch"
      },
      "outputs": [],
      "source": [
        "tree_model._predict_for_group(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjaJWSkxjnch"
      },
      "outputs": [],
      "source": [
        "tree_model.predict_mean_function(x,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvkR_qaOjnch"
      },
      "outputs": [],
      "source": [
        "tree_model.predict_mean_function(x,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4SDHeN3jnch"
      },
      "outputs": [],
      "source": [
        "tree_model.predict_mean_function(x,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yeMx4ZFjnch"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbIQ_ckGjnch"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgJjU2Vhjnch"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDP8foPNjnch"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id5m4R0Ojnci"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puKE-AQdjnci"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEvLiXA_jnci"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmsKDTySjnci"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6C0ZZw7u4VM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numbers import Integral, Real\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "def _get_n_samples_bootstrap(n_ids, max_samples):\n",
        "    \"\"\"\n",
        "    Modified for recurrent events. Get the number of IDs in a bootstrap sample.\n",
        "    \"\"\"\n",
        "    if max_samples is None:\n",
        "        return n_ids\n",
        "\n",
        "    if isinstance(max_samples, Integral):\n",
        "        if max_samples > n_ids:\n",
        "            msg = \"`max_samples` must be <= n_ids={} but got value {}\"\n",
        "            raise ValueError(msg.format(n_ids, max_samples))\n",
        "        return max_samples\n",
        "\n",
        "    if isinstance(max_samples, Real):\n",
        "        return max(round(n_ids * max_samples), 1)\n",
        "\n",
        "def _generate_sample_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Sample unique IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    random_instance = check_random_state(random_state)\n",
        "    sampled_ids = np.random.choice(ids, n_ids_bootstrap, replace=True)\n",
        "    return sampled_ids\n",
        "\n",
        "def _generate_unsampled_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Determine unsampled IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    unique_ids = np.unique(ids)\n",
        "    sampled_ids = _generate_sample_indices(random_state, unique_ids, n_ids_bootstrap)\n",
        "    unique_sampled_ids = np.unique(sampled_ids)  # Ensure sampled IDs are unique\n",
        "    unsampled_ids = np.setdiff1d(unique_ids, unique_sampled_ids)\n",
        "\n",
        "    # Expand these unsampled IDs to include all their associated events.\n",
        "    unsampled_indices = np.where(np.isin(ids, unsampled_ids))[0]\n",
        "\n",
        "    return unsampled_indices\n",
        "\n",
        "from warnings import catch_warnings, simplefilter\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "def _parallel_build_trees(\n",
        "    tree,\n",
        "    bootstrap,\n",
        "    X,\n",
        "    y,  # Now, y is expected to be a dictionary with 'id', 'time_start', 'time_stop', and 'event' as keys\n",
        "    sample_weight,\n",
        "    tree_idx,\n",
        "    n_trees,\n",
        "    verbose=0,\n",
        "    class_weight=None,\n",
        "    n_ids_bootstrap=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Private function used to fit a single tree in parallel for recurrent events.\n",
        "    \"\"\"\n",
        "    # Extract necessary data from y\n",
        "    ids = y['id']\n",
        "    time_start = y['time_start']\n",
        "    time_stop = y['time_stop']\n",
        "    event = y['event']\n",
        "\n",
        "    if verbose > 1:\n",
        "        print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n",
        "\n",
        "    if bootstrap:\n",
        "        unique_ids = np.unique(ids)\n",
        "        n_ids = len(unique_ids)\n",
        "\n",
        "        # Generate bootstrap samples using IDs\n",
        "        sampled_ids = _generate_sample_indices(\n",
        "            tree.random_state, unique_ids, n_ids_bootstrap\n",
        "        )\n",
        "\n",
        "        # Expand sampled IDs to all their associated events\n",
        "        indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "\n",
        "        if sample_weight is None:\n",
        "            curr_sample_weight = np.ones((X.shape[0],), dtype=np.float64)\n",
        "        else:\n",
        "            curr_sample_weight = sample_weight.copy()\n",
        "\n",
        "        # Adjust the sample weight based on how many times each ID was sampled\n",
        "        sample_counts_for_ids = np.bincount(np.searchsorted(unique_ids, sampled_ids), minlength=n_ids)\n",
        "        curr_sample_weight *= sample_counts_for_ids[np.searchsorted(unique_ids, ids)]\n",
        "\n",
        "        if class_weight == \"subsample\":\n",
        "            with catch_warnings():\n",
        "                simplefilter(\"ignore\", DeprecationWarning)\n",
        "                curr_sample_weight *= compute_sample_weight(\"auto\", event, indices=indices)\n",
        "        elif class_weight == \"balanced_subsample\":\n",
        "            curr_sample_weight *= compute_sample_weight(\"balanced\", event, indices=indices)\n",
        "\n",
        "        tree.fit(X[indices], {'id': ids[indices], 'time_start': time_start[indices], 'time_stop': time_stop[indices], 'event': event[indices]}, sample_weight=curr_sample_weight[indices], check_input=False)\n",
        "    else:\n",
        "        tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
        "\n",
        "    return tree\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nfxib_49tgG"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.utils import check_array\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "class RecurrentRandomForest(BaseEstimator):\n",
        "    \"\"\"\n",
        "    A Random Forest model designed for recurrent event data.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2,\n",
        "                 min_samples_leaf=1, bootstrap=True, oob_score=False, n_jobs=None,\n",
        "                 random_state=None, verbose=0, warm_start=False, max_samples=None,\n",
        "                 min_impurity_decrease=0.0, max_features=None):  # Add new parameters\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.bootstrap = bootstrap\n",
        "        self.oob_score = oob_score\n",
        "        self.n_jobs = n_jobs\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.warm_start = warm_start\n",
        "        self.max_samples = max_samples\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.max_features = max_features\n",
        "        self.estimators_ = [self._make_estimator(random_state=i) for i in range(self.n_estimators)]\n",
        "\n",
        "    def _make_estimator(self, random_state=None):\n",
        "        \"\"\"\n",
        "        Constructs a new instances of the 'RecurrentTree' with the specified hyperparameters\n",
        "        Allows for creating each tree with a different 'random_state' for randomness\n",
        "        \"\"\"\n",
        "        return RecurrentTree(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            random_state=random_state,\n",
        "            min_impurity_decrease=self.min_impurity_decrease,  # Pass the new parameter\n",
        "            max_features=self.max_features  # Pass the new parameter\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Trains the random forest using the input data\n",
        "        \"\"\"\n",
        "        X = self._validate_data(X, accept_sparse='csc', ensure_min_samples=2)\n",
        "        ids = y['id']\n",
        "        time_start = y['time_start']\n",
        "        time_stop = y['time_stop']\n",
        "        event = y['event']\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "\n",
        "        n_samples_bootstrap = _get_n_samples_bootstrap(len(np.unique(ids)), self.max_samples)\n",
        "\n",
        "        def _fit_tree(tree):\n",
        "            if self.bootstrap:\n",
        "                unique_ids = np.unique(ids)\n",
        "                sampled_ids = _generate_sample_indices(tree.random_state, unique_ids, n_samples_bootstrap)\n",
        "                bootstrap_indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "                X_bootstrap = X[bootstrap_indices]\n",
        "                ids_bootstrap = np.array(ids)[bootstrap_indices]\n",
        "                time_start_bootstrap = np.array(time_start)[bootstrap_indices]\n",
        "                time_stop_bootstrap = np.array(time_stop)[bootstrap_indices]\n",
        "                event_bootstrap = np.array(event)[bootstrap_indices]\n",
        "            else:\n",
        "                X_bootstrap = X\n",
        "                ids_bootstrap = ids\n",
        "                time_start_bootstrap = time_start\n",
        "                time_stop_bootstrap = time_stop\n",
        "                event_bootstrap = event\n",
        "\n",
        "            tree.fit(X_bootstrap, ids_bootstrap, time_start_bootstrap, time_stop_bootstrap, event_bootstrap)\n",
        "            return tree\n",
        "\n",
        "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(_fit_tree)(tree) for tree in self.estimators_\n",
        "        )\n",
        "\n",
        "        for estimator in self.estimators_:\n",
        "            if hasattr(estimator, 'riskset_counter') and estimator.riskset_counter is not None:\n",
        "                estimator.riskset_counter.reset()\n",
        "\n",
        "        if self.oob_score:\n",
        "            self._set_oob_score_and_attributes(X, y)\n",
        "        return self\n",
        "\n",
        "    def _set_oob_score_and_attributes(self, X, y):\n",
        "        \"\"\"\n",
        "        Calculates the out-of-bag (OOB) scores using the ensemble's predictions for the training data samples that were not seen during the training of a given tree.\n",
        "        Also sets the 'oob_prediction_' and 'oob_score_' attributes of the class.\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Assuming y is a structured array with these keys.\n",
        "        ids = y['id']\n",
        "        time_start = y['time_start']\n",
        "        time_stop = y['time_stop']\n",
        "        event = y['event']\n",
        "\n",
        "        # Calculate total number of events for each ID\n",
        "        total_events = {}\n",
        "        for i in range(n_samples):\n",
        "            total_events[ids[i]] = total_events.get(ids[i], 0) + event[i]\n",
        "\n",
        "        all_predictions = {uid: [] for uid in np.unique(ids)}\n",
        "        n_predictions = np.zeros(n_samples)\n",
        "\n",
        "        n_samples_bootstrap = _get_n_samples_bootstrap(len(np.unique(ids)), self.max_samples)\n",
        "        max_shape = None\n",
        "\n",
        "        for estimator in self.estimators_:\n",
        "            unsampled_indices = _generate_unsampled_indices(estimator.random_state, np.unique(ids), n_samples_bootstrap)\n",
        "            p_estimator_result, _ = estimator.predict_mean_function(X[unsampled_indices, :], unsampled_indices)\n",
        "\n",
        "            # Update the max shape if necessary\n",
        "            current_shape = np.array(p_estimator_result).shape\n",
        "            if max_shape is None or current_shape[1] > max_shape[1]:\n",
        "                max_shape = current_shape\n",
        "\n",
        "            for idx, uid in enumerate(ids[unsampled_indices]):\n",
        "                all_predictions[uid].append(p_estimator_result[idx])\n",
        "                n_predictions[unsampled_indices[idx]] += 1\n",
        "\n",
        "        # Adjust the shape of predictions\n",
        "        for uid in all_predictions:\n",
        "            for i in range(len(all_predictions[uid])):\n",
        "                if np.array(all_predictions[uid][i]).shape[0] < max_shape[1]:\n",
        "                    padding = np.zeros(max_shape[1] - np.array(all_predictions[uid][i]).shape[0])\n",
        "                    all_predictions[uid][i] = np.concatenate([all_predictions[uid][i], padding])\n",
        "\n",
        "        predictions = {}\n",
        "        for uid in all_predictions:\n",
        "            if all_predictions[uid]:\n",
        "                predictions[uid] = np.mean(all_predictions[uid], axis=0)\n",
        "\n",
        "        # Convert the dictionary into a list to match the original structure\n",
        "        final_predictions = np.array([predictions.get(uid, np.zeros(max_shape[1])) for uid in ids])\n",
        "\n",
        "\n",
        "        self.oob_prediction_ = final_predictions\n",
        "\n",
        "        # Pass the calculated total_events to the method\n",
        "        self.oob_score_ = self._estimate_recurrent_concordance_index(final_predictions, X, event, ids, total_events)\n",
        "\n",
        "    def _estimate_recurrent_concordance_index(self, predictions, X, event, ids, total_events):\n",
        "        \"\"\"\n",
        "        Estimate the C-index for recurrent events using OOB ensemble estimates for right-censored data.\n",
        "\n",
        "        Parameters:\n",
        "        - predictions: Predicted mean functions for all samples using OOB.\n",
        "        - X: The data matrix.\n",
        "        - event: Observed recurrent events for all samples.\n",
        "        - ids: IDs for each event.\n",
        "        - total_events: Total number of events for each ID.\n",
        "\n",
        "        Returns:\n",
        "        - C-index estimate.\n",
        "        \"\"\"\n",
        "        unique_ids = np.unique(ids)\n",
        "        n_unique_ids = len(unique_ids)\n",
        "        id_to_avg_prediction = {uid: np.mean(predictions[ids == uid]) for uid in unique_ids}\n",
        "\n",
        "        concordant_pairs = 0\n",
        "        permissible_pairs = 0\n",
        "\n",
        "        for i in range(n_unique_ids):\n",
        "            for j in range(i+1, n_unique_ids):\n",
        "                uid_i = unique_ids[i]\n",
        "                uid_j = unique_ids[j]\n",
        "\n",
        "                right_censored_i = event[ids == uid_i][0] < total_events[uid_i]\n",
        "                right_censored_j = event[ids == uid_j][0] < total_events[uid_j]\n",
        "\n",
        "                if not right_censored_i and not right_censored_j:  # Both are not right-censored\n",
        "                    if event[ids == uid_i][0] > event[ids == uid_j][0]:\n",
        "                        permissible_pairs += 1\n",
        "                        if id_to_avg_prediction[uid_i] > id_to_avg_prediction[uid_j]:\n",
        "                            concordant_pairs += 1\n",
        "                else:  # At least one is right-censored\n",
        "                    if not right_censored_i:  # i is not right-censored but j is\n",
        "                        permissible_pairs += 1\n",
        "                        if id_to_avg_prediction[uid_i] > id_to_avg_prediction[uid_j]:\n",
        "                            concordant_pairs += 1\n",
        "                    elif not right_censored_j:  # j is not right-censored but i is\n",
        "                        permissible_pairs += 1\n",
        "                        if id_to_avg_prediction[uid_i] < id_to_avg_prediction[uid_j]:\n",
        "                            concordant_pairs += 1\n",
        "\n",
        "        c_index = concordant_pairs / permissible_pairs if permissible_pairs > 0 else 0\n",
        "        return 2 * c_index\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"Validate input data('X') to ensure it's in the correct format and meets the necessary conditions for processing.\"\"\"\n",
        "    def _validate_data(self, X, accept_sparse=False, ensure_min_samples=1):\n",
        "        \"\"\"Validate input data('X') to ensure it's in the correct format and meets the necessary conditions for processing.\"\"\"\n",
        "        return check_array(X, accept_sparse=accept_sparse, ensure_min_samples=ensure_min_samples)\n",
        "\n",
        "    def _validate_X_predict(self, X):\n",
        "        \"\"\"Validate X whenever one tries to predict.\"\"\"\n",
        "        X = check_array(X)\n",
        "        if X.shape[1] != self.n_features_in_:\n",
        "            raise ValueError(\"Number of features of the model must match the input. Model n_features is {} and input n_features is {}.\"\n",
        "                             .format(self.n_features_in_, X.shape[1]))\n",
        "        return X\n",
        "\n",
        "\n",
        "    def predict_rate_function(self, X, ids):\n",
        "        X = self._validate_X_predict(X)\n",
        "        all_predictions = {uid: [] for uid in np.unique(ids)}\n",
        "\n",
        "        for tree in self.estimators_:\n",
        "            tree_predictions = tree.apply(X)\n",
        "            rate_predictions = [tree.tree_.value[node][0][0] for node in tree_predictions]\n",
        "\n",
        "            for idx, uid in enumerate(ids):\n",
        "                all_predictions[uid].append(rate_predictions[idx])\n",
        "\n",
        "        # Average the predictions for each ID\n",
        "        averaged_predictions = {uid: np.mean(all_predictions[uid]) for uid in all_predictions}\n",
        "        return averaged_predictions\n",
        "\n",
        "    def predict_mean_function(self, X, ids):\n",
        "        X = self._validate_X_predict(X)\n",
        "        all_predictions = {uid: [] for uid in np.unique(ids)}\n",
        "\n",
        "        for tree in self.estimators_:\n",
        "            tree_predictions = tree.apply(X)\n",
        "            mean_predictions = [tree.tree_.value[node][0][1] for node in tree_predictions]\n",
        "\n",
        "            for idx, uid in enumerate(ids):\n",
        "                all_predictions[uid].append(mean_predictions[idx])\n",
        "\n",
        "        # Average the predictions for each ID\n",
        "        averaged_predictions = {uid: np.mean(all_predictions[uid]) for uid in all_predictions}\n",
        "        return averaged_predictions\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly9w9VroGTOB"
      },
      "outputs": [],
      "source": [
        "rrf = RecurrentRandomForest(n_estimators=10, max_depth=3, min_samples_leaf=5, min_impurity_decrease=0.2, random_state=42, oob_score=True, n_jobs=6)\n",
        "y = {\n",
        "    'id': ids,\n",
        "    'time_start': time_start,\n",
        "    'time_stop': time_stop,\n",
        "    'event': event\n",
        "}\n",
        "rrf.fit(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufi_AWHUG4ap"
      },
      "outputs": [],
      "source": [
        "rrf.predict_mean_function(X=x, ids=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD1GX-83jncj"
      },
      "outputs": [],
      "source": [
        "rrf.predict_mean_function(X=x, ids=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCOCSJ9PbJpf"
      },
      "outputs": [],
      "source": [
        "rrf.oob_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMSFu9jJedyB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "보류\n",
        "\"\"\"\n",
        "\n",
        "class PermutationImportance:\n",
        "    def __init__(self, model, n_repeats=30, random_state=None):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        'model': The trained model for which we want to compute feature importances\n",
        "        'n_repeats': Number of times to repeat the permutation for each feature to get a reliable estimate.\n",
        "        'random_state': Seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.n_repeats = n_repeats\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _compute_baseline_cindex(self, X, event, time_stop, ids):\n",
        "        \"\"\"\n",
        "        Computes the baseline C-index using the original (non-permuted) data.\n",
        "        The C-index is a metric to evaluate the model's predictions, especially for recurrent events.\n",
        "        This function first predicts the mean function using the model, averages the predictions, and then calculates the C-index.\n",
        "        \"\"\"\n",
        "        # Get predictions using the model\n",
        "        predictions = self.model.predict_mean_function(X)\n",
        "\n",
        "        # Take the mean of the predictions for each individual for the C-index computation\n",
        "        mean_predictions = np.mean(predictions, axis=1)\n",
        "\n",
        "        # Compute total number of events for each ID\n",
        "        unique_ids, total_events = np.unique(ids, return_counts=True)\n",
        "        total_events_dict = dict(zip(unique_ids, total_events))\n",
        "        total_events_arr = np.array([total_events_dict[id_] for id_ in ids])\n",
        "\n",
        "        return self._estimate_concordance_index_recurrent(time_stop, event, mean_predictions, ids, total_events_arr)\n",
        "\n",
        "    def _estimate_concordance_index_recurrent(self, time_stop, event, predictions, ids, total_events):\n",
        "        \"\"\"\n",
        "        A wrapper around the model's method to estimate the recurrent C-index.\n",
        "        It's used for convenience and to make the code more readable.\n",
        "        \"\"\"\n",
        "        return self.model._estimate_recurrent_concordance_index(predictions, time_stop, event, ids, total_events)\n",
        "\n",
        "    def compute_importance(self, X, event, time_stop, ids):\n",
        "        \"\"\"\n",
        "        The main function that computes the feature importances\n",
        "        For each feature:\n",
        "            It permutes (shuffles) the feature's values a number of times (specified by 'n_repeats')\n",
        "            For each permutation, it calculates the drop in C-index (compared to the baseline C-index) due to the permutation\n",
        "            The drop in performance (C-index) due to the permutation gives an indicartion of the feature's importance.\n",
        "        It returns the computed importances matrix where each row corresponds to a feature and each column to a permutation repitition\n",
        "        \"\"\"\n",
        "        baseline_cindex = self._compute_baseline_cindex(X, event, time_stop, ids)\n",
        "\n",
        "        rng = check_random_state(self.random_state)\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        importances = np.zeros((n_features, self.n_repeats))\n",
        "\n",
        "        unique_ids = np.unique(ids)\n",
        "\n",
        "        for feature in range(n_features):\n",
        "            for repeat in range(self.n_repeats):\n",
        "                X_permuted = X.copy()\n",
        "\n",
        "                permuted_ids = rng.permutation(unique_ids)  # ID를 섞습니다.\n",
        "\n",
        "                # ID에 따라 값을 변경합니다.\n",
        "                for orig_id, new_id in zip(unique_ids, permuted_ids):\n",
        "                    orig_idx = np.where(ids == orig_id)[0]\n",
        "                    new_idx = np.where(ids == new_id)[0]\n",
        "                    X_permuted[orig_idx, feature] = X[new_idx, feature]\n",
        "\n",
        "                # Calculate c-index for permuted X\n",
        "                permuted_cindex = self._compute_baseline_cindex(X_permuted, event, time_stop, ids)\n",
        "\n",
        "                # The importance is the drop in c-index\n",
        "                importances[feature, repeat] = baseline_cindex - permuted_cindex\n",
        "\n",
        "        return importances\n",
        "\n",
        "    def report_importance(self, X, event, time_stop, ids):\n",
        "        \"\"\"\n",
        "        Computes the feature importances and then calculates the mean and standard deviation of the importances for each feature across all the repeats.\n",
        "        The mean gives an average measure of the importance of each feature, while the standard deviation provides an estimate of the variability or uncertainty in the importance estimates.\n",
        "        It returns the mean and standard deviation of the feature importances.\n",
        "        \"\"\"\n",
        "        importances = self.compute_importance(X, event, time_stop, ids)\n",
        "\n",
        "        # Compute mean and std of importances\n",
        "        importance_mean = np.mean(importances, axis=1)\n",
        "        importance_std = np.std(importances, axis=1)\n",
        "\n",
        "        return importance_mean, importance_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9JHyEydNere"
      },
      "outputs": [],
      "source": [
        "permutation_importance = PermutationImportance(rrf, n_repeats=10, random_state=42)\n",
        "mean_importances, std_importances = permutation_importance.report_importance(x, event, time_stop, ids)\n",
        "\n",
        "print(\"Mean Importances:\", mean_importances)\n",
        "print(\"STD Importances:\", std_importances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPRWug3kjncj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}