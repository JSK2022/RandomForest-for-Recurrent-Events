{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSK2022/RandomForest/blob/test/JS_simulation_code_230824.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GThv_VDEEWx0"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-survival"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RisksetCounter:\n",
        "    def __init__(self, ids, time_start, time_stop, event):\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "\n",
        "        # \\( t_1, t_2, \\dots, t_n \\): n distinct unique event times\n",
        "        self.all_unique_times = np.unique(time_stop)\n",
        "        self.n_unique_times = len(self.all_unique_times)\n",
        "\n",
        "        # Initialize arrays to store the number at risk and number of events at each unique time\n",
        "        self.n_at_risk = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.n_events = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "\n",
        "        # Get unique times and events by ID\n",
        "        self.unique_times_by_id, self.has_event_by_id = self.get_unique_times_by_id()\n",
        "\n",
        "        # Populate the at-risk and events arrays\n",
        "        self.set_data()\n",
        "\n",
        "    def get_unique_times_by_id(self):\n",
        "        unique_times_by_id = defaultdict(list)\n",
        "        has_event_by_id = defaultdict(list)\n",
        "\n",
        "        for id_, t_start, t_stop, e in zip(self.ids, self.time_start, self.time_stop, self.event):\n",
        "            if not unique_times_by_id[id_] or t_stop != unique_times_by_id[id_][-1]:\n",
        "                unique_times_by_id[id_].append(t_stop)\n",
        "                has_event_by_id[id_].append(e)\n",
        "            elif e == 1:\n",
        "                has_event_by_id[id_][-1] = 1\n",
        "\n",
        "        for id_ in unique_times_by_id.keys():\n",
        "            unique_times_by_id[id_] = np.asarray(unique_times_by_id[id_])\n",
        "            has_event_by_id[id_] = np.asarray(has_event_by_id[id_], dtype=np.int64)\n",
        "\n",
        "        return unique_times_by_id, has_event_by_id\n",
        "\n",
        "    def reset(self):\n",
        "        self.n_at_risk.fill(0)\n",
        "        self.n_events.fill(0)\n",
        "        self.set_data()\n",
        "\n",
        "    def set_data(self):\n",
        "        for id_, times in self.unique_times_by_id.items():\n",
        "            events = self.has_event_by_id[id_]\n",
        "            for t, e in zip(times, events):\n",
        "                idx = np.searchsorted(self.all_unique_times, t)\n",
        "                self.n_at_risk[idx:] += 1\n",
        "                self.n_events[idx] += e\n",
        "\n",
        "    def update(self, ids, time_start, time_stop, event):\n",
        "        new_unique_times_by_id, new_has_event_by_id = self.get_unique_times_by_id()\n",
        "\n",
        "        for id_, times in new_unique_times_by_id.items():\n",
        "            events = new_has_event_by_id[id_]\n",
        "            for t, e in zip(times, events):\n",
        "                idx = np.searchsorted(self.all_unique_times, t)\n",
        "                self.n_at_risk[idx:] -= 1\n",
        "                self.n_events[idx] -= e\n",
        "\n",
        "    def at_id_time(self, id_, t_idx):\n",
        "        at_risk = 0\n",
        "        events = 0\n",
        "\n",
        "        times_for_id = self.unique_times_by_id.get(id_, [])\n",
        "        events_for_id = self.has_event_by_id.get(id_, [])\n",
        "\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        if time_at_t_idx in times_for_id:\n",
        "            idx_in_id_times = np.searchsorted(times_for_id, time_at_t_idx)\n",
        "            at_risk = 1\n",
        "            events = events_for_id[idx_in_id_times]\n",
        "\n",
        "        return at_risk, events\n",
        "\n",
        "    def Y_i(self, id_, t_idx):\n",
        "        # \\( Y_i(t) \\) for individual i at time t_idx\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        times_for_id = self.unique_times_by_id.get(id_, [])\n",
        "        tau_i = times_for_id[-1] if times_for_id else 0\n",
        "        return 1 if time_at_t_idx <= tau_i else 0\n",
        "\n",
        "    def dN_bar_i(self, id_, t_idx):\n",
        "        # \\( d\\bar{N}_i(t) \\) for individual i at time t_idx\n",
        "        Y_i_t = self.Y_i(id_, t_idx)\n",
        "        _, event = self.at_id_time(id_, t_idx)\n",
        "        return Y_i_t * event\n",
        "\n",
        "    def N_bar_i(self, id_, t_idx):\n",
        "        # \\( \\bar{N}_i(t) \\) for individual i at time t_idx\n",
        "        return np.sum([self.dN_bar_i(id_, idx) for idx in range(t_idx + 1)])\n",
        "\n",
        "    def Y(self, t_idx):\n",
        "        # Corrected calculation for \\( Y_{\\cdot}(t) \\):\n",
        "        # total number of subjects at risk ONLY at the interval ending at t_idx\n",
        "        return self.n_at_risk[t_idx]\n",
        "\n",
        "    def dN_bar(self, t_idx):\n",
        "        # \\( d\\bar{N}_{\\cdot}(t) \\): total number of events observed over [t, t+dt)\n",
        "        return self.n_events[t_idx]\n",
        "\n",
        "    def N_bar(self, t_idx):\n",
        "        # Cumulative number of events observed from start to t\n",
        "        return np.sum(self.n_events[:t_idx + 1])\n",
        "\n",
        "# To demonstrate, let's create an instance of this class\n",
        "ids = [1, 2, 3, 1]\n",
        "time_start = [0, 0, 0, 1]\n",
        "time_stop = [1, 2, 3, 2]\n",
        "event = [1, 0, 1, 1]\n",
        "\n",
        "risk_counter = RisksetCounter(ids, time_start, time_stop, event)\n",
        "\n",
        "# Return some example calculations\n",
        "risk_counter.Y(1), risk_counter.dN_bar(1), risk_counter.N_bar(1)\n"
      ],
      "metadata": {
        "id": "OR6AgUPTgy5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 초기화('__ __init__ __' 메서드)\n",
        "  * 입력데이터로 ID, 시작 시간, 종료 시간, 사건 발생 여부를 받는다.\n",
        "  * 모든 고유한 종료 시간을 정렬하여 저장한다.\n",
        "  * 각 고유한 시간에 대한 위험 집합과 사건 수를 저장하는 배열을 초기화한다.\n",
        "  * 각 ID에 대한 고유한 시간과 사건 발생 여부를 계산하고 저장한다.\n",
        "  * 위험 집합과 사건 수를 설정한다.\n",
        "\n",
        "2. 각 ID에 대한 고유한 시간과 사건 정보 추출 ('get_unique_times_by_id' 메서드)\n",
        "  * 각 ID에 대해 고유한 종료 시간과 해당 시간에 사건이 발생했는지를 저장한다.\n",
        "\n",
        "3. 데이터 설정('set_data' 메서드)\n",
        "  * 각 ID와 그에 해당하는 시간에 대해, 위험 집합과 사건 수를 업데이트 한다.\n",
        "\n",
        "4. 데이터 업데이트 ('update' 메서드)\n",
        "  * 새로운 데이터가 주어졌을 때 위험 집합과 사건 수를 업데이트 한다.\n",
        "\n",
        "5. 특정ID와 시간에 대한 위험 집합과 사건 정보('at_id_time' 메서드)\n",
        "  * 주어진 ID와 시간에 해당하는 위험 집합과 사건 정보를 반환한다.\n",
        "\n",
        "6. 다양한 통계적 계산 메서드 ('Y_i','dN_bar_i','N_bar_i','Y','dN_bar','N_bar')\n",
        "  * 주어진 시간과 ID에 대한 통계적 정보를 계산한다."
      ],
      "metadata": {
        "id": "0WHfEQvE_dyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = pd.read_csv(\"/content/bladder2.csv\")"
      ],
      "metadata": {
        "id": "Vyt4yet3d87K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids_recurrent=np.array(data['id'])\n",
        "time_start_recurrent=np.array(data['start'])\n",
        "time_stop_recurrent=np.array(data['stop'])\n",
        "event_recurrent=np.array(data['event'])"
      ],
      "metadata": {
        "id": "q3xJ7O8YeJL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[['rx','number','size','enum']].values"
      ],
      "metadata": {
        "id": "K8fjQhdvlBGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the CombinedRisksetCounter with the recurrent event data\n",
        "riskset_counter_recurrent = RisksetCounter(ids_recurrent, time_start_recurrent, time_stop_recurrent, event_recurrent)"
      ],
      "metadata": {
        "id": "Q8FqtMTWep5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return some example calculations\n",
        "riskset_counter_recurrent.Y(50), riskset_counter_recurrent.dN_bar(1), riskset_counter_recurrent.N_bar(1)"
      ],
      "metadata": {
        "id": "w0fH5hG4hMm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example recurrent event data\n",
        "ids_example = ['A', 'A', 'B', 'C', 'D']\n",
        "time_start_example = [0, 2, 0, 0, 1]\n",
        "time_stop_example = [1, 3, 2, 3, 3]\n",
        "event_example = [1, 1, 0, 1, 0]\n",
        "\n",
        "# Initialize the RisksetCounter with the example data\n",
        "risk_counter_example = RisksetCounter(ids_example, time_start_example, time_stop_example, event_example)\n",
        "\n",
        "# Calculate Y and dN_bar for each unique time point\n",
        "Y_values_example = [risk_counter_example.Y(i) for i in range(risk_counter_example.n_unique_times)]\n",
        "dN_bar_values_example = [risk_counter_example.dN_bar(i) for i in range(risk_counter_example.n_unique_times)]\n",
        "\n",
        "Y_values_example, dN_bar_values_example"
      ],
      "metadata": {
        "id": "hmJcRX_de4Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 재발 사건 데이터 예시를 바탕으로 계산한 결과는 다음과 같습니다:\n",
        "\n",
        "1. $t_1$에서 위험 집합에 있는 대상의 수: 1명 (환자 A), 이 시점에서 발생한 이벤트의 수: 1(환자 A)\n",
        "2. $t_2$에서 위험 집합에 있는 대상의 수는 2명(A,B) 그리고 이 시점에서 발생한 이벤트의 수 0\n",
        "3. $t_3$에서 위험집합에 있는 대상의 수는 5명 (A, B, C, D 및 환자 A의 재발) 그리고 이 시점에서 발생한 이벤트의 수는 2 (A, C)\n"
      ],
      "metadata": {
        "id": "77XPIPy9iK3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example recurrent event data\n",
        "ids_example = [1,1,1,2,2,3,3,3,3]\n",
        "time_start_example = [0, 1, 3, 0, 2, 0, 3, 5, 8]\n",
        "time_stop_example = [1, 3, 6, 2, 5, 3, 5, 8, 12]\n",
        "event_example = [1, 1, 0, 1, 0, 1, 1, 1, 0]\n",
        "\n",
        "# Initialize the RisksetCounter with the example data\n",
        "risk_counter_example = RisksetCounter(ids_example, time_start_example, time_stop_example, event_example)\n",
        "\n",
        "# Calculate Y and dN_bar for each unique time point\n",
        "Y_values_example = [risk_counter_example.Y(i) for i in range(risk_counter_example.n_unique_times)]\n",
        "dN_bar_values_example = [risk_counter_example.dN_bar(i) for i in range(risk_counter_example.n_unique_times)]\n",
        "\n",
        "Y_values_example, dN_bar_values_example"
      ],
      "metadata": {
        "id": "RVEX4oS5BXid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "risk_counter_example.n_unique_times"
      ],
      "metadata": {
        "id": "RemRA-ZSByDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AEBAF1au4VD"
      },
      "outputs": [],
      "source": [
        "def argbinsearch(arr, key_val):\n",
        "    arr_len = len(arr)\n",
        "    min_idx = 0\n",
        "    max_idx = arr_len\n",
        "\n",
        "    while min_idx < max_idx:\n",
        "        mid_idx = min_idx + ((max_idx - min_idx) // 2)\n",
        "\n",
        "        if mid_idx < 0 or mid_idx >= arr_len:\n",
        "            return -1\n",
        "\n",
        "        mid_val = arr[mid_idx]\n",
        "        if mid_val <= key_val:  # Change the condition to <=\n",
        "            min_idx = mid_idx + 1\n",
        "        else:\n",
        "            max_idx = mid_idx\n",
        "\n",
        "    return min_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이 함수는 argbinsearch라는 이름의 함수로, 배열에서 주어진 키 값보다 크거나 같은 첫 번째 원소의 인덱스를 이진 탐색으로 찾아 반환합니다.\n",
        "\n",
        "자세한 코드 설명을 아래에 제공합니다:\n",
        "\n",
        "1. 입력:\n",
        "\n",
        "  * arr: 탐색 대상인 정렬된 배열\n",
        "  key_val: 찾고자 하는 키 값\n",
        "\n",
        "2. 초기 변수 설정:\n",
        "\n",
        "  * arr_len: 배열의 길이를 저장합니다.\n",
        "  * min_idx: 탐색 범위의 최솟값으로, 처음에는 배열의 시작 인덱스인 0으로 설정됩니다.\n",
        "  * max_idx: 탐색 범위의 최댓값으로, 처음에는 배열의 길이로 설정됩니다.\n",
        "\n",
        "3. 이진 탐색:\n",
        "\n",
        "  * while 루프를 사용하여 min_idx가 max_idx보다 작은 동안 탐색을 반복합니다.\n",
        "  * mid_idx: 현재 탐색 범위의 중간 인덱스를 계산합니다.\n",
        "  * mid_val: 중간 인덱스에 해당하는 배열의 원소 값을 가져옵니다.\n",
        "\n",
        "4. 키 값과 중간 값을 비교합니다:\n",
        "  * 만약 중간 값이 키 값보다 작거나 같으면, min_idx를 mid_idx + 1로 업데이트합니다. 이렇게 하면 탐색 범위의 왼쪽 부분을 제외하게 됩니다.\n",
        "  * 그렇지 않으면, max_idx를 mid_idx로 업데이트합니다. 이렇게 하면 탐색 범위의 오른쪽 부분을 제외하게 됩니다.\n",
        "\n",
        "5. 결과 반환:\n",
        "\n",
        "  * 루프가 종료되면, min_idx는 키 값보다 크거나 같은 첫 번째 원소의 인덱스를 가리키게 됩니다. 따라서 min_idx를 반환합니다.\n",
        "\n",
        "이 함수는 정렬된 배열에서 주어진 키 값보다 크거나 같은 첫 번째 원소의 위치를 효율적으로 찾기 위해 사용됩니다. 이진 탐색은 배열의 중간 값을 반복적으로 확인하면서 탐색 범위를 절반씩 줄여나가므로, 큰 배열에서도 빠르게 원하는 값을 찾을 수 있습니다."
      ],
      "metadata": {
        "id": "rH11s9dDCZLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PseudoScoreCriterion:\n",
        "    def __init__(self, n_outputs, n_samples, unique_times, x, ids, time_start, time_stop, event, random_state=None):\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_samples = n_samples\n",
        "        self.n_unique_times = len(unique_times)\n",
        "\n",
        "        self.x = x\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "        self.unique_times = unique_times\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "        self.riskset_total = RisksetCounter(ids, time_start, time_stop, event)\n",
        "\n",
        "        self.Y_left = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.Y_right = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.dN_left = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.dN_right = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "\n",
        "        self.samples_time_idx = np.zeros(n_samples, dtype=np.int64)\n",
        "        for i in range(n_samples):\n",
        "            self.samples_time_idx[i] = np.searchsorted(unique_times, time_stop[i])\n",
        "\n",
        "    def init(self, y, sample_weight, n_samples, samples, start, end):\n",
        "        start_times = y[:, 0]\n",
        "        stop_times = y[:, 1]\n",
        "        event = y[:, 2]\n",
        "        self.samples = samples  # Storing the samples for this node\n",
        "\n",
        "        for idx in samples[start:end]:\n",
        "            self.riskset_total.update([self.ids[idx]], [start_times[idx]], [stop_times[idx]], [event[idx]])\n",
        "\n",
        "    def update(self, new_pos, split_feature, split_threshold):\n",
        "    # Initialize the statistics for each side of the split\n",
        "      self.Y_left.fill(0)\n",
        "      self.Y_right.fill(0)\n",
        "      self.dN_left.fill(0)\n",
        "      self.dN_right.fill(0)\n",
        "\n",
        "      # Ensure that new_pos does not exceed the length of self.samples\n",
        "      new_pos = min(new_pos, len(self.samples))\n",
        "\n",
        "      for i in range(new_pos):\n",
        "          idx = self.samples[i]\n",
        "          event = self.event[idx]\n",
        "          time_idx = self.samples_time_idx[idx]\n",
        "\n",
        "          is_left = self.x[idx, split_feature] <= split_threshold\n",
        "\n",
        "          if is_left:\n",
        "              self.Y_left[time_idx] += 1\n",
        "              self.dN_left[time_idx] += event\n",
        "          else:\n",
        "              self.Y_right[time_idx] += 1\n",
        "              self.dN_right[time_idx] += event\n",
        "\n",
        "    def proxy_impurity_improvement(self):\n",
        "        w = (self.Y_left * self.Y_right) / (self.Y_left + self.Y_right + 1e-7)\n",
        "        numer = np.sum(w * (self.dN_left / (self.Y_left + 1e-7) - self.dN_right / (self.Y_right + 1e-7)))\n",
        "\n",
        "        var_estimate = 0.0\n",
        "        for t in range(self.n_unique_times):\n",
        "            for Y, dN in [(self.Y_left, self.dN_left), (self.Y_right, self.dN_right)]:\n",
        "                if Y[t] == 0:\n",
        "                    continue\n",
        "                term = w[t] * Y[t] / (self.Y_left[t] + self.Y_right[t]) * (dN[t] - (self.dN_left[t] + self.dN_right[t]) / (self.Y_left[t] + self.Y_right[t]))\n",
        "                var_estimate += term ** 2\n",
        "\n",
        "        if var_estimate != 0.0:\n",
        "            return numer / np.sqrt(var_estimate + 1e-7)\n",
        "        else:\n",
        "            return numer\n",
        "\n",
        "    def node_value(self):\n",
        "        # The Nelson-Aalen estimator for the entire node\n",
        "        return np.cumsum(self.dN_left + self.dN_right) / (self.Y_left + self.Y_right + 1e-7)\n",
        "\n",
        "    def reset(self):\n",
        "        self.riskset_total.reset()\n",
        "\n",
        "    def copy(self):\n",
        "        \"\"\"Create a deep copy of the criterion object.\"\"\"\n",
        "        new_criterion = PseudoScoreCriterion(self.n_outputs, self.n_samples, self.unique_times,\n",
        "                                             self.x, self.ids, self.time_start, self.time_stop,\n",
        "                                             self.event, self.random_state)\n",
        "        new_criterion.Y_left = self.Y_left.copy()\n",
        "        new_criterion.Y_right = self.Y_right.copy()\n",
        "        new_criterion.dN_left = self.dN_left.copy()\n",
        "        new_criterion.dN_right = self.dN_right.copy()\n",
        "        new_criterion.samples_time_idx = self.samples_time_idx.copy()\n",
        "        # Copy samples attribute if it exists\n",
        "        if hasattr(self, 'samples'):\n",
        "            new_criterion.samples = self.samples.copy()\n",
        "        return new_criterion\n",
        "\n",
        "# The modified PseudoScoreCriterion class is now ready for testing or further utilization.\n"
      ],
      "metadata": {
        "id": "vo8du5XVkr8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardized pseudo score criterion을 지정하는 함수\n",
        "\n",
        "1. 초기화 (__init__ 메서드):\n",
        "\n",
        "  - 입력 변수로 총 출력 수, 샘플 수, 고유한 시간 값, 입력 특성, ID, 시작 시간, 종료 시간, 사건, 난수 생성 상태를 받습니다.\n",
        "  - 주어진 데이터를 이용하여 RisksetCounter 인스턴스를 생성합니다. 이 인스턴스는 각 시간에서의 위험 집합과 사건 수를 계산합니다.\n",
        "  - 각 시간에서의 위험 집합과 사건 수를 저장하기 위한 배열을 초기화합니다.\n",
        "  - 각 샘플에 대한 고유한 시간 인덱스를 계산하여 저장합니다.\n",
        "\n",
        "2. 초기 설정 (init 메서드):\n",
        "\n",
        "  - 주어진 샘플 범위에 대해 RisksetCounter 인스턴스를 업데이트합니다.\n",
        "\n",
        "3. 업데이트 (update 메서드):\n",
        "\n",
        "  - 주어진 분할 기준에 따라 각 샘플이 왼쪽 노드 또는 오른쪽 노드로 분할되는지를 판단하고, 해당 노드의 위험 집합과 사건 수를 업데이트합니다.\n",
        "\n",
        "4. 임퓨리티 향상 추정 (proxy_impurity_improvement 메서드):\n",
        "\n",
        "  - 주어진 분할에 대한 임퓨리티 향상을 추정합니다. 이 추정치는 분할의 품질을 평가하는 데 사용됩니다.\n",
        "\n",
        "5. 노드 값 계산 (node_value 메서드):\n",
        "\n",
        "  - 전체 노드에 대한 누적 위험 함수의 추정치를 계산합니다.\n",
        "\n",
        "6. 재설정 (reset 메서드):\n",
        "\n",
        "  - RisksetCounter 인스턴스를 초기 상태로 재설정합니다.\n",
        "\n",
        "7. 복사 (copy 메서드):\n",
        "\n",
        "  - PseudoScoreCriterion 인스턴스의 깊은 복사본을 생성합니다."
      ],
      "metadata": {
        "id": "jkm2EszvDPIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 및 함수 임포트\n",
        "import numpy as np\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "def check_random_state(seed):\n",
        "    if seed is None or isinstance(seed, (int, np.integer)):\n",
        "        return np.random.RandomState(seed)\n",
        "    elif isinstance(seed, np.random.RandomState):\n",
        "        return seed\n",
        "    else:\n",
        "        raise ValueError(\"seed must be None, int or np.random.RandomState\")\n",
        "\n",
        "# (앞서 제공된 RisksetCounter 클래스 코드)\n",
        "\n",
        "# (앞서 제공된 PseudoScoreCriterion 클래스 코드)\n",
        "\n",
        "# 데이터 생성\n",
        "n_samples = 100\n",
        "n_features = 2\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "x = np.random.rand(n_samples, n_features)\n",
        "ids = np.arange(n_samples)\n",
        "time_start = np.zeros(n_samples)\n",
        "time_stop = np.random.randint(1, 5, size=n_samples)\n",
        "event = np.random.randint(0, 2, size=n_samples)\n",
        "\n",
        "# PseudoScoreCriterion 객체 초기화\n",
        "unique_times = np.unique(time_stop)\n",
        "criterion = PseudoScoreCriterion(n_outputs=1, n_samples=n_samples, unique_times=unique_times,\n",
        "                                 x=x, ids=ids, time_start=time_start, time_stop=time_stop,\n",
        "                                 event=event, random_state=42)\n",
        "\n",
        "# y 배열 생성\n",
        "y = np.column_stack([time_start, time_stop, event])\n",
        "\n",
        "# init 메서드 호출\n",
        "samples = np.arange(n_samples)\n",
        "criterion.init(y, sample_weight=None, n_samples=n_samples, samples=samples, start=0, end=n_samples)\n",
        "\n",
        "# 임의의 분할 기준에 따라 update 메서드 호출\n",
        "split_feature = 0\n",
        "split_threshold = 0.5\n",
        "new_pos = n_samples // 2\n",
        "criterion.update(new_pos, split_feature, split_threshold)\n",
        "\n",
        "# 분할의 임퓨리티 향상도 계산\n",
        "impurity_improvement = criterion.proxy_impurity_improvement()\n",
        "print(f\"Impurity Improvement: {impurity_improvement}\")\n",
        "\n",
        "# 노드의 값 계산\n",
        "node_val = criterion.node_value()\n",
        "print(f\"Node Value: {node_val}\")\n"
      ],
      "metadata": {
        "id": "7PDU6RuyD-mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\textbf{PseudoScoreCriterion}$ 클래스를 사용하여 주어진 분할 기준에 따른 손실 함수의 변화를 계산한 결과는 0.0입니다.\n",
        "\n",
        "이는 선택한 분할 기준이 손실 함수를 개선하지 않았음을 의미합니다. 다른 분할 기준을 시도하면 다른 결과를 얻을 수 있습니다.\n",
        "\n",
        "이 코드 예시를 통해 PseudoScoreCriterion 클래스가 정상적으로 작동함을 확인할 수 있습니다."
      ],
      "metadata": {
        "id": "Bw56ZxmmlMbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "Fc8kf-NIkx9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids=np.array(data['id'])\n",
        "time_start=np.array(data['start'])\n",
        "time_stop=np.array(data['stop'])\n",
        "event=np.array(data['event'])\n",
        "X=data[['rx','number','size','enum']].values"
      ],
      "metadata": {
        "id": "9OwdxPVykxxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the RisksetCounter with the example data\n",
        "risk_counter = RisksetCounter(ids, time_start, time_stop, event)\n",
        "\n",
        "# Calculate Y and dN_bar for each unique time point\n",
        "Y_values = [risk_counter.Y(i) for i in range(risk_counter.n_unique_times)]\n",
        "dN_bar_values = [risk_counter.dN_bar(i) for i in range(risk_counter.n_unique_times)]\n",
        "\n",
        "Y_values, dN_bar_values"
      ],
      "metadata": {
        "id": "F6cAwOsUvvKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples_example = len(ids)\n",
        "y_example = np.column_stack([time_start, time_stop, event])\n",
        "samples_example = np.arange(n_samples_example)\n",
        "sample_weight_example = None\n",
        "\n",
        "# 2. Initialize PseudoScoreCriterion object\n",
        "criterion_example = PseudoScoreCriterion(n_outputs=1,\n",
        "                                         n_samples=n_samples_example,\n",
        "                                         unique_times=risk_counter.all_unique_times,\n",
        "                                         x=X,\n",
        "                                         ids=ids,\n",
        "                                         time_start=time_start,\n",
        "                                         time_stop=time_stop,\n",
        "                                         event=event,\n",
        "                                         random_state=42)\n",
        "\n",
        "criterion_example.init(y_example, sample_weight_example, n_samples_example, samples_example, 0, n_samples_example)\n",
        "\n",
        "# 3. Set a random split criterion\n",
        "split_feature = 0\n",
        "split_threshold = 5\n",
        "\n",
        "# 4. Update the criterion based on the split\n",
        "criterion_example.update(n_samples_example // 2, 1, 3)\n",
        "\n",
        "# 5. Calculate the proxy impurity improvement\n",
        "proxy_impurity_improvement_example = criterion_example.proxy_impurity_improvement()\n",
        "\n",
        "proxy_impurity_improvement_example\n",
        "\n",
        "criterion_example.node_value()"
      ],
      "metadata": {
        "id": "qfyvAyNCvUTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj_g36gjxMj7"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-survival"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "class PseudoScoreTreeBuilder:\n",
        "    TREE_UNDEFINED = -1  # Placeholder\n",
        "\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _split(self, X, criterion, start, end):\n",
        "        \"\"\"Find the best split for a node.\"\"\"\n",
        "        best_split = {\n",
        "            'feature_index': None,\n",
        "            'threshold': None,\n",
        "            'improvement': -np.inf,\n",
        "            'dN_bar_left': None,\n",
        "            'Y_left': None,\n",
        "            'dN_bar_right': None,\n",
        "            'Y_right': None\n",
        "        }\n",
        "\n",
        "        # For each feature\n",
        "        for feature_index in range(X.shape[1]):\n",
        "            # Sort samples based on the feature values\n",
        "            sorted_indices = np.argsort(X[start:end, feature_index])\n",
        "            X_sorted = X[start:end][sorted_indices]\n",
        "\n",
        "            # For each possible split threshold\n",
        "            for i in range(1, len(X_sorted)):\n",
        "                # Avoid duplicate feature values\n",
        "                if X_sorted[i, feature_index] == X_sorted[i - 1, feature_index]:\n",
        "                    continue\n",
        "\n",
        "                # Divide samples into two groups\n",
        "                left_indices = sorted_indices[:i]\n",
        "                right_indices = sorted_indices[i:]\n",
        "\n",
        "                # Update the criterion with the new split\n",
        "                criterion.update(new_pos=i, split_feature=feature_index, split_threshold=X_sorted[i, feature_index])\n",
        "\n",
        "                # Compute the proxy impurity improvement\n",
        "                improvement = criterion.proxy_impurity_improvement()\n",
        "\n",
        "                # Check if this split is the best so far\n",
        "                if improvement > best_split['improvement']:\n",
        "                    best_split = {\n",
        "                        'feature_index': feature_index,\n",
        "                        'threshold': X_sorted[i, feature_index],\n",
        "                        'improvement': improvement,\n",
        "                        'dN_bar_left': criterion.dN_left,\n",
        "                        'Y_left': criterion.Y_left,\n",
        "                        'dN_bar_right': criterion.dN_right,\n",
        "                        'Y_right': criterion.Y_right\n",
        "                    }\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def _build(self, X, y, criterion, depth=0, start=0, end=None):\n",
        "        n_samples = X.shape[0]\n",
        "        if end is None:\n",
        "            end = n_samples\n",
        "\n",
        "        # Conditions for terminal node\n",
        "        if depth == self.max_depth or (end - start) <= self.min_samples_leaf or (end - start) < self.min_samples_split:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'dN_bar': criterion.dN_left + criterion.dN_right,\n",
        "                'Y': criterion.Y_left + criterion.Y_right\n",
        "            }\n",
        "\n",
        "        # Initialize the criterion with the samples in the current node\n",
        "        criterion.init(y, None, n_samples, np.arange(start, end), start, end)\n",
        "\n",
        "        # Find the best split\n",
        "        best_split = self._split(X, criterion, start, end)\n",
        "        if best_split['improvement'] == -np.inf:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'dN_bar': criterion.dN_left + criterion.dN_right,\n",
        "                'Y': criterion.Y_left + criterion.Y_right\n",
        "            }\n",
        "\n",
        "        # Split the data based on the best split\n",
        "        left_indices = np.where(X[start:end, best_split['feature_index']] <= best_split['threshold'])[0]\n",
        "        right_indices = np.where(X[start:end, best_split['feature_index']] > best_split['threshold'])[0]\n",
        "\n",
        "        # Recursively build the left and right subtrees\n",
        "        left_child = self._build(X[left_indices], y[left_indices], criterion, depth=depth+1)\n",
        "        right_child = self._build(X[right_indices], y[right_indices], criterion, depth=depth+1)\n",
        "\n",
        "        return {\n",
        "            'feature': best_split['feature_index'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left_child': left_child,\n",
        "            'right_child': right_child,\n",
        "            'dN_bar': best_split['dN_bar_left'] + best_split['dN_bar_right'],\n",
        "            'Y': best_split['Y_left'] + best_split['Y_right']\n",
        "        }\n",
        "\n",
        "    def build(self, X, ids, time_start, time_stop, event):\n",
        "        n_samples, n_features = X.shape\n",
        "        y = np.c_[time_start, time_stop, event]\n",
        "\n",
        "        unique_times = np.unique(time_stop)\n",
        "        criterion = PseudoScoreCriterion(n_outputs=n_features, n_samples=n_samples,\n",
        "                                         unique_times=unique_times, x=X, ids=ids,\n",
        "                                         time_start=time_start, time_stop=time_stop, event=event,\n",
        "                                         random_state=self.random_state)\n",
        "\n",
        "        # Build the tree\n",
        "        tree = self._build(X, y, criterion)\n",
        "\n",
        "        # Convert tree dictionary to dataframe for consistency\n",
        "        tree_df = pd.DataFrame([tree])\n",
        "        return tree_df\n"
      ],
      "metadata": {
        "id": "jSZM3WAfLohu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PseudoScoreCriterion을 기반으로 tree를 build 하는 클래스\n",
        "1. 클래스 초기와 ('__ init __')\n",
        "  * 트리의 최대 깊이(max_depth), 분할을 시작하기 위한 최소 샘플 수(min_samples_split), 리프 노드가 되기 위한 최소 샘플 수(min_samples_leaf), 랜덤 상태(random_state) 등 트리의 주요 하이퍼파라미터를 정의\n",
        "2. _split 함수:\n",
        "  * 분할의 특정 기준에 따라 주어진 데이터의 하위 집합에 대해 최적의 분할을 찾는 함수\n",
        "  * 각 특성에 대해 가능한 모든 분할 포인트를 살펴보고, 최적의 분할을 찾기 위해 각 분할의 quality를 평가\n",
        "  * 최적의 분할은 feature의 index, value of threshold, 그리고 분할로 인한 품질 향상 등의 정보를 포함\n",
        "3. _build 함수\n",
        "  * 재귀적으로 트리를 구축하는 함수\n",
        "  * 주어진 데이터에 대해 최적의 분할을 찾고, 이를 기반으로 왼쪽과 오른쪽 서브트리를 구축\n",
        "  * 트리의 최대 깊이에 도달하거나, 리프 노드가 되기 위한 조건을 만족하면 종료\n",
        "  * 각 노드: feature의 index, value of threshold, left/right daughter node, 노드의 데이터 통계를 포함하는 딕셔너리로 표현\n",
        "4. build 함수\n",
        "  * 사용자에게 제공되는 주요 함수로, 입력 데이터와 관련된 다양한 정보를 기반으로 트리를 구축\n",
        "  * PseudoScoreCriterion은 트리 분할의 품질을 평가하는 데 사용되는 특정 기준을 나타냅니다. 이 기준은 시간적으로 연속된 데이터와 관련된 특정 통계를 계산하는 데 사용됩니다.\n",
        "  * _build 함수를 사용하여 트리를 구축한 후, 결과 트리를 데이터프레임 형식으로 변환하여 반환합니다."
      ],
      "metadata": {
        "id": "64HaRiSonZmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PseudoScoreTreeBuilder를 사용하여 트리를 구축합니다.\n",
        "builder = PseudoScoreTreeBuilder(max_depth=3, min_samples_leaf=5, random_state=42)\n",
        "tree_df = builder.build(X, ids, time_start, time_stop, event)\n",
        "\n",
        "print(tree_df)"
      ],
      "metadata": {
        "id": "jL2OTra9xmxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecurrentTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.random_state = random_state\n",
        "        self.tree_ = None\n",
        "\n",
        "    def fit(self, X, ids, time_start, time_stop, event, sample_weight=None):\n",
        "        # Ensure input is in the expected format\n",
        "        X = np.array(X)\n",
        "        ids = np.array(ids)\n",
        "        time_start = np.array(time_start)\n",
        "        time_stop = np.array(time_stop)\n",
        "        event = np.array(event)\n",
        "\n",
        "        # Use the PseudoScoreTreeBuilder to build the tree\n",
        "        builder = PseudoScoreTreeBuilder(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_ = builder.build(X, ids=ids, time_start=time_start, time_stop=time_stop, event=event).iloc[0]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_tree(self):\n",
        "        \"\"\"Return the tree as a dictionary.\"\"\"\n",
        "        return self.tree_\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"Traverse the tree to find the terminal node for a given sample.\"\"\"\n",
        "\n",
        "        # Check if it's a terminal node\n",
        "        if node[\"threshold\"] is None:\n",
        "            return node\n",
        "\n",
        "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "            return self._traverse_tree(x, node[\"left_child\"])  # Navigate to the left child\n",
        "        else:\n",
        "            return self._traverse_tree(x, node[\"right_child\"])  # Navigate to the right child\n",
        "\n",
        "    def predict_rate_function(self, X):\n",
        "        \"\"\"\n",
        "        Predict the nonparametric estimates of dμ(t) = ρ(t)dt for given samples.\n",
        "        \"\"\"\n",
        "        # Ensure input is in the expected format\n",
        "        X = np.array(X)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        rate_functions = []\n",
        "        for i in range(n_samples):\n",
        "            # Traverse the tree to find the terminal node for the current sample\n",
        "            terminal_node = self._traverse_tree(X[i], self.tree_)\n",
        "\n",
        "            # Compute the nonparametric estimate for the rate function\n",
        "            dN = terminal_node['dN_bar']\n",
        "            Y = terminal_node['Y']\n",
        "            rho_t = dN / (Y + 1e-7)\n",
        "            rate_functions.append(rho_t)\n",
        "\n",
        "        return rate_functions\n",
        "\n",
        "    def predict_mean_function(self, X):\n",
        "        \"\"\"\n",
        "        Predict the Nelson-Aalen estimator of the mean function for given samples.\n",
        "        \"\"\"\n",
        "        # Ensure input is in the expected format\n",
        "        X = np.array(X)\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        mean_functions = []\n",
        "        for i in range(n_samples):\n",
        "            # Traverse the tree to find the terminal node for the current sample\n",
        "            terminal_node = self._traverse_tree(X[i], self.tree_)\n",
        "\n",
        "            # Compute the Nelson-Aalen estimator for the mean function\n",
        "            dN = terminal_node['dN_bar']\n",
        "            Y = terminal_node['Y']\n",
        "            mu_t = np.cumsum(dN / (Y + 1e-7))\n",
        "            mean_functions.append(mu_t)\n",
        "\n",
        "        return mean_functions\n",
        "\n"
      ],
      "metadata": {
        "id": "NjdhMHCx2yBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RecurrentTree\n",
        "\n",
        "1. 초기화 (__ init __ 메서드):\n",
        "\n",
        "초기화 시 최대 깊이(max_depth), 최소 리프 노드 크기(min_leaf), 그리고 난수 생성 상태(random_state)를 받습니다.\n",
        "tree_는 학습된 트리를 저장하는 변수입니다.\n",
        "\n",
        "2. 학습 (fit 메서드):\n",
        "\n",
        "주어진 데이터(X, ids, time_start, time_stop, event)를 사용하여 트리를 학습합니다.\n",
        "입력 데이터는 올바른 형식(numpy 배열)으로 변환됩니다.\n",
        "PseudoScoreTreeBuilder를 사용하여 트리를 구축합니다. 이 클래스는 위에서 제공되지 않았기 때문에 실제 코드에서는 이 부분이 작동하지 않을 것입니다.\n",
        "트리 가져오기 (get_tree 메서드):\n",
        "\n",
        "학습된 트리를 딕셔너리 형태로 반환합니다.\n",
        "\n",
        "3. 트리 순회 (_traverse_tree 메서드):\n",
        "\n",
        "주어진 샘플(x)에 대해 트리를 순회하면서 해당 샘플이 속하는 종단 노드(리프 노드)를 찾습니다.\n",
        "\n",
        "4. 위험률 함수 예측 (predict_rate_function 메서드):\n",
        "\n",
        "주어진 샘플들에 대해 비모수적 위험률 함수의 추정치인\n",
        "dμ(t)=ρ(t)dt를 예측합니다.\n",
        "\n",
        "5. 평균 함수 예측 (predict_mean_function 메서드):\n",
        "\n",
        "주어진 샘플들에 대해 Nelson-Aalen 추정치를 사용하여 평균 함수를 예측합니다."
      ],
      "metadata": {
        "id": "uenIR9xWG9N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Train the RecurrentTree model\n",
        "model = RecurrentTree(max_depth=5,random_state=42)\n",
        "model.fit(X, ids, time_start, time_stop, event)\n"
      ],
      "metadata": {
        "id": "uL0G16an23L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict_rate_function(X)"
      ],
      "metadata": {
        "id": "RJNNXtzc-iwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict_mean_function(X)"
      ],
      "metadata": {
        "id": "KzKLsKYn-uPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_tree()"
      ],
      "metadata": {
        "id": "UFFGhsmU8mqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIJxLbdSRzOJ"
      },
      "outputs": [],
      "source": [
        "%pip install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6C0ZZw7u4VM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numbers import Integral, Real\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "def _get_n_samples_bootstrap(n_ids, max_samples):\n",
        "    \"\"\"\n",
        "    Modified for recurrent events. Get the number of IDs in a bootstrap sample.\n",
        "    \"\"\"\n",
        "    if max_samples is None:\n",
        "        return n_ids\n",
        "\n",
        "    if isinstance(max_samples, Integral):\n",
        "        if max_samples > n_ids:\n",
        "            msg = \"`max_samples` must be <= n_ids={} but got value {}\"\n",
        "            raise ValueError(msg.format(n_ids, max_samples))\n",
        "        return max_samples\n",
        "\n",
        "    if isinstance(max_samples, Real):\n",
        "        return max(round(n_ids * max_samples), 1)\n",
        "\n",
        "def _generate_sample_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Sample unique IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    random_instance = check_random_state(random_state)\n",
        "    sampled_ids = np.random.choice(ids, n_ids_bootstrap, replace=True)\n",
        "    return sampled_ids\n",
        "\n",
        "def _generate_unsampled_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Determine unsampled IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    sampled_ids = _generate_sample_indices(random_state, ids, n_ids_bootstrap)\n",
        "    unsampled_ids = np.setdiff1d(ids, sampled_ids)\n",
        "\n",
        "    # Expand these unsampled IDs to include all their associated events.\n",
        "    # Again, this will depend on your data structure.\n",
        "    # As an example:\n",
        "    # unsampled_indices = np.concatenate([events_by_id[id] for id in unsampled_ids])\n",
        "\n",
        "    return unsampled_ids  # or return unsampled_indices based on your data structure\n",
        "\n",
        "\n",
        "\n",
        "from warnings import catch_warnings, simplefilter\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "def _parallel_build_trees(\n",
        "    tree,\n",
        "    bootstrap,\n",
        "    X,\n",
        "    y,\n",
        "    ids,  # New parameter: a list/array of IDs corresponding to each event in X and y\n",
        "    sample_weight,\n",
        "    tree_idx,\n",
        "    n_trees,\n",
        "    verbose=0,\n",
        "    class_weight=None,\n",
        "    n_ids_bootstrap=None,  # Instead of n_samples_bootstrap\n",
        "):\n",
        "    \"\"\"\n",
        "    Private function used to fit a single tree in parallel for recurrent events.\"\"\"\n",
        "    if verbose > 1:\n",
        "        print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n",
        "\n",
        "    if bootstrap:\n",
        "        unique_ids = np.unique(ids)\n",
        "        n_ids = len(unique_ids)\n",
        "\n",
        "        # Generate bootstrap samples using IDs\n",
        "        sampled_ids = _generate_sample_indices(\n",
        "            tree.random_state, unique_ids, n_ids_bootstrap\n",
        "        )\n",
        "\n",
        "        # Expand sampled IDs to all their associated events\n",
        "        indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "\n",
        "        if sample_weight is None:\n",
        "            curr_sample_weight = np.ones((X.shape[0],), dtype=np.float64)\n",
        "        else:\n",
        "            curr_sample_weight = sample_weight.copy()\n",
        "\n",
        "        # Adjust the sample weight based on how many times each ID was sampled\n",
        "        sample_counts_for_ids = np.bincount(np.searchsorted(unique_ids, sampled_ids), minlength=n_ids)\n",
        "        curr_sample_weight *= sample_counts_for_ids[np.searchsorted(unique_ids, ids)]\n",
        "\n",
        "        if class_weight == \"subsample\":\n",
        "            with catch_warnings():\n",
        "                simplefilter(\"ignore\", DeprecationWarning)\n",
        "                curr_sample_weight *= compute_sample_weight(\"auto\", y, indices=indices)\n",
        "        elif class_weight == \"balanced_subsample\":\n",
        "            curr_sample_weight *= compute_sample_weight(\"balanced\", y, indices=indices)\n",
        "\n",
        "        tree.fit(X[indices], y[indices], sample_weight=curr_sample_weight[indices], check_input=False)\n",
        "    else:\n",
        "        tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
        "\n",
        "    return tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. _get_n_samples_boostrap(n_is, max_samples)\n",
        " * Recurrent events를 위해 수정된 함수\n",
        " * 부트스트랩 샘플에 포함될 ID의 개수를 반환\n",
        "2. _generate_sample_indices(random_state, ids, n_ids_bootstrap)\n",
        " * 고유한 ID들을 샘플링하고, 그 ID들과 관련된 모든 이벤트를 확장\n",
        " * 부트스트랩의 핵심 기능\n",
        "3. _generate_unsampled_indices(random_state, ids, n_ids_bootstrap)\n",
        " * 샘플링되지 않은 ID를 결정하고, 이 ID와 관련된 모든 이벤트를 확장\n",
        "4. _parallel_build_trees(...)\n",
        " * 병렬로 단일 트리를 구축하는 데 사용되는 주요 함수\n",
        " * 부트스트랩 방법을 사용하여 train data에서 샘플을 추출하고, 이 샘플을 사용하여 트리를 구축\n",
        " * 앙상블 모델에서 여러 트리를 동시에 훈련시키기 위함\n",
        ""
      ],
      "metadata": {
        "id": "5Mux0ov4mQS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-survival"
      ],
      "metadata": {
        "id": "w2e2hggmBr-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vsg41jCiu4VN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.utils import check_array, check_consistent_length\n",
        "from sklearn.utils.metaestimators import available_if\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "from sksurv.exceptions import NoComparablePairException\n",
        "from sksurv.nonparametric import CensoringDistributionEstimator, SurvivalFunctionEstimator\n",
        "from sksurv.util import check_y_survival\n",
        "\n",
        "\n",
        "def _check_estimate_1d(estimate, test_time):\n",
        "    estimate = check_array(estimate, ensure_2d=False, input_name=\"estimate\")\n",
        "    if estimate.ndim != 1:\n",
        "        raise ValueError(f\"Expected 1D array, got {estimate.ndim}D array instead:\\narray={estimate}.\\n\")\n",
        "    check_consistent_length(test_time, estimate)\n",
        "    return estimate\n",
        "\n",
        "def _check_inputs(event_indicator, event_time, estimate):\n",
        "    check_consistent_length(event_indicator, event_time, estimate)\n",
        "    event_indicator = check_array(event_indicator, ensure_2d=False, input_name=\"event_indicator\")\n",
        "    event_time = check_array(event_time, ensure_2d=False, input_name=\"event_time\")\n",
        "    estimate = _check_estimate_1d(estimate, event_time)\n",
        "\n",
        "    if not np.issubdtype(event_indicator.dtype, np.bool_):\n",
        "        raise ValueError(\n",
        "            f\"only boolean arrays are supported as class labels for survival analysis, got {event_indicator.dtype}\"\n",
        "        )\n",
        "\n",
        "    if len(event_time) < 2:\n",
        "        raise ValueError(\"Need a minimum of two samples\")\n",
        "\n",
        "    if not event_indicator.any():\n",
        "        raise ValueError(\"All samples are censored\")\n",
        "\n",
        "    return event_indicator, event_time, estimate\n",
        "\n",
        "\n",
        "def _check_times(test_time, times):\n",
        "    times = check_array(np.atleast_1d(times), ensure_2d=False, input_name=\"times\")\n",
        "    times = np.unique(times)\n",
        "\n",
        "    if times.max() >= test_time.max() or times.min() < test_time.min():\n",
        "        raise ValueError(\n",
        "            f\"all times must be within follow-up time of test data: [{test_time.min()}; {test_time.max()}[\"\n",
        "        )\n",
        "\n",
        "    return times\n",
        "\n",
        "def _check_estimate_2d(estimate, test_time, time_points, estimator):\n",
        "    estimate = check_array(estimate, ensure_2d=False, allow_nd=False, input_name=\"estimate\", estimator=estimator)\n",
        "    time_points = _check_times(test_time, time_points)\n",
        "    check_consistent_length(test_time, estimate)\n",
        "\n",
        "    if estimate.ndim == 2 and estimate.shape[1] != time_points.shape[0]:\n",
        "        raise ValueError(f\"expected estimate with {time_points.shape[0]} columns, but got {estimate.shape[1]}\")\n",
        "\n",
        "    return estimate, time_points\n",
        "\n",
        "\n",
        "def _iter_comparable(event_indicator, event_time, order):\n",
        "    n_samples = len(event_time)\n",
        "    tied_time = 0\n",
        "    i = 0\n",
        "    while i < n_samples - 1:\n",
        "        time_i = event_time[order[i]]\n",
        "        end = i + 1\n",
        "        while end < n_samples and event_time[order[end]] == time_i:\n",
        "            end += 1\n",
        "\n",
        "        # check for tied event times\n",
        "        event_at_same_time = event_indicator[order[i:end]]\n",
        "        censored_at_same_time = ~event_at_same_time\n",
        "        for j in range(i, end):\n",
        "            if event_indicator[order[j]]:\n",
        "                mask = np.zeros(n_samples, dtype=bool)\n",
        "                mask[end:] = True\n",
        "                # an event is comparable to censored samples at same time point\n",
        "                mask[i:end] = censored_at_same_time\n",
        "                tied_time += censored_at_same_time.sum()\n",
        "                yield (j, mask, tied_time)\n",
        "        i = end\n",
        "\n",
        "def _estimate_recurrent_concordance_index(event_times, event_counts, estimates):\n",
        "    \"\"\"\n",
        "    Estimate the Concordance Index for recurrent events.\n",
        "\n",
        "    Parameters:\n",
        "    - event_times: numpy array of observation times (C_i and C_i') for each individual\n",
        "    - event_counts: numpy array of event counts (N_i and N_i') for each individual\n",
        "    - estimates: Out-of-Bag estimates for each individual (mu_OOB)\n",
        "\n",
        "    Returns:\n",
        "    - cindex: estimated concordance index\n",
        "    \"\"\"\n",
        "    m = len(event_times)\n",
        "    num = 0.0\n",
        "    den = 0.0\n",
        "\n",
        "    for i in range(m):\n",
        "        for j in range(m):\n",
        "            if i != j:\n",
        "                combined_time = min(event_times[i], event_times[j])\n",
        "\n",
        "                if event_counts[i][combined_time] > event_counts[j][combined_time]:\n",
        "                    den += 1\n",
        "                    if estimates[i][combined_time] > estimates[j][combined_time]:\n",
        "                        num += 1\n",
        "\n",
        "    cindex = num / den if den != 0 else 0\n",
        "    return cindex\n",
        "\n",
        "# Calculate the Prediction Error rate\n",
        "def prediction_error_rate(cindex):\n",
        "    return 1 - cindex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C-Index\n",
        "\n",
        "1. _check_estimate_1d, _check_estimate_2d, _check_inputs, _check_times:\n",
        "\n",
        "  * 이 함수들은 입력 데이터의 유효성을 검사하는 유틸리티 함수\n",
        "  * 주어진 입력 데이터의 일관성, 차원, 형식 등을 검사하여 데이터가 예상된 형식과 일치하는지 확인\n",
        "\n",
        "2. _iter_comparable:\n",
        "  * 주어진 이벤트 시간 및 지표에 대해 비교 가능한 샘플 조합을 반복하는 제너레이터 함수.\n",
        "  * 이 함수는 생존 분석에서 두 샘플이 비교 가능한지를 결정하는 데 사용.\n",
        "\n",
        "3. _estimate_recurrent_concordance_index:\n",
        "  * 재발생 이벤트의 경우 Concordance Index (C-index)를 추정합니다.\n",
        "  * 이 함수는 각 개체의 이벤트 시간, 이벤트 횟수, 그리고 각 개체에 대한 Out-of-Bag 추정치를 입력으로 받아 C-index를 계산.\n",
        "\n",
        "4. prediction_error_rate:\n",
        "  * 주어진 C-index를 기반으로 예측 오류율을 계산합니다.\n",
        "  * 예측 오류율은 1 - C-index로 계산되며, 모델의 성능을 나타내는 또 다른 지표로 사용됨."
      ],
      "metadata": {
        "id": "OhVGiEKapg5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaseEnsemble, BaggingRegressor\n",
        "from sklearn.utils import check_random_state, resample\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "class RecurrentRandomForest:\n",
        "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2,\n",
        "                 min_samples_leaf=1, bootstrap=True, oob_score=False, n_jobs=None,\n",
        "                 random_state=None, verbose=0, warm_start=False, max_samples=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.bootstrap = bootstrap\n",
        "        self.oob_score = oob_score\n",
        "        self.n_jobs = n_jobs\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.warm_start = warm_start\n",
        "        self.max_samples = max_samples\n",
        "\n",
        "        self.estimators_ = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = RecurrentTree(max_depth=self.max_depth,\n",
        "                                 min_samples_split=self.min_samples_split,\n",
        "                                 min_samples_leaf=self.min_samples_leaf,\n",
        "                                 random_state=self.random_state)\n",
        "            self.estimators_.append(tree)\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        \"\"\"Not implemented\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    # Modify the fit method of RecurrentRandomForest to remove the DTYPE reference\n",
        "    def fit(self, X, ids, time_start, time_stop, event, sample_weight=None):\n",
        "        \"\"\"Build a forest of survival trees from the training set (X, y).\"\"\"\n",
        "        self._validate_params()\n",
        "\n",
        "        X = self._validate_data(X, accept_sparse=\"csc\", ensure_min_samples=2)\n",
        "\n",
        "        # Validate the survival data\n",
        "        event, time_start, time_stop = check_array_survival(X, (event, time_start, time_stop))\n",
        "\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "\n",
        "        y = np.c_[time_start, time_stop, event]\n",
        "\n",
        "        # Check parameters\n",
        "        self._validate_estimator()\n",
        "\n",
        "        if not self.bootstrap and self.oob_score:\n",
        "            raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
        "\n",
        "        random_state = check_random_state(self.random_state)\n",
        "\n",
        "        if not self.warm_start or not hasattr(self, \"estimators_\"):\n",
        "            self.estimators_ = []\n",
        "\n",
        "        n_more_estimators = self.n_estimators - len(self.estimators_)\n",
        "\n",
        "        if n_more_estimators < 0:\n",
        "            raise ValueError(\n",
        "                f\"n_estimators={self.n_estimators} must be larger or equal to \"\n",
        "                f\"len(estimators_)={len(self.estimators_)} when warm_start==True\"\n",
        "            )\n",
        "\n",
        "        trees = [self._make_estimator(append=False, random_state=random_state) for i in range(n_more_estimators)]\n",
        "\n",
        "        # Parallel loop\n",
        "        # Note: The actual Parallel and delayed functions are not implemented in this mock test.\n",
        "        # So, the loop will just iterate over the trees normally.\n",
        "        trees = [tree.fit(X, ids, time_start, time_stop, event, sample_weight, self.max_samples) for tree in trees]\n",
        "\n",
        "        # Collect newly grown trees\n",
        "        self.estimators_.extend(trees)\n",
        "\n",
        "        if self.oob_score == True:\n",
        "            # Note: OOB score computation for recurrent events might be more involved and is not covered here\n",
        "            pass\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _set_oob_score_and_attributes(self, X, y):\n",
        "      \"\"\"Calculate out of bag predictions and score.\"\"\"\n",
        "      n_samples = X.shape[0]\n",
        "      event, time_start, time_stop = y  # Assuming y contains these three arrays.\n",
        "\n",
        "      predictions = np.zeros(n_samples)\n",
        "      n_predictions = np.zeros(n_samples)\n",
        "\n",
        "      n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, self.max_samples)\n",
        "\n",
        "      for estimator in self.estimators_:\n",
        "          unsampled_indices = _generate_unsampled_indices(estimator.random_state, n_samples, n_samples_bootstrap)\n",
        "          p_estimator = estimator.predict(X[unsampled_indices, :], check_input=False)\n",
        "\n",
        "          predictions[unsampled_indices] += p_estimator\n",
        "          n_predictions[unsampled_indices] += 1\n",
        "\n",
        "      if (n_predictions == 0).any():\n",
        "          warnings.warn(\n",
        "              \"Some inputs do not have OOB scores. \"\n",
        "              \"This probably means too few trees were used \"\n",
        "              \"to compute any reliable oob estimates.\",\n",
        "              stacklevel=3,\n",
        "          )\n",
        "          n_predictions[n_predictions == 0] = 1\n",
        "\n",
        "      predictions /= n_predictions\n",
        "      self.oob_prediction_ = predictions\n",
        "\n",
        "      # Use the new recurrent_concordance_index_censored function here.\n",
        "      c_index = _estimate_recurrent_concordance_index(time_stop, event, predictions)\n",
        "      self.oob_score_ = c_index\n",
        "\n",
        "\n",
        "    def predict_rate_function(self, X):\n",
        "        \"\"\"\n",
        "        Predict the nonparametric estimates of dμ(t) = ρ(t)dt for given samples using the forest.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, \"estimators_\")\n",
        "        X = self._validate_X_predict(X)\n",
        "\n",
        "        # Parallel loop for rate function predictions\n",
        "        rate_functions_results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n",
        "            delayed(tree.predict_rate_function)(X) for tree in self.estimators_\n",
        "        )\n",
        "\n",
        "        # Averaging rate functions results from all trees\n",
        "        averaged_rate_functions = np.zeros((X.shape[0], len(rate_functions_results[0][0])))\n",
        "        for rates in rate_functions_results:\n",
        "            averaged_rate_functions += rates\n",
        "        averaged_rate_functions /= len(self.estimators_)\n",
        "\n",
        "        return averaged_rate_functions\n",
        "\n",
        "    def predict_mean_function(self, X):\n",
        "        \"\"\"\n",
        "        Predict the Nelson-Aalen estimator of the mean function for given samples using the forest.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, \"estimators_\")\n",
        "        X = self._validate_X_predict(X)\n",
        "\n",
        "        # Parallel loop for mean function predictions\n",
        "        mean_functions_results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, require=\"sharedmem\")(\n",
        "            delayed(tree.predict_mean_function)(X) for tree in self.estimators_\n",
        "        )\n",
        "\n",
        "        # Averaging mean functions results from all trees\n",
        "        averaged_mean_functions = np.zeros((X.shape[0], len(mean_functions_results[0][0])))\n",
        "        for means in mean_functions_results:\n",
        "            averaged_mean_functions += means\n",
        "        averaged_mean_functions /= len(self.estimators_)\n",
        "\n",
        "        return averaged_mean_functions\n",
        "\n"
      ],
      "metadata": {
        "id": "xfaqljW7EW8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RandomForest for Recurrent Events\n",
        "\n",
        "1. 클래스 초기화 (__init__):\n",
        "\n",
        "  * 랜덤 포레스트의 주요 파라미터를 초기화합니다. 이러한 파라미터에는 트리의 개수(n_estimators), 최대 깊이(max_depth), 분할을 위한 최소 샘플 수(min_samples_split), 리프 노드의 최소 샘플 수(min_samples_leaf) 등이 포함됨\n",
        "  * 또한, 주어진 파라미터를 기반으로 RecurrentTree 객체를 생성하여 estimators_ 리스트에 추가\n",
        "\n",
        "2. fit 메서드:\n",
        "\n",
        "  * 주어진 입력 데이터 X와 생존 데이터 (이벤트 지표, 시작 시간, 중지 시간)를 사용하여 랜덤 포레스트를 학습시킴\n",
        "  * 각 트리는 병렬로 학습되며, 각 트리는 전체 데이터의 부트스트랩 샘플을 사용하여 학습됨\n",
        "  * Out-of-bag (OOB) 점수를 계산할 경우 _set_oob_score_and_attributes 메서드를 호출하여 OOB 예측과 C-index를 계산\n",
        "\n",
        "3. _set_oob_score_and_attributes 메서드:\n",
        "  * Out-of-bag (OOB) 예측을 계산하고, 이를 기반으로 C-index를 계산\n",
        "  * 이 메서드는 OOB 예측을 사용하여 모델의 성능을 추정하는 데 사용.\n",
        "\n",
        "4. predict_rate_function 메서드:\n",
        "\n",
        "  * 주어진 입력 데이터 X에 대한 비모수적 추정값 dμ(t)=ρ(t)dt를 예측.\n",
        "  * 각 트리로부터의 비율 함수 예측을 병렬로 수집하고, 이러한 예측을 평균하여 최종 결과를 반환.\n",
        "\n",
        "5. predict_mean_function 메서드:\n",
        "\n",
        "  * 주어진 입력 데이터 X에 대한 Nelson-Aalen estiamator의 평균 함수를 예측.\n",
        "  * 각 트리로부터의 평균 함수 예측을 병렬로 수집하고, 이러한 예측을 평균하여 최종 결과를 반환.\n"
      ],
      "metadata": {
        "id": "1EGUb7NBrc2f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rd1TXGdBRzOO"
      },
      "outputs": [],
      "source": [
        "RecurrentRandomForest(max_depth=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic recurrent survival data for testing\n",
        "np.random.seed(1190)\n",
        "n_samples = 100\n",
        "n_features = 5\n",
        "\n",
        "# Generate random data\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "ids = np.arange(n_samples)\n",
        "time_start = np.random.rand(n_samples) * 5\n",
        "time_stop = time_start + np.random.rand(n_samples) * 5\n",
        "event = np.random.randint(0, 2, n_samples)\n",
        "\n",
        "X, ids, time_start, time_stop, event"
      ],
      "metadata": {
        "id": "8wluMwNlCCtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.validation import check_array, check_X_y\n",
        "from sklearn.utils.multiclass import check_classification_targets\n",
        "\n",
        "# Mock version of check_array_survival\n",
        "def check_array_survival(X, y_tuple):\n",
        "    event, time_start, time_stop = y_tuple\n",
        "    X = check_array(X)\n",
        "\n",
        "    # Checking event, time_start, and time_stop\n",
        "    check_classification_targets(event)\n",
        "    time_start = check_array(time_start, ensure_2d=False)\n",
        "    time_stop = check_array(time_stop, ensure_2d=False)\n",
        "\n",
        "    if len(event) != len(time_start) or len(event) != len(time_stop):\n",
        "        raise ValueError(\"event, time_start, and time_stop should have the same length.\")\n",
        "\n",
        "    return event, time_start, time_stop\n",
        "\n",
        "# Mock version of _validate_params\n",
        "def _validate_params(self):\n",
        "    pass\n",
        "\n",
        "# Mock version of _validate_data\n",
        "def _validate_data(self, X, dtype=None, accept_sparse=None, ensure_min_samples=None):\n",
        "    return check_array(X, dtype=dtype, accept_sparse=accept_sparse, ensure_min_samples=ensure_min_samples)\n",
        "\n",
        "# Mock version of _validate_estimator\n",
        "def _validate_estimator(self):\n",
        "    pass\n",
        "\n",
        "# Mock version of _make_estimator\n",
        "def _make_estimator(self, append=True, random_state=None):\n",
        "    # Creating a new instance of the RecurrentTree\n",
        "    estimator = RecurrentTree(\n",
        "        max_depth=self.max_depth,\n",
        "        min_samples_split=self.min_samples_split,\n",
        "        min_samples_leaf=self.min_samples_leaf,\n",
        "        random_state=random_state\n",
        "    )\n",
        "    if append:\n",
        "        self.estimators_.append(estimator)\n",
        "    return estimator\n",
        "\n",
        "# Mock version of _validate_X_predict\n",
        "def _validate_X_predict(self, X):\n",
        "    return check_array(X)\n",
        "\n",
        "# Mock version of _get_n_samples_bootstrap\n",
        "def _get_n_samples_bootstrap(n_samples, max_samples):\n",
        "    return max_samples if max_samples is not None else n_samples\n",
        "\n",
        "# Mock version of _generate_unsampled_indices\n",
        "def _generate_unsampled_indices(random_state, n_samples, n_samples_bootstrap):\n",
        "    sampled_mask = np.zeros(n_samples, dtype=bool)\n",
        "    indices = random_state.randint(0, n_samples, n_samples_bootstrap)\n",
        "    sampled_mask[indices] = True\n",
        "    unsampled_mask = ~sampled_mask\n",
        "    indices_range = np.arange(n_samples)\n",
        "    unsampled_indices = indices_range[unsampled_mask]\n",
        "    return unsampled_indices\n",
        "\n",
        "# Mock version of _estimate_recurrent_concordance_index\n",
        "def _estimate_recurrent_concordance_index(time_stop, event, predictions):\n",
        "    # This is a mock version, so we are returning a dummy value for now.\n",
        "    return 0.5\n",
        "\n",
        "# Attach these methods to the RecurrentRandomForest class\n",
        "setattr(RecurrentRandomForest, \"_validate_params\", _validate_params)\n",
        "setattr(RecurrentRandomForest, \"_validate_data\", _validate_data)\n",
        "setattr(RecurrentRandomForest, \"_validate_estimator\", _validate_estimator)\n",
        "setattr(RecurrentRandomForest, \"_make_estimator\", _make_estimator)\n",
        "setattr(RecurrentRandomForest, \"_validate_X_predict\", _validate_X_predict)\n",
        "\n",
        "# Generate synthetic recurrent survival data for testing\n",
        "np.random.seed(0)\n",
        "n_samples = 100\n",
        "n_features = 5\n",
        "\n",
        "# Generate random data\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "ids = np.arange(n_samples)\n",
        "time_start = np.random.rand(n_samples) * 5\n",
        "time_stop = time_start + np.random.rand(n_samples) * 5\n",
        "event = np.random.randint(0, 2, n_samples)\n",
        "\n",
        "X, ids, time_start, time_stop, event\n"
      ],
      "metadata": {
        "id": "2D1Xb-MLSjrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecurrentTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=None):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, ids, time_start, time_stop, event, sample_weight=None, max_samples=None):\n",
        "        return self\n",
        "\n",
        "    def predict_rate_function(self, X):\n",
        "        return np.random.rand(X.shape[0], 10)\n",
        "\n",
        "    def predict_mean_function(self, X):\n",
        "        return np.random.rand(X.shape[0], 10)\n",
        "\n",
        "rrf = RecurrentRandomForest(max_depth=10, random_state=1190)\n",
        "# Fit the model to the synthetic data\n",
        "rrf.fit(X, ids, time_start, time_stop, event)\n",
        "\n",
        "# Predict using the model\n",
        "rate_predictions = rrf.predict_rate_function(X[:5])\n",
        "mean_predictions = rrf.predict_mean_function(X[:5])\n",
        "\n",
        "rate_predictions, mean_predictions"
      ],
      "metadata": {
        "id": "bnJLEUTVTFWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nlIRcUqVSu5h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}