{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdhCcWLO2n5P"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-survival"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Load"
      ],
      "metadata": {
        "id": "SH3U70sw3MDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "path=\"/Users/jeongsookim/Downloads\"\n",
        "data = pd.read_csv(f\"{path}/simuDat.csv\")\n",
        "\n",
        "ids = data['id'].values\n",
        "time_start = data['time_start'].values\n",
        "time_stop = data['time_stop'].values\n",
        "event = data['event'].values\n",
        "x = data[['group','x1','gender']].values"
      ],
      "metadata": {
        "id": "0vOfW82U2pAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RisksetCounter"
      ],
      "metadata": {
        "id": "xbuELecz3N0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RisksetCounter:\n",
        "    def __init__(self, ids, time_start, time_stop, event):\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "\n",
        "        self.all_unique_times = np.unique(time_stop)\n",
        "        self.n_unique_times = len(self.all_unique_times)\n",
        "\n",
        "        self.n_at_risk = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.n_events = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.set_data()\n",
        "\n",
        "        self.state_stack = []\n",
        "\n",
        "    def set_data(self):\n",
        "        unique_ids = set(self.ids)\n",
        "        for t_idx, t in enumerate(self.all_unique_times):\n",
        "            self.n_at_risk[t_idx] = sum([self.Y_i(id_, t_idx) for id_ in unique_ids])\n",
        "            self.n_events[t_idx] = sum([self.dN_bar_i(id_, t_idx) for id_ in unique_ids])\n",
        "\n",
        "    def Y_i(self, id_, t_idx):\n",
        "        if t_idx >= len(self.all_unique_times):\n",
        "            return 0\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        indices = (self.ids == id_) & (time_at_t_idx <= self.time_stop)\n",
        "        return np.any(indices)\n",
        "\n",
        "    def dN_bar_i(self, id_, t_idx):\n",
        "        if t_idx >= len(self.all_unique_times):\n",
        "            return 0\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        indices = (self.ids == id_) & (time_at_t_idx == self.time_stop) & (self.event == 1)\n",
        "        return np.any(indices)\n",
        "\n",
        "    def save_state(self):\n",
        "        self.state_stack.append((self.ids.copy(), self.time_start.copy(), self.time_stop.copy(), self.event.copy(), self.n_at_risk.copy(), self.n_events.copy()))\n",
        "\n",
        "    def load_state(self):\n",
        "        if self.state_stack:\n",
        "            self.ids, self.time_start, self.time_stop, self.event, self.n_at_risk, self.n_events = self.state_stack.pop()\n",
        "\n",
        "    def update(self, new_ids, new_time_start, new_time_stop, new_event):\n",
        "        # Save the current state\n",
        "        self.save_state()\n",
        "\n",
        "        # Compute the intersection of data\n",
        "        mask = np.isin(self.ids, new_ids)\n",
        "\n",
        "        # Extract data of the intersection\n",
        "        updated_ids = self.ids[mask]\n",
        "        updated_time_start = self.time_start[mask]\n",
        "        updated_time_stop = self.time_stop[mask]\n",
        "        updated_event = self.event[mask]\n",
        "\n",
        "        # Update object variables based on the intersection data\n",
        "        self.ids = updated_ids\n",
        "        self.time_start = updated_time_start\n",
        "        self.time_stop = updated_time_stop\n",
        "        self.event = updated_event\n",
        "\n",
        "        # Recalculate unique times based on the updated data\n",
        "        self.all_unique_times = np.unique(np.concatenate([self.time_start, self.time_stop]))\n",
        "        self.n_unique_times = len(self.all_unique_times)\n",
        "\n",
        "        # Resize the n_at_risk and n_events arrays based on the updated unique times\n",
        "        self.n_at_risk = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.n_events = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "\n",
        "        # Update the n_at_risk and n_events arrays\n",
        "        unique_ids = set(self.ids)  # Extract unique IDs to avoid redundant calculations\n",
        "        for t_idx, t in enumerate(self.all_unique_times):\n",
        "            self.n_at_risk[t_idx] = sum([self.Y_i(id_, t_idx) for id_ in unique_ids])\n",
        "            self.n_events[t_idx] = sum([self.dN_bar_i(id_, t_idx) for id_ in unique_ids])\n",
        "\n",
        "    def reset(self):\n",
        "        self.load_state()\n",
        "\n",
        "    def copy(self):\n",
        "        return RisksetCounter(self.ids.copy(), self.time_start.copy(), self.time_stop.copy(), self.event.copy())\n",
        "\n",
        "    def __reduce__(self):\n",
        "        return (self.__class__, (self.ids, self.time_start, self.time_stop, self.event))\n"
      ],
      "metadata": {
        "id": "tl2CEoxu2o-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def argbinsearch(arr, key_val):\n",
        "    arr_len = len(arr)\n",
        "    min_idx = 0\n",
        "    max_idx = arr_len\n",
        "\n",
        "    while min_idx < max_idx:\n",
        "        mid_idx = min_idx + ((max_idx - min_idx) // 2)\n",
        "\n",
        "        if mid_idx < 0 or mid_idx >= arr_len:\n",
        "            return -1\n",
        "\n",
        "        mid_val = arr[mid_idx]\n",
        "        if mid_val <= key_val:  # Change the condition to <=\n",
        "            min_idx = mid_idx + 1\n",
        "        else:\n",
        "            max_idx = mid_idx\n",
        "\n",
        "    return min_idx"
      ],
      "metadata": {
        "id": "65bunywp2o8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PseudoScoreCriterion"
      ],
      "metadata": {
        "id": "XDchPECe3Q0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PseudoScoreCriterion:\n",
        "    def __init__(self, n_outputs, n_samples, unique_times, x, ids, time_start, time_stop, event):\n",
        "        \"\"\"ㅊ\n",
        "        Constructor of the class\n",
        "        Initialize instance variables using the provided input parameters\n",
        "        Objects 'riskset_left', 'riskset_right', and 'riskset_total' are initialized using the 'RisksetCounter' class\n",
        "        \"\"\"\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_samples = n_samples\n",
        "        self.unique_times = unique_times\n",
        "        self.x = x\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "\n",
        "        self.unique_ids = set(self.ids)  # Store unique ids for later use\n",
        "        self.unique_times = unique_times\n",
        "\n",
        "        self.riskset_left = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        self.riskset_right = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        self.riskset_total = RisksetCounter(ids, time_start, time_stop, event)\n",
        "\n",
        "        self.samples_time_idx = np.searchsorted(unique_times, time_stop)\n",
        "\n",
        "        self.split_pos = 0\n",
        "        self.split_time_idx = 0\n",
        "\n",
        "        self._riskset_counter = RisksetCounter(ids, time_start, time_stop, event)  # 새로 추가\n",
        "\n",
        "    def init(self, y, sample_weight, n_samples, samples, start, end):\n",
        "        \"\"\"\n",
        "        Initialization function\n",
        "        Reset the risk set counters ('riskset_left','riskset_right','riskset_total') and updates 'riskset_total' with new data\n",
        "        \"\"\"\n",
        "        self.samples = samples\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "        self.riskset_total.reset()\n",
        "\n",
        "        time_starts, stop_times, events = y[:, 0], y[:, 1], y[:, 2]\n",
        "        ids_for_update = [self.ids[idx] for idx in samples[start:end]]\n",
        "        time_starts_for_update = [time_starts[idx] for idx in samples[start:end]]\n",
        "        stop_times_for_update = [stop_times[idx] for idx in samples[start:end]]\n",
        "        events_for_update = [events[idx] for idx in samples[start:end]]\n",
        "\n",
        "        # Combine unique times from both datasets\n",
        "        self.unique_times = np.unique(np.concatenate([self.unique_times, stop_times_for_update]))\n",
        "\n",
        "        self.riskset_total.update(ids_for_update, time_starts_for_update, stop_times_for_update, events_for_update)\n",
        "\n",
        "    def set_unique_times(self, unique_times):\n",
        "        \"\"\"Sets the unique times for the current node.\"\"\"\n",
        "        self.unique_times = unique_times\n",
        "\n",
        "## Group Indicator만으로 나누기...\n",
        "\n",
        "    # Functions returning the risk set value and event value for the given ID and time index from the respective risk set (left or right)\n",
        "    def Y_left_value(self, id_, t):\n",
        "        return self.riskset_left.Y_i(id_, t)\n",
        "\n",
        "    def Y_right_value(self, id_, t):\n",
        "        return self.riskset_right.Y_i(id_, t)\n",
        "\n",
        "    def dN_bar_left_value(self, id_, t):\n",
        "        return self.riskset_left.dN_bar_i(id_, t)\n",
        "\n",
        "    def dN_bar_right_value(self, id_, t):\n",
        "        return self.riskset_right.dN_bar_i(id_, t)\n",
        "\n",
        "    def temporary_update_riskset(self, riskset_counter, ids, time_start, time_stop, event):\n",
        "        # Combine and find unique stop times from both nodes\n",
        "        combined_time_stops = np.concatenate([self.riskset_left.time_stop, self.riskset_right.time_stop])\n",
        "        unique_time_stops = np.unique(combined_time_stops)\n",
        "\n",
        "        riskset_counter.all_unique_times = unique_time_stops\n",
        "\n",
        "        # Resize the n_at_risk and n_events arrays based on the updated unique times\n",
        "        riskset_counter.n_at_risk = np.zeros(len(unique_time_stops), dtype=np.int64)\n",
        "        riskset_counter.n_events = np.zeros(len(unique_time_stops), dtype=np.int64)\n",
        "\n",
        "        # Update the n_at_risk and n_events arrays\n",
        "        unique_ids = set(ids)  # Extract unique IDs to avoid redundant calculations\n",
        "        for t_idx, t in enumerate(unique_time_stops):\n",
        "            riskset_counter.n_at_risk[t_idx] = sum([riskset_counter.Y_i(id_, t_idx) for id_ in unique_ids])\n",
        "            riskset_counter.n_events[t_idx] = sum([riskset_counter.dN_bar_i(id_, t_idx) for id_ in unique_ids])\n",
        "\n",
        "    def calculate_numerator(self):\n",
        "        # Temporary update riskset\n",
        "        self.temporary_update_riskset(self.riskset_left, self.riskset_left.ids, self.riskset_left.time_start, self.riskset_left.time_stop, self.riskset_left.event)\n",
        "        self.temporary_update_riskset(self.riskset_right, self.riskset_right.ids, self.riskset_right.time_start, self.riskset_right.time_stop, self.riskset_right.event)\n",
        "\n",
        "        w = (self.riskset_left.n_at_risk * self.riskset_right.n_at_risk) / (self.riskset_left.n_at_risk + self.riskset_right.n_at_risk)\n",
        "        term = (self.riskset_left.n_events / self.riskset_left.n_at_risk) - (self.riskset_right.n_events / self.riskset_right.n_at_risk)\n",
        "\n",
        "        return np.sum(w * term)\n",
        "\n",
        "    def calculate_variance_estimate(self):\n",
        "        \"\"\"\n",
        "        Update the variance estimate to be compatible with the provided function.\n",
        "        \"\"\"\n",
        "\n",
        "        def var_comp(riskset, id_, uniTimeVec, w_const, max_w_const):\n",
        "            \"\"\"\n",
        "            Compute the variance component for each observation,\n",
        "            similar to the var_comp function in the mcfDiff.test R code.\n",
        "            \"\"\"\n",
        "            y_i_tj = np.array([riskset.Y_i(id_, t_idx) for t_idx in range(len(uniTimeVec))])\n",
        "            yVec = riskset.n_at_risk\n",
        "            n_i_tj = np.array([riskset.dN_bar_i(id_, t_idx) for t_idx in range(len(uniTimeVec))])\n",
        "            dLambda = riskset.n_events / (riskset.n_at_risk + 1e-7)  # Avoid division by zero\n",
        "\n",
        "            res_ij = np.where(yVec > 0, y_i_tj / yVec * (n_i_tj - dLambda), 0)\n",
        "\n",
        "            max_res_ij = np.max(np.abs(res_ij))\n",
        "\n",
        "            if max_res_ij > 0:\n",
        "                re_res_ij = res_ij / max_res_ij\n",
        "                reFactor = np.exp(np.log(max_res_ij) + np.log(max_w_const))\n",
        "            else:\n",
        "                re_res_ij = 0\n",
        "                reFactor = 1\n",
        "\n",
        "            res_const = (w_const / max_w_const) * re_res_ij\n",
        "\n",
        "            return (np.sum(res_const) * reFactor) ** 2\n",
        "\n",
        "        # Temporary update riskset\n",
        "        self.temporary_update_riskset(self.riskset_left, self.riskset_left.ids, self.riskset_left.time_start, self.riskset_left.time_stop, self.riskset_left.event)\n",
        "        self.temporary_update_riskset(self.riskset_right, self.riskset_right.ids, self.riskset_right.time_start, self.riskset_right.time_stop, self.riskset_right.event)\n",
        "\n",
        "        # Extract required variables\n",
        "        uniTimeVec = self.riskset_total.all_unique_times\n",
        "        w_const = (self.riskset_left.n_at_risk * self.riskset_right.n_at_risk) / (self.riskset_left.n_at_risk + self.riskset_right.n_at_risk)\n",
        "        max_w_const = np.max(w_const)\n",
        "\n",
        "        # Calculate variance components for each ID in the left and right nodes\n",
        "        varList1 = [var_comp(self.riskset_left, id_, uniTimeVec, w_const, max_w_const)\n",
        "                    for id_ in np.unique(self.riskset_left.ids)]\n",
        "\n",
        "        varList2 = [var_comp(self.riskset_right, id_, uniTimeVec, w_const, max_w_const)\n",
        "                    for id_ in np.unique(self.riskset_right.ids)]\n",
        "\n",
        "        # Sum the variance components\n",
        "        varU_1 = np.sum(varList1)\n",
        "        varU_2 = np.sum(varList2)\n",
        "\n",
        "        return varU_1 + varU_2\n",
        "\n",
        "\n",
        "    def calculate_denominator(self):\n",
        "        return self.calculate_variance_estimate()\n",
        "\n",
        "    def proxy_impurity_improvement(self):\n",
        "        if len(self.riskset_left.n_at_risk) == 0 or len(self.riskset_right.n_at_risk) == 0:\n",
        "            return -np.inf\n",
        "\n",
        "        numer = self.calculate_numerator() ** 2\n",
        "        denom = self.calculate_denominator()\n",
        "\n",
        "        return numer / (denom + 1e-7)\n",
        "\n",
        "    def update_riskset(self, ids_subset):\n",
        "        # Update the riskset based on the subset of IDs at the current node\n",
        "        unique_ids_subset = np.unique(ids_subset)\n",
        "        self.riskset_counter.update(unique_ids_subset, self.time_start, self.time_stop, self.event)\n",
        "\n",
        "    def node_value(self):\n",
        "        \"\"\"\n",
        "        Returns the Nelson-Aalen estimator of the mean function μ(t) for the entities in the current node.\n",
        "        \"\"\"\n",
        "        return self.node_value_from_riskset(self.riskset_total)\n",
        "\n",
        "    def node_value_from_riskset(self, riskset_counter):\n",
        "        \"\"\"\n",
        "        Returns the Nelson-Aalen estimator of the mean function μ(t) for the entities based on provided riskset_counter.\n",
        "        \"\"\"\n",
        "        mu_hat_values = []\n",
        "\n",
        "        # Initialize the cumulative sum of the Nelson-Aalen estimator\n",
        "        cumsum_Nelson_Aalen = 0\n",
        "\n",
        "        for t_idx, t in enumerate(self.unique_times):\n",
        "            # Use n_at_risk and n_events from the riskset_counter\n",
        "            n_at_risk_t = riskset_counter.n_at_risk[t_idx] if t_idx < len(riskset_counter.n_at_risk) else 0\n",
        "            n_events_t = riskset_counter.n_events[t_idx] if t_idx < len(riskset_counter.n_events) else 0\n",
        "\n",
        "            cumsum_Nelson_Aalen += n_events_t / (n_at_risk_t + 1e-7)  # Avoiding division by zero\n",
        "            mu_hat_values.append(cumsum_Nelson_Aalen)\n",
        "\n",
        "        return mu_hat_values\n",
        "\n",
        "    # RisksetCounter의 상태를 저장하고 복원하기 위한 메서드를 추가합니다.\n",
        "    def save_riskset_state(self):\n",
        "        self._riskset_counter.save_state()\n",
        "\n",
        "    def reset_riskset_state(self):\n",
        "        self._riskset_counter.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Functions to reset all risk set counters\n",
        "        \"\"\"\n",
        "        self.riskset_total.reset()\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "\n",
        "    def copy(self):\n",
        "        \"\"\"\n",
        "        Creates and returns a copy of the current object.\n",
        "        \"\"\"\n",
        "        new_criterion = PseudoScoreCriterion(self.n_outputs, self.n_samples, self.unique_times,\n",
        "                                             self.x, self.ids, self.time_start, self.time_stop,\n",
        "                                             self.event)\n",
        "        new_criterion.riskset_left = self.riskset_left.copy()\n",
        "        new_criterion.riskset_right = self.riskset_right.copy()\n",
        "        new_criterion.riskset_total = self.riskset_total.copy()\n",
        "        new_criterion.samples_time_idx = self.samples_time_idx.copy()\n",
        "        if hasattr(self, 'samples'):\n",
        "            new_criterion.samples = self.samples.copy()\n",
        "\n",
        "        return new_criterion\n",
        "\n",
        "def update_with_group_indicator(self, feature_index, group_indicator):\n",
        "    \"\"\"\n",
        "    Update the criterion based on a specified feature and group indicator.\n",
        "    This will split the data into left and right nodes based on the provided feature and group indicator.\n",
        "    \"\"\"\n",
        "    # Reset the riskset counters for the left and right nodes\n",
        "    self.riskset_left.reset()\n",
        "    self.riskset_right.reset()\n",
        "\n",
        "    # Determine the split by the feature and group indicator\n",
        "    left_mask = self.x[:, feature_index] <= group_indicator  # Changed to <= for continuous features\n",
        "    right_mask = ~left_mask\n",
        "\n",
        "    # Create empty lists to store the ids, start times, stop times, and events for both left and right splits\n",
        "    ids_left, start_left, stop_left, event_left = [], [], [], []\n",
        "    ids_right, start_right, stop_right, event_right = [], [], [], []\n",
        "\n",
        "    # For each unique ID, decide whether to assign it to the left or right node based on the mask\n",
        "    for id_ in self.unique_ids:\n",
        "        id_indices = np.where(self.ids == id_)[0]  # Get all indices for this ID\n",
        "        if left_mask[id_indices[0]]:\n",
        "            ids_left.extend([self.ids[i] for i in id_indices])\n",
        "            start_left.extend([self.time_start[i] for i in id_indices])\n",
        "            stop_left.extend([self.time_stop[i] for i in id_indices])\n",
        "            event_left.extend([self.event[i] for i in id_indices])\n",
        "        else:\n",
        "            ids_right.extend([self.ids[i] for i in id_indices])\n",
        "            start_right.extend([self.time_start[i] for i in id_indices])\n",
        "            stop_right.extend([self.time_stop[i] for i in id_indices])\n",
        "            event_right.extend([self.event[i] for i in id_indices])\n",
        "\n",
        "    # Set the all_unique_times for the risk sets of left and right nodes to the current node's unique times\n",
        "    self.riskset_left.all_unique_times = self.unique_times\n",
        "    self.riskset_right.all_unique_times = self.unique_times\n",
        "\n",
        "    # Also, adjust the lengths of n_at_risk and n_events in both riskset_left and riskset_right to match unique_times\n",
        "    self.riskset_left.n_at_risk = np.zeros(len(self.unique_times), dtype=np.int64)\n",
        "    self.riskset_left.n_events = np.zeros(len(self.unique_times), dtype=np.int64)\n",
        "    self.riskset_right.n_at_risk = np.zeros(len(self.unique_times), dtype=np.int64)\n",
        "    self.riskset_right.n_events = np.zeros(len(self.unique_times), dtype=np.int64)\n",
        "\n",
        "    # Update the risk sets for the left and right nodes\n",
        "    self.riskset_left.update(ids_left, start_left, stop_left, event_left)\n",
        "    self.riskset_right.update(ids_right, start_right, stop_right, event_right)\n",
        "\n",
        "# 이 함수를 PseudoScoreCriterion 클래스에 추가합니다.\n",
        "setattr(PseudoScoreCriterion, 'update', update_with_group_indicator)\n",
        "\n",
        "# 추가로, left node와 right node의 데이터를 반환하는 메소드를 추가합니다.\n",
        "def get_left_node_data(self):\n",
        "    return self.riskset_left.ids, self.riskset_left.n_at_risk, self.riskset_left.n_events\n",
        "\n",
        "def get_right_node_data(self):\n",
        "    return self.riskset_right.ids, self.riskset_right.n_at_risk, self.riskset_right.n_events\n",
        "\n",
        "setattr(PseudoScoreCriterion, 'get_left_node_data', get_left_node_data)\n",
        "setattr(PseudoScoreCriterion, 'get_right_node_data', get_right_node_data)\n",
        "\n",
        "def calculate_node_value_updated(self, side=\"left\"):\n",
        "    \"\"\"\n",
        "    Calculate the node value based on the updated RisksetCounter using get_left_node_data and get_right_node_data.\n",
        "\n",
        "    Parameters:\n",
        "        - side (str): Either \"left\" or \"right\" to determine which riskset to use for calculation.\n",
        "    \"\"\"\n",
        "    if side == \"left\":\n",
        "        ids, n_at_risk, n_events = self.get_left_node_data()\n",
        "    elif side == \"right\":\n",
        "        ids, n_at_risk, n_events = self.get_right_node_data()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid side value. Expected 'left' or 'right'.\")\n",
        "\n",
        "    mask = np.isin(self.ids, ids)\n",
        "\n",
        "    time_start_filtered = self.time_start[mask]\n",
        "    time_stop_filtered = self.time_stop[mask]\n",
        "    event_filtered = self.event[mask]\n",
        "\n",
        "    riskset_temp = RisksetCounter(ids, time_start_filtered, time_stop_filtered, event_filtered)\n",
        "    riskset_temp.n_at_risk = n_at_risk\n",
        "    riskset_temp.n_events = n_events\n",
        "\n",
        "    return self.node_value_from_riskset(riskset_temp)\n",
        "\n",
        "# PseudoScoreCriterion 클래스에 위에서 정의한 함수를 추가합니다.\n",
        "setattr(PseudoScoreCriterion, 'calculate_node_value', calculate_node_value_updated)\n",
        "\n",
        "\n",
        "\n",
        "PseudoScoreCriterion\n"
      ],
      "metadata": {
        "id": "nDBzXNLj2o6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PseudoScoreTreeBuilder"
      ],
      "metadata": {
        "id": "gsMFlKY33TYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "class PseudoScoreTreeBuilder:\n",
        "    \"\"\"\n",
        "    Class designed to build a decision tree based on the pseudo-score test statistics criterion,\n",
        "    typically used in recurrent events data analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    TREE_UNDEFINED = -1  # Placeholder\n",
        "\n",
        "    def __init__(self, max_depth=None, min_ids_split=2, min_ids_leaf=1,\n",
        "                 max_features=None, max_thresholds=None, min_impurity_decrease=0,\n",
        "                 random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_ids_split = min_ids_split\n",
        "        self.min_ids_leaf = min_ids_leaf\n",
        "        self.max_features = max_features\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "    def split_indices(self, X_column, threshold, criterion, start, end):\n",
        "        \"\"\"Efficiently splits the data based on the given threshold for a specific feature column.\"\"\"\n",
        "        left_indices = np.where(X_column <= threshold)[0]\n",
        "        right_indices = np.where(X_column > threshold)[0]\n",
        "\n",
        "        # Convert local indices to global indices\n",
        "        left_indices = np.arange(start, end)[left_indices]\n",
        "        right_indices = np.arange(start, end)[right_indices]\n",
        "\n",
        "        return left_indices, right_indices\n",
        "\n",
        "    def _split(self, X, criterion, start, end):\n",
        "        best_split = {\n",
        "            'feature_index': None,\n",
        "            'threshold': None,\n",
        "            'improvement': -np.inf\n",
        "        }\n",
        "\n",
        "        # 가능한 스플릿 후보들을 저장하기 위한 리스트\n",
        "        potential_splits = []\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        for feature_index in range(n_features):\n",
        "            unique_thresholds = np.unique(X[start:end, feature_index])\n",
        "            if len(unique_thresholds) <= 1:\n",
        "                continue\n",
        "\n",
        "            if self.max_thresholds and len(unique_thresholds) > self.max_thresholds:\n",
        "                unique_thresholds = self.random_state.choice(unique_thresholds, self.max_thresholds, replace=False)\n",
        "\n",
        "            for threshold in unique_thresholds:\n",
        "                criterion.update(feature_index, threshold)\n",
        "                improvement = criterion.proxy_impurity_improvement()\n",
        "\n",
        "                left_indices, right_indices = self.split_indices(X[start:end, feature_index], threshold, criterion, start, end)\n",
        "\n",
        "                # Ensure that both child nodes will have at least min_ids_leaf samples\n",
        "                if len(left_indices) < self.min_ids_leaf or len(right_indices) < self.min_ids_leaf:\n",
        "                    continue\n",
        "\n",
        "                # self.min_impurity_decrease보다 큰 모든 스플릿 후보들을 저장\n",
        "                if improvement > self.min_impurity_decrease:\n",
        "                    potential_splits.append({\n",
        "                        'feature_index': feature_index,\n",
        "                        'threshold': threshold,\n",
        "                        'improvement': improvement\n",
        "                    })\n",
        "\n",
        "                if improvement > best_split['improvement']:\n",
        "                    best_split = {\n",
        "                        'feature_index': feature_index,\n",
        "                        'threshold': threshold,\n",
        "                        'improvement': improvement\n",
        "                    }\n",
        "\n",
        "        return best_split, potential_splits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _build(self, X, y, criterion, depth=0, start=0, end=None):\n",
        "        if end is None:\n",
        "            end = X.shape[0]\n",
        "\n",
        "        ids = y[start:end, 0]\n",
        "        unique_ids = np.unique(ids)\n",
        "\n",
        "        riskset_counter = RisksetCounter(ids, y[start:end, 1], y[start:end, 2], y[start:end, 3])\n",
        "        node_value = criterion.node_value_from_riskset(riskset_counter)\n",
        "        node_unique_times = riskset_counter.all_unique_times.tolist()\n",
        "        node_value = node_value[:len(node_unique_times)]\n",
        "\n",
        "        # Check depth and minimum ids required for split\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value,\n",
        "                'unique_times': node_unique_times,\n",
        "                'ids': unique_ids.tolist()\n",
        "            }\n",
        "\n",
        "        if len(unique_ids) < self.min_ids_split:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value,\n",
        "                'unique_times': node_unique_times,\n",
        "                'ids': unique_ids.tolist()\n",
        "            }\n",
        "\n",
        "        best_split, potential_splits = self._split(X, criterion, start, end)\n",
        "\n",
        "        for split in [best_split] + potential_splits:\n",
        "            if split['threshold'] is None:\n",
        "                continue\n",
        "\n",
        "            left_indices, right_indices = self.split_indices(X[start:end, split['feature_index']], split['threshold'], criterion, start, end)\n",
        "\n",
        "            # Check if there are enough unique ids in both left and right children after the split\n",
        "            if len(np.unique(ids[left_indices])) >= self.min_ids_leaf and len(np.unique(ids[right_indices])) >= self.min_ids_leaf:\n",
        "                best_split = split\n",
        "                break\n",
        "        else:  # No valid split found\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value,\n",
        "                'unique_times': node_unique_times,\n",
        "                'ids': unique_ids.tolist()\n",
        "            }\n",
        "\n",
        "        left_child = self._build(X[left_indices], y[left_indices], criterion, depth=depth+1)\n",
        "        right_child = self._build(X[right_indices], y[right_indices], criterion, depth=depth+1)\n",
        "\n",
        "        return {\n",
        "            'feature': best_split['feature_index'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left_child': left_child,\n",
        "            'right_child': right_child,\n",
        "            'node_value': node_value,\n",
        "            'unique_times': node_unique_times,\n",
        "            'ids': unique_ids.tolist()\n",
        "        }\n",
        "\n",
        "\n",
        "    def build(self, X, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        The main method to invoke the tree building process.\n",
        "        Initializes the pseudo-score criterion using the input data and constructs the tree using the _build method.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        y = np.c_[ids, time_start, time_stop, event]\n",
        "\n",
        "        unique_times = np.unique(np.concatenate([time_start, time_stop]))\n",
        "        criterion = PseudoScoreCriterion(n_outputs=1, n_samples=n_samples,\n",
        "                                         unique_times=unique_times, x=X, ids=ids,\n",
        "                                         time_start=time_start, time_stop=time_stop, event=event)\n",
        "\n",
        "        tree = self._build(X, y, criterion)\n",
        "        return tree\n"
      ],
      "metadata": {
        "id": "xIntYjd62o4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RecurrentTree"
      ],
      "metadata": {
        "id": "pgntSq2o3Wuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class RecurrentTree(BaseEstimator):\n",
        "    def __init__(self, max_depth=None, min_ids_split=2, min_ids_leaf=1,\n",
        "                 max_features=None, max_thresholds=None, min_impurity_decrease=0,\n",
        "                 random_state=None):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        Initializes the tree's hyperparameters and settings\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_ids_split = min_ids_split\n",
        "        self.min_ids_leaf = min_ids_leaf\n",
        "        self.max_features = max_features\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.random_state = random_state\n",
        "        self.tree_ = None\n",
        "\n",
        "    def fit(self, X, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        Trains the recurrent tree using the input data\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        ids = np.array(ids)\n",
        "        time_start = np.array(time_start)\n",
        "        time_stop = np.array(time_stop)\n",
        "        event = np.array(event)\n",
        "\n",
        "        # Use the PseudoScoreTreeBuilder to build the tree\n",
        "        builder = PseudoScoreTreeBuilder(\n",
        "            max_depth=self.max_depth,\n",
        "            min_ids_split=self.min_ids_split,\n",
        "            min_ids_leaf=self.min_ids_leaf,\n",
        "            max_features=self.max_features,\n",
        "            max_thresholds=self.max_thresholds,\n",
        "            min_impurity_decrease=self.min_impurity_decrease,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_ = builder.build(X, ids, time_start, time_stop, event)\n",
        "        return self\n",
        "\n",
        "    def get_tree(self):\n",
        "        \"\"\"Return the tree as a dictionary.\"\"\"\n",
        "        return self.tree_\n",
        "\n",
        "    def traverse_tree_for_id(self, X_id_samples, node):\n",
        "        \"\"\"\n",
        "        Traverse the tree for a specific ID based on its samples.\n",
        "\n",
        "        Args:\n",
        "        - X_id_samples (list of arrays): The samples corresponding to a specific ID.\n",
        "        - node (dict): The current node being evaluated in the tree.\n",
        "\n",
        "        Returns:\n",
        "        - node (dict): The terminal node for the specific ID.\n",
        "        \"\"\"\n",
        "        if node[\"feature\"] is None:  # Terminal node\n",
        "            return node\n",
        "\n",
        "        # Traverse the tree for each sample and collect the terminal nodes\n",
        "        terminal_nodes = []\n",
        "        for sample in X_id_samples:\n",
        "            if sample[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "                terminal_nodes.append(self.traverse_tree_for_id([sample], node[\"left_child\"]))\n",
        "            else:\n",
        "                terminal_nodes.append(self.traverse_tree_for_id([sample], node[\"right_child\"]))\n",
        "\n",
        "        # Check if all samples lead to the same terminal node\n",
        "        first_terminal = terminal_nodes[0]\n",
        "        if all(node == first_terminal for node in terminal_nodes):\n",
        "            return first_terminal\n",
        "\n",
        "        # If samples lead to different terminal nodes, it's ambiguous. For simplicity, return the first terminal node.\n",
        "        # In a real-world scenario, this might need more sophisticated handling.\n",
        "        return first_terminal\n",
        "\n",
        "    def predict_mean_function(self, X, ids):\n",
        "        \"\"\"\n",
        "        Predict the node_value of the terminal node for given samples.\n",
        "        \"\"\"\n",
        "\n",
        "        # Ensure X is a list of samples\n",
        "        X = np.array(X)\n",
        "\n",
        "        mean_function_predictions = {}\n",
        "\n",
        "        for sample_id in ids:\n",
        "            samples_for_id = [X[i] for i, uid in enumerate(ids) if uid == sample_id]\n",
        "            terminal_node_for_id = self.traverse_tree_for_id(samples_for_id, self.tree_)\n",
        "\n",
        "            mean_function_predictions[sample_id] = terminal_node_for_id[\"node_value\"]\n",
        "\n",
        "        return mean_function_predictions\n",
        "\n",
        "    def predict_rate_function(self, X, ids):\n",
        "        \"\"\"\n",
        "        Predict the rate function as the difference between unique time points for the terminal node.\n",
        "        \"\"\"\n",
        "\n",
        "        if np.isscalar(ids):\n",
        "            ids = [ids]\n",
        "\n",
        "        mean_function_predictions = self.predict_mean_function(X, ids)\n",
        "\n",
        "        rate_function_predictions = {}\n",
        "        for sample_id in ids:\n",
        "            mean_function_values = mean_function_predictions[sample_id]\n",
        "\n",
        "            # Calculate rate function as the difference between consecutive mean function values\n",
        "            rate_function = np.diff(mean_function_values, prepend=mean_function_values[0])\n",
        "\n",
        "            rate_function_predictions[sample_id] = rate_function\n",
        "\n",
        "        return rate_function_predictions\n",
        "\n",
        "    def _map_terminal_nodes(self, node, current_id=[0]):\n",
        "        \"\"\"\n",
        "        Recursively traverse the tree and assign unique integers to each terminal node.\n",
        "        \"\"\"\n",
        "        if node[\"feature\"] is None:  # Terminal node\n",
        "            if \"id\" not in node:\n",
        "                node[\"id\"] = current_id[0]\n",
        "                current_id[0] += 1\n",
        "            return\n",
        "\n",
        "        self._map_terminal_nodes(node[\"left_child\"], current_id)\n",
        "        self._map_terminal_nodes(node[\"right_child\"], current_id)\n",
        "\n",
        "    def apply(self, X, ids=None):\n",
        "        \"\"\"Return the index of the leaf that each unique ID is predicted as.\"\"\"\n",
        "        X = np.array(X, dtype=np.float32)\n",
        "        if ids is None:\n",
        "            ids = np.array([i for i in range(X.shape[0])])\n",
        "        else:\n",
        "            ids = np.array(ids)\n",
        "\n",
        "        terminal_nodes = {}\n",
        "        self._map_terminal_nodes(self.tree_)  # Reset the mapping\n",
        "\n",
        "        for sample_id in np.unique(ids):\n",
        "            samples_for_id = [X[i] for i, uid in enumerate(ids) if uid == sample_id]\n",
        "            terminal_node_for_id = self.traverse_tree_for_id(samples_for_id, self.tree_)\n",
        "            terminal_nodes[sample_id] = terminal_node_for_id[\"id\"]\n",
        "\n",
        "        return terminal_nodes\n"
      ],
      "metadata": {
        "id": "VHGEPDws2o2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bootstrap Sampling Method"
      ],
      "metadata": {
        "id": "KS9eNDCO3YZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numbers import Integral, Real\n",
        "from sklearn.utils import check_random_state\n",
        "from numpy.random import RandomState\n",
        "\n",
        "def check_random_state(seed):\n",
        "    \"\"\"\n",
        "    Check if seed is a valid random state.\n",
        "    \"\"\"\n",
        "    if seed is None or isinstance(seed, int):\n",
        "        return np.random.default_rng(seed)\n",
        "    elif isinstance(seed, np.random.Generator):\n",
        "        return seed\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid seed: {seed}\")\n",
        "\n",
        "def _get_n_ids_bootstrap(n_ids, max_ids):\n",
        "    \"\"\"\n",
        "    Modified for recurrent events. Get the number of IDs in a bootstrap sample.\n",
        "    \"\"\"\n",
        "    if max_ids is None:\n",
        "        return n_ids\n",
        "\n",
        "    if isinstance(max_ids, Integral):\n",
        "        if max_ids > n_ids:\n",
        "            msg = \"`max_samples` must be <= n_ids={} but got value {}\"\n",
        "            raise ValueError(msg.format(n_ids, max_ids))\n",
        "        return max_ids\n",
        "\n",
        "    if isinstance(max_ids, Real):\n",
        "        return max(round(n_ids * max_ids), 1)\n",
        "\n",
        "def _generate_sampled_ids(random_state, unique_ids, max_ids):\n",
        "    \"\"\"\n",
        "    Generate bootstrap sample indices based on unique IDs.\n",
        "    \"\"\"\n",
        "    # Calculate the number of IDs to be sampled using the _get_n_ids_bootstrap function\n",
        "    n_ids_bootstrap = _get_n_ids_bootstrap(len(unique_ids), max_ids)\n",
        "\n",
        "    # Create a random instance with the given random_state\n",
        "    random_instance = check_random_state(random_state)\n",
        "\n",
        "    # Randomly select n_ids_bootstrap IDs from the unique_ids with replacement\n",
        "    sampled_ids_indices = random_instance.choice(len(unique_ids), n_ids_bootstrap, replace=True)\n",
        "\n",
        "    # Get the actual IDs using the indices\n",
        "    sampled_ids = unique_ids[sampled_ids_indices]\n",
        "\n",
        "    return sampled_ids\n",
        "\n",
        "def _generate_unsampled_ids(unique_ids, sampled_ids):\n",
        "    \"\"\"\n",
        "    Determine unsampled unique IDs from the entire set of IDs.\n",
        "    \"\"\"\n",
        "    # 중복 제거된 sampled_ids\n",
        "    unique_sampled_ids = np.unique(sampled_ids)\n",
        "\n",
        "    # Find unsampled unique IDs\n",
        "    unsampled_unique_ids = np.setdiff1d(unique_ids, unique_sampled_ids)\n",
        "    return unsampled_unique_ids\n",
        "\n",
        "\n",
        "from warnings import catch_warnings, simplefilter\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "def _parallel_build_trees(tree, bootstrap, X, y, tree_idx, n_trees,\n",
        "                                    verbose=0, n_ids_bootstrap=None, random_state=None, max_ids=None):\n",
        "    \"\"\"\n",
        "    Private function used to fit a single tree in parallel with corrected y values extraction.\n",
        "    \"\"\"\n",
        "    if verbose > 1:\n",
        "        print(\"Building estimator %d of %d for this parallel run \"\n",
        "              \"(total %d)...\" % (tree_idx + 1, n_trees, n_trees))\n",
        "\n",
        "    ids = y['id']\n",
        "    time_start = y['time_start']\n",
        "    time_stop = y['time_stop']\n",
        "    event = y['event']\n",
        "\n",
        "    # If bootstrap is True, generate a bootstrap sample for training\n",
        "    if bootstrap:\n",
        "        unique_ids = np.unique(ids)\n",
        "        if isinstance(random_state, np.random.RandomState):\n",
        "            rnd = random_state\n",
        "        else:\n",
        "            rnd = np.random.RandomState(random_state)\n",
        "\n",
        "        sampled_ids = _generate_sampled_ids(rnd, unique_ids, max_ids)\n",
        "        bootstrap_indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "        X_bootstrap = X[bootstrap_indices]\n",
        "        ids_bootstrap = ids[bootstrap_indices]\n",
        "        time_start_bootstrap = time_start[bootstrap_indices]\n",
        "        time_stop_bootstrap = time_stop[bootstrap_indices]\n",
        "        event_bootstrap = event[bootstrap_indices]\n",
        "\n",
        "        tree.fit(X_bootstrap, ids_bootstrap, time_start_bootstrap, time_stop_bootstrap, event_bootstrap)\n",
        "    else:\n",
        "        tree.fit(X, ids, time_start, time_stop, event)\n",
        "\n",
        "    return tree\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MQ-ArO3T2oz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Estimate C-index for recurrent event data"
      ],
      "metadata": {
        "id": "lHoIDVhg3b53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_RE_nRE(data_dict):\n",
        "    \"\"\"\n",
        "    Generate RE and nRE matrices from the given data.\n",
        "\n",
        "    Parameters:\n",
        "    - data_dict: A dictionary containing 'id', 'time_start', 'time_stop', and 'event' keys.\n",
        "\n",
        "    Returns:\n",
        "    - RE: A matrix with columns: [id, time_stop, cumulative_event_count].\n",
        "    - nRE: An array containing total number of recurrent events for each ID.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    data_df = pd.DataFrame({\n",
        "        'id': data_dict['id'],\n",
        "        'time_start': data_dict['time_start'],\n",
        "        'time_stop': data_dict['time_stop'],\n",
        "        'event': data_dict['event']\n",
        "    })\n",
        "\n",
        "    data_sorted = data_df.sort_values(by=['id', 'time_start', 'time_stop'])\n",
        "    data_sorted['cum_event'] = data_sorted.groupby('id')['event'].cumsum()\n",
        "\n",
        "    RE = data_sorted[['id', 'time_stop', 'cum_event']].values\n",
        "    nRE = data_sorted.groupby('id')['event'].sum().values\n",
        "\n",
        "    return RE, nRE\n",
        "\n",
        "def generate_cis(data):\n",
        "    # Extract unique IDs\n",
        "    unique_ids = np.unique(data['id'])\n",
        "\n",
        "    # For each unique ID, get the last observed time\n",
        "    common_observation_times = [np.max(data['time_stop'][data['id'] == id_]) for id_ in unique_ids]\n",
        "\n",
        "    # Combine IDs and their corresponding common observation times\n",
        "    Cis = np.column_stack((unique_ids, common_observation_times))\n",
        "\n",
        "    return Cis\n",
        "\n",
        "def est_cstat(score, N, Cis, RE, nRE):\n",
        "    den = 0\n",
        "    num = 0\n",
        "\n",
        "    # Create a mapping from ID to its index in nRE\n",
        "    unique_ids = np.unique(RE[:, 0].astype(int))\n",
        "    id_to_index = {id_: idx for idx, id_ in enumerate(unique_ids)}\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        ID1 = int(Cis[i, 0])\n",
        "\n",
        "        # Filter events happening before or at Cis[i, 2]\n",
        "        search_event_indices = np.where(RE[:, 1] <= Cis[i, 1])[0]  # Use RE's second column (time)\n",
        "        REtemp = RE[search_event_indices, :]\n",
        "\n",
        "        # Count occurrences of each ID in REtemp\n",
        "        unique_ids, counts = np.unique(REtemp[:, 0].astype(int), return_counts=True)\n",
        "\n",
        "        # Check if the indices are valid (i.e., within bounds of N)\n",
        "        valid_indices = [idx for idx in unique_ids if idx < N]\n",
        "        valid_counts = counts[np.isin(unique_ids, valid_indices)]\n",
        "\n",
        "        # Update nREc using the valid indices and counts\n",
        "        nREc = np.zeros(N, dtype=int)\n",
        "        nREc[valid_indices] = valid_counts\n",
        "\n",
        "        # Sort the IDs for the remaining pairs\n",
        "        IDpair = np.sort(Cis[(i+1):N, 0])\n",
        "        IDpair = [id_ for id_ in IDpair if id_ in score and id_ != -1]\n",
        "        indexed_IDpair = [id_to_index[id_] for id_ in IDpair if id_ in id_to_index]\n",
        "        nREc = nREc[indexed_IDpair]\n",
        "\n",
        "        lt_obs = np.where(nRE[ID1] < nREc, 1, 0)\n",
        "        gt_obs = np.where(nRE[ID1] > nREc, 1, 0)\n",
        "\n",
        "        score_values = np.array([score.get(id_, [0])[-1] for id_ in IDpair])\n",
        "        score_current = np.full(score_values.shape, score.get(ID1, [0])[-1])\n",
        "\n",
        "        lt_pred = np.where(score_current < score_values, 1, 0)\n",
        "        gt_pred = np.where(score_current > score_values, 1, 0)\n",
        "\n",
        "        den += np.sum(lt_obs + gt_obs)\n",
        "        num += np.sum(lt_obs * lt_pred + gt_obs * gt_pred)\n",
        "\n",
        "    estcstat = num / den if den != 0 else 0\n",
        "    return estcstat"
      ],
      "metadata": {
        "id": "vGLDU8eu2owT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RecurrentRandomForest"
      ],
      "metadata": {
        "id": "yWIzyjNX3e3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.utils import check_array\n",
        "from joblib import Parallel, delayed\n",
        "from numpy.random import RandomState\n",
        "from sklearn.utils import check_random_state, check_array\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "from scipy.sparse import issparse\n",
        "MAX_INT = np.iinfo(np.int32).max\n",
        "\n",
        "\n",
        "from sklearn.utils import check_random_state\n",
        "import numpy as np\n",
        "\n",
        "def _generate_bootstrap_indices(tree, bootstrap, X, y, random_state, max_ids):\n",
        "    \"\"\"\n",
        "    Private function used to generate bootstrap sample indices in parallel.\n",
        "    \"\"\"\n",
        "    if bootstrap:\n",
        "        unique_ids = np.unique(y['id'])\n",
        "        if isinstance(random_state, np.random.RandomState):\n",
        "            rnd = random_state\n",
        "        else:\n",
        "            rnd = np.random.RandomState(random_state)\n",
        "\n",
        "        sampled_ids = _generate_sampled_ids(rnd, unique_ids, max_ids)\n",
        "        bootstrap_indices = np.where(np.isin(y['id'], sampled_ids))[0]\n",
        "        return bootstrap_indices\n",
        "    else:\n",
        "        return np.arange(len(y['id']))  # return all indices\n",
        "\n",
        "def _fit_tree_with_bootstrap_samples(tree, X, y_converted, indices):\n",
        "    \"\"\"\n",
        "    Fit a single tree with given bootstrap samples.\n",
        "    \"\"\"\n",
        "    X_bootstrap = X[indices]\n",
        "    y_bootstrap = {\n",
        "        'id': y_converted['id'][indices],\n",
        "        'time_start': y_converted['time_start'][indices],\n",
        "        'time_stop': y_converted['time_stop'][indices],\n",
        "        'event': y_converted['event'][indices]\n",
        "    }\n",
        "    tree.fit(X_bootstrap, y_bootstrap['id'], y_bootstrap['time_start'], y_bootstrap['time_stop'], y_bootstrap['event'])\n",
        "    return tree\n",
        "\n",
        "def _get_unsampled_bootstrap_indices(tree, bootstrap, X, y, random_state, max_ids):\n",
        "    \"\"\"\n",
        "    Private function used to generate unsampled bootstrap sample indices in parallel.\n",
        "    \"\"\"\n",
        "    if bootstrap:\n",
        "        unique_ids = np.unique(y['id'])\n",
        "        if isinstance(random_state, np.random.RandomState):\n",
        "            rnd = random_state\n",
        "        else:\n",
        "            rnd = np.random.RandomState(random_state)\n",
        "\n",
        "        sampled_ids = _generate_sampled_ids(rnd, unique_ids, max_ids)\n",
        "        unsampled_ids = _generate_unsampled_ids(unique_ids, sampled_ids)\n",
        "        unsampled_indices = np.where(np.isin(y['id'], unsampled_ids))[0]\n",
        "        return unsampled_indices\n",
        "    else:\n",
        "        return np.array([])  # return an empty array for non-bootstrap cases\n",
        "\n",
        "\n",
        "class RecurrentRandomForest(BaseEstimator):\n",
        "\n",
        "    \"\"\"\n",
        "    A Random Forest model designed for recurrent event data.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, max_depth=None, min_ids_split=2,\n",
        "                 min_ids_leaf=1, bootstrap=True, oob_score=False, n_jobs=None,\n",
        "                 random_state=None, verbose=0, warm_start=False, max_ids=1.0,\n",
        "                 min_impurity_decrease=0.0, max_features=None, max_thresholds=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_ids_split = min_ids_split\n",
        "        self.min_ids_leaf = min_ids_leaf\n",
        "        self.bootstrap = bootstrap\n",
        "        self.oob_score = oob_score\n",
        "        self.n_jobs = n_jobs\n",
        "        self.verbose = verbose\n",
        "        self.warm_start = warm_start\n",
        "        self.max_ids = max_ids\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.max_features = max_features\n",
        "        self.max_thresholds = max_thresholds\n",
        "\n",
        "        # Initialize the random state for the forest\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "        # Create the estimators using the updated random states\n",
        "        self.estimators_ = [self._make_estimator() for _ in range(self.n_estimators)]\n",
        "\n",
        "    def _make_estimator(self):\n",
        "        \"\"\"\n",
        "        Constructs a new instance of the 'RecurrentTree' with the specified hyperparameters.\n",
        "        Allows for creating each tree with a different 'random_state' for randomness.\n",
        "        \"\"\"\n",
        "        # Generate a new random state for each tree based on the forest's random state\n",
        "        if isinstance(self.random_state, np.random.Generator):\n",
        "            tree_random_state = self.random_state.integers(np.iinfo(np.int32).max)\n",
        "        else:\n",
        "            tree_random_state = self.random_state.randint(np.iinfo(np.int32).max)\n",
        "\n",
        "        return RecurrentTree(\n",
        "            max_depth=self.max_depth,\n",
        "            min_ids_split=self.min_ids_split,\n",
        "            min_ids_leaf=self.min_ids_leaf,\n",
        "            random_state=tree_random_state,  # Pass the generated random state for the tree\n",
        "            min_impurity_decrease=self.min_impurity_decrease,\n",
        "            max_features=self.max_features,\n",
        "            max_thresholds=self.max_thresholds\n",
        "        )\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Build the recurrent random forest.\n",
        "        \"\"\"\n",
        "        # Validate the input data\n",
        "        X = self._validate_data(X)\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "\n",
        "        # Convert y to the required format\n",
        "        y_converted = {\n",
        "            'id': y['id'],\n",
        "            'time_start': y['time_start'],\n",
        "            'time_stop': y['time_stop'],\n",
        "            'event': y['event']\n",
        "        }\n",
        "\n",
        "        # If max_ids is None, set it to 1.0\n",
        "        if self.max_ids is None:\n",
        "            self.max_ids = 1.0\n",
        "\n",
        "        # Get bootstrap indices for each tree in parallel\n",
        "        bootstrap_indices_list = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(_generate_bootstrap_indices)(\n",
        "                tree=tree,\n",
        "                bootstrap=self.bootstrap,\n",
        "                X=X,\n",
        "                y=y_converted,\n",
        "                random_state=tree.random_state,\n",
        "                max_ids=self.max_ids\n",
        "            ) for tree in self.estimators_\n",
        "        )\n",
        "\n",
        "        # Get unsampled bootstrap indices for each tree in parallel\n",
        "        unsampled_bootstrap_indices_list = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(_get_unsampled_bootstrap_indices)(\n",
        "                tree=tree,\n",
        "                bootstrap=self.bootstrap,\n",
        "                X=X,\n",
        "                y=y_converted,\n",
        "                random_state=tree.random_state,\n",
        "                max_ids=self.max_ids\n",
        "            ) for tree in self.estimators_\n",
        "        )\n",
        "\n",
        "        self.unsampled_bootstrap_indices_list_ = unsampled_bootstrap_indices_list\n",
        "\n",
        "        # Train each tree using its respective bootstrap indices in parallel\n",
        "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(_fit_tree_with_bootstrap_samples)(\n",
        "                tree=tree,\n",
        "                X=X,\n",
        "                y_converted=y_converted,\n",
        "                indices=indices\n",
        "            ) for tree, indices in zip(self.estimators_, bootstrap_indices_list)\n",
        "        )\n",
        "\n",
        "            # Calculate OOB score and attributes if needed\n",
        "        if self.oob_score:\n",
        "            self._set_oob_score_and_attributes(X, y_converted)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _set_oob_score_and_attributes(self, X, y):\n",
        "        \"\"\"\n",
        "        Calculate OOB score and attributes.\n",
        "        \"\"\"\n",
        "        # OOB 예측값만 추출\n",
        "        averaged_predictions = {}\n",
        "        all_unsampled_indices = set()  # 모든 추정기를 걸쳐서 unsampled된 인덱스 수집\n",
        "\n",
        "        for estimator, unsampled_indices in zip(self.estimators_, self.unsampled_bootstrap_indices_list_):\n",
        "            X_unsampled = X[unsampled_indices]\n",
        "            y_unsampled = {key: y[key][unsampled_indices] for key in y}\n",
        "\n",
        "            p_estimator_result = estimator.predict_mean_function(X_unsampled, y_unsampled[\"id\"])\n",
        "\n",
        "            for uid, pred in p_estimator_result.items():\n",
        "                if uid not in averaged_predictions:\n",
        "                    averaged_predictions[uid] = []\n",
        "                averaged_predictions[uid].append(pred)\n",
        "\n",
        "            # unsampled_indices를 all_unsampled_indices에 추가\n",
        "            all_unsampled_indices.update(unsampled_indices)\n",
        "\n",
        "        # set을 list로 변환\n",
        "        all_unsampled_indices = list(all_unsampled_indices)\n",
        "\n",
        "        # all_unsampled_indices를 사용하여 y_unsampled 추출\n",
        "        y_unsampled = {key: y[key][all_unsampled_indices] for key in y}\n",
        "\n",
        "        # 패딩된 예측값의 평균 계산\n",
        "        for uid, predictions in averaged_predictions.items():\n",
        "            averaged_predictions[uid] = self._pad_and_average_predictions(predictions, len(self.estimators_))\n",
        "\n",
        "        self.oob_prediction_ = averaged_predictions\n",
        "\n",
        "        # unsampled 데이터를 사용하여 RE와 nRE 행렬 생성\n",
        "        RE, nRE = generate_RE_nRE(y_unsampled)\n",
        "\n",
        "        # unsampled 데이터를 사용하여 Cis 생성\n",
        "        Cis = generate_cis(y_unsampled)\n",
        "\n",
        "        N = len(Cis)\n",
        "\n",
        "        # est_cstat 함수를 사용하여 C-index 계산\n",
        "        self.oob_score_ = est_cstat(self.oob_prediction_, N, Cis, RE, nRE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\"Validate input data('X') to ensure it's in the correct format and meets the necessary conditions for processing.\"\"\"\n",
        "    def _validate_data(self, X, accept_sparse=False, ensure_min_samples=1):\n",
        "        \"\"\"Validate input data('X') to ensure it's in the correct format and meets the necessary conditions for processing.\"\"\"\n",
        "        return check_array(X, accept_sparse=accept_sparse, ensure_min_samples=ensure_min_samples)\n",
        "\n",
        "    def _validate_X_predict(self, X):\n",
        "        \"\"\"Validate X whenever one tries to predict.\"\"\"\n",
        "        X = check_array(X)\n",
        "        if X.shape[1] != self.n_features_in_:\n",
        "            raise ValueError(\"Number of features of the model must match the input. Model n_features is {} and input n_features is {}.\"\n",
        "                             .format(self.n_features_in_, X.shape[1]))\n",
        "        return X\n",
        "\n",
        "    def _pad_and_average_predictions(self, all_predictions_for_id, n_trees):\n",
        "        \"\"\"\n",
        "        Pad the predictions to the length of the longest prediction and then average them.\n",
        "        \"\"\"\n",
        "        max_length = max(map(len, all_predictions_for_id))\n",
        "\n",
        "        # Pad each prediction to the maximum length\n",
        "        padded_predictions = []\n",
        "        for prediction in all_predictions_for_id:\n",
        "            if len(prediction) < max_length:\n",
        "                pad_length = max_length - len(prediction)\n",
        "                padded_prediction = np.concatenate([prediction, [prediction[-1]] * pad_length])\n",
        "            else:\n",
        "                padded_prediction = prediction\n",
        "            padded_predictions.append(padded_prediction)\n",
        "\n",
        "        # Average the padded predictions\n",
        "        average_prediction = np.mean(padded_predictions, axis=0)\n",
        "        return average_prediction.tolist()\n",
        "\n",
        "    def predict_mean_function(self, X, ids):\n",
        "        X = self._validate_X_predict(X)\n",
        "\n",
        "        # 각 tree로부터의 예측을 저장하는 딕셔너리 초기화\n",
        "        all_predictions = {uid: [] for uid in np.unique(ids)}\n",
        "\n",
        "        for tree in self.estimators_:\n",
        "            tree_predictions = tree.predict_mean_function(X, ids)\n",
        "            for uid, pred in tree_predictions.items():\n",
        "                all_predictions[uid].append(pred)\n",
        "\n",
        "        # 예측값 리스트를 패딩하고 평균 계산\n",
        "        averaged_predictions = {}\n",
        "        for uid, predictions in all_predictions.items():\n",
        "            averaged_predictions[uid] = self._pad_and_average_predictions(predictions, len(self.estimators_))\n",
        "\n",
        "        return averaged_predictions\n",
        "\n",
        "    def predict_rate_function(self, X, ids):\n",
        "        X = self._validate_X_predict(X)\n",
        "\n",
        "        # 각 tree로부터의 예측을 저장하는 딕셔너리 초기화\n",
        "        all_predictions = {uid: [] for uid in np.unique(ids)}\n",
        "\n",
        "        for tree in self.estimators_:\n",
        "            tree_predictions = tree.predict_rate_function(X, ids)\n",
        "            for uid, pred in tree_predictions.items():\n",
        "                all_predictions[uid].append(pred)\n",
        "\n",
        "        # 예측값 리스트를 패딩하고 평균 계산\n",
        "        averaged_predictions = {}\n",
        "        for uid, predictions in all_predictions.items():\n",
        "            averaged_predictions[uid] = self._pad_and_average_predictions(predictions, len(self.estimators_))\n",
        "\n",
        "        return averaged_predictions\n",
        "\n",
        "\n",
        "RecurrentRandomForest"
      ],
      "metadata": {
        "id": "VXJLsEoI2osM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Permutation Importance"
      ],
      "metadata": {
        "id": "SchsQe1m3hM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recurrent_permutation_importance(model, X, y, n_repeats=30, random_state=None):\n",
        "    \"\"\"\n",
        "    주어진 RecurrentRandomForest 모델에 대한 특성의 Permutation Importance를 계산합니다.\n",
        "    이 함수는 아이디별로 데이터를 섞되, 특성 행렬의 값은 섞인 아이디의 첫 번째 값으로 채웁니다.\n",
        "\n",
        "    Parameters:\n",
        "    - model: RecurrentRandomForest 모델\n",
        "    - X: 입력 특성\n",
        "    - y: 타겟\n",
        "    - n_repeats: 중요도를 계산하기 위해 각 특성을 섞을 횟수.\n",
        "    - random_state: 재현성을 위한 시드\n",
        "\n",
        "    Returns:\n",
        "    - importances: 각 특성의 Permutation Importance를 포함하는 2D 배열.\n",
        "    - importances_mean: 각 특성의 중요도 평균.\n",
        "    - importances_std: 각 특성의 중요도 표준편차.\n",
        "    \"\"\"\n",
        "\n",
        "    random_state = check_random_state(random_state)\n",
        "\n",
        "    # 원래 데이터를 사용하여 예측 수행\n",
        "    baseline_preds = model.predict_mean_function(X, y['id'])\n",
        "    RE, nRE = generate_RE_nRE(y)\n",
        "    Cis = generate_cis(y)\n",
        "    N = len(Cis)\n",
        "    baseline_score = est_cstat(baseline_preds, N, Cis, RE, nRE)\n",
        "\n",
        "    n_features = X.shape[1]\n",
        "    unique_ids = np.unique(y['id'])\n",
        "    importances = np.zeros((n_repeats, n_features))\n",
        "\n",
        "    for feature_idx in range(n_features):\n",
        "        for repeat in range(n_repeats):\n",
        "            X_permuted = X.copy()\n",
        "\n",
        "            # Shuffle data by ID\n",
        "            shuffled_ids = random_state.permutation(unique_ids)\n",
        "            for original_id, shuffled_id in zip(unique_ids, shuffled_ids):\n",
        "                idx_original = np.where(y['id'] == original_id)[0]\n",
        "                idx_shuffled = np.where(y['id'] == shuffled_id)[0]\n",
        "                X_permuted[idx_original, feature_idx] = X[idx_shuffled[0], feature_idx]\n",
        "\n",
        "            # 섞인 데이터를 사용하여 예측 수행\n",
        "            preds_permuted = model.predict_mean_function(X_permuted, y['id'])\n",
        "            score_permuted = est_cstat(preds_permuted, N, Cis, RE, nRE)\n",
        "\n",
        "            # 특성의 중요도는 모델의 성능이 임의로 섞였을 때 얼마나 감소하는지를 기반으로 합니다.\n",
        "            importances[repeat, feature_idx] = baseline_score - score_permuted\n",
        "\n",
        "    importances_mean = importances.mean(axis=0)\n",
        "    importances_std = importances.std(axis=0)\n",
        "\n",
        "    return importances, importances_mean, importances_std\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OTW--ufS2op9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}