{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSK2022/RandomForest-for-Recurrent-Events/blob/Thesis-Code/JS_simulation_code_230901_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GThv_VDEEWx0"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-survival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfYDm4PeJx8W"
      },
      "outputs": [],
      "source": [
        "# Update the RisksetCounter class again with the above mentioned changes\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "#from functools import lru_cache\n",
        "\n",
        "class RisksetCounter:\n",
        "    def __init__(self, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        클래스 초기화\n",
        "        중복 없는 고유한 time_stop 값을 정렬, all_unique_times에 저장, all_unique_times의 길이를 n_unique_times에 저장\n",
        "        n_at_risk, n_events를 0으로 초기화, set_data 호출하여 data 설정\n",
        "        \"\"\"\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "\n",
        "        self.all_unique_times = np.unique(time_stop)\n",
        "        self.n_unique_times = len(self.all_unique_times)\n",
        "\n",
        "        self.n_at_risk = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.n_events = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.set_data()\n",
        "\n",
        "    def set_data(self):\n",
        "        \"\"\"\n",
        "        all_unique_times에 있는 각 시간에 대한 Riskset과 number of events 계산\n",
        "        \"\"\"\n",
        "        for t_idx, t in enumerate(self.all_unique_times):\n",
        "            self.n_at_risk[t_idx] = sum([self.Y_i(id_, t_idx) for id_ in set(self.ids)])\n",
        "            self.n_events[t_idx] = sum([self.dN_bar_i(id_, t_idx) for id_ in set(self.ids)])\n",
        "\n",
        "    # @lru_cache(maxsize=None)  # Unbounded cache\n",
        "    def Y_i(self, id_, t_idx):\n",
        "        \"\"\"\n",
        "        주어진 id_와 시간 인덱스 t_idx에 대해 해당 ID의 개체가 위험 집합에 있는지 반환 - 있으면 1, 없으면 0\n",
        "        \"\"\"\n",
        "        indices = [i for i, x in enumerate(self.ids) if x == id_]\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        for index in indices:\n",
        "            tau_i = self.time_stop[index]\n",
        "            if time_at_t_idx <= tau_i:\n",
        "                return 1\n",
        "        return 0\n",
        "\n",
        "    # @lru_cache(maxsize=None)  # Unbounded cache\n",
        "    def dN_bar_i(self, id_, t_idx):\n",
        "        \"\"\"\n",
        "        주어진 id_와 시간 인덱스 t_idx에 대해 해당 ID의 subject의 event 발생 여부 - 발생 시 1, 아닐 시 0\n",
        "        \"\"\"\n",
        "        indices = [i for i, x in enumerate(self.ids) if x == id_]\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        for index in indices:\n",
        "            if time_at_t_idx == self.time_stop[index] and self.event[index] == 1:\n",
        "                return self.Y_i(id_, t_idx)\n",
        "        return 0\n",
        "\n",
        "    def update(self, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        새로운 데이터를 인스턴스 변수에 추가, riskset 및 number of events 업데이트\n",
        "        \"\"\"\n",
        "        self.ids = np.concatenate([self.ids, ids])\n",
        "        self.time_start = np.concatenate([self.time_start, time_start])\n",
        "        self.time_stop = np.concatenate([self.time_stop, time_stop])\n",
        "        self.event = np.concatenate([self.event, event])\n",
        "\n",
        "        # Clear the cache as the input data has changed\n",
        "        #self.Y_i.cache_clear()\n",
        "        #self.dN_bar_i.cache_clear()\n",
        "\n",
        "        for t_idx, t in enumerate(self.all_unique_times):\n",
        "            self.n_at_risk[t_idx] = sum([self.Y_i(id_, t_idx) for id_ in set(self.ids)])\n",
        "            self.n_events[t_idx] = sum([self.dN_bar_i(id_, t_idx) for id_ in set(self.ids)])\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        n_at_risk, n_events 데이터 구조를 재설정\n",
        "        \"\"\"\n",
        "        # Reset the data structures and clear the cache\n",
        "        self.n_at_risk.fill(0)\n",
        "        self.n_events.fill(0)\n",
        "        #self.Y_i.cache_clear()\n",
        "        #self.dN_bar_i.cache_clear()\n",
        "\n",
        "    def copy(self):\n",
        "        \"\"\"\n",
        "        현재 객체 복사\n",
        "        \"\"\"\n",
        "        return RisksetCounter(self.ids.copy(), self.time_start.copy(), self.time_stop.copy(), self.event.copy())\n",
        "\n",
        "    def __reduce__(self):\n",
        "        \"\"\"\n",
        "        객체의 생성자와 생성자의 인수 반환\n",
        "        \"\"\"\n",
        "        # Return a tuple of class constructor and its arguments to bypass caching\n",
        "        return (self.__class__, (self.ids, self.time_start, self.time_stop, self.event))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QCDyBxyJx8X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "path=\"/Users/jeongsookim/Downloads\"\n",
        "data = pd.read_csv(f\"{path}/simuDat.csv\")\n",
        "\n",
        "ids = data['ID'].values\n",
        "time_start = data['start'].values\n",
        "time_stop = data['stop'].values\n",
        "event = data['event'].values\n",
        "x = data[['group','x1','gender']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prWzlfosJx8Y"
      },
      "outputs": [],
      "source": [
        "counter = RisksetCounter(ids, time_start, time_stop, event)\n",
        "initial_n_at_risk = counter.n_at_risk.copy()\n",
        "initial_n_events = counter.n_events.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-pGqGEUJx8Y"
      },
      "outputs": [],
      "source": [
        "initial_n_at_risk ##위험 집합의 수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H0pHyYOJx8Y"
      },
      "outputs": [],
      "source": [
        "initial_n_events ## 각 위험 집합에 따른 사건 발생 횟수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AEBAF1au4VD"
      },
      "outputs": [],
      "source": [
        "def argbinsearch(arr, key_val):\n",
        "    arr_len = len(arr)\n",
        "    min_idx = 0\n",
        "    max_idx = arr_len\n",
        "\n",
        "    while min_idx < max_idx:\n",
        "        mid_idx = min_idx + ((max_idx - min_idx) // 2)\n",
        "\n",
        "        if mid_idx < 0 or mid_idx >= arr_len:\n",
        "            return -1\n",
        "\n",
        "        mid_val = arr[mid_idx]\n",
        "        if mid_val <= key_val:  # Change the condition to <=\n",
        "            min_idx = mid_idx + 1\n",
        "        else:\n",
        "            max_idx = mid_idx\n",
        "\n",
        "    return min_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH11s9dDCZLK"
      },
      "source": [
        "이 함수는 argbinsearch라는 이름의 함수로, 배열에서 주어진 키 값보다 크거나 같은 첫 번째 원소의 인덱스를 이진 탐색으로 찾아 반환합니다.\n",
        "\n",
        "자세한 코드 설명을 아래에 제공합니다:\n",
        "\n",
        "1. 입력:\n",
        "\n",
        "  * arr: 탐색 대상인 정렬된 배열\n",
        "  key_val: 찾고자 하는 키 값\n",
        "\n",
        "2. 초기 변수 설정:\n",
        "\n",
        "  * arr_len: 배열의 길이를 저장합니다.\n",
        "  * min_idx: 탐색 범위의 최솟값으로, 처음에는 배열의 시작 인덱스인 0으로 설정됩니다.\n",
        "  * max_idx: 탐색 범위의 최댓값으로, 처음에는 배열의 길이로 설정됩니다.\n",
        "\n",
        "3. 이진 탐색:\n",
        "\n",
        "  * while 루프를 사용하여 min_idx가 max_idx보다 작은 동안 탐색을 반복합니다.\n",
        "  * mid_idx: 현재 탐색 범위의 중간 인덱스를 계산합니다.\n",
        "  * mid_val: 중간 인덱스에 해당하는 배열의 원소 값을 가져옵니다.\n",
        "\n",
        "4. 키 값과 중간 값을 비교합니다:\n",
        "  * 만약 중간 값이 키 값보다 작거나 같으면, min_idx를 mid_idx + 1로 업데이트합니다. 이렇게 하면 탐색 범위의 왼쪽 부분을 제외하게 됩니다.\n",
        "  * 그렇지 않으면, max_idx를 mid_idx로 업데이트합니다. 이렇게 하면 탐색 범위의 오른쪽 부분을 제외하게 됩니다.\n",
        "\n",
        "5. 결과 반환:\n",
        "\n",
        "  * 루프가 종료되면, min_idx는 키 값보다 크거나 같은 첫 번째 원소의 인덱스를 가리키게 됩니다. 따라서 min_idx를 반환합니다.\n",
        "\n",
        "이 함수는 정렬된 배열에서 주어진 키 값보다 크거나 같은 첫 번째 원소의 위치를 효율적으로 찾기 위해 사용됩니다. 이진 탐색은 배열의 중간 값을 반복적으로 확인하면서 탐색 범위를 절반씩 줄여나가므로, 큰 배열에서도 빠르게 원하는 값을 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBl19p45Jx8Z"
      },
      "outputs": [],
      "source": [
        "# 필요한 라이브러리 및 함수 임포트\n",
        "import numpy as np\n",
        "\n",
        "class PseudoScoreCriterion:\n",
        "    def __init__(self, n_outputs, n_samples, unique_times, x, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        Initialize instance variables using the provided input parameters\n",
        "        Objects 'riskset_left', 'riskset_right', and 'riskset_total' are initialized using the 'RisksetCounter' class\n",
        "        \"\"\"\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_samples = n_samples\n",
        "        self.unique_times = unique_times\n",
        "        self.x = x\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "\n",
        "        self.unique_ids = set(self.ids)  # Store unique ids for later use\n",
        "\n",
        "        self.riskset_left = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        self.riskset_right = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        self.riskset_total = RisksetCounter(ids, time_start, time_stop, event)\n",
        "\n",
        "        self.samples_time_idx = np.searchsorted(unique_times, time_stop)\n",
        "\n",
        "        self.split_pos = 0\n",
        "        self.split_time_idx = 0\n",
        "\n",
        "    def init(self, y, sample_weight, n_samples, samples, start, end):\n",
        "        \"\"\"\n",
        "        Initialization function\n",
        "        Reset the risk set counters ('riskset_left','riskset_right','riskset_total') and updates 'riskset_total' with new data\n",
        "        \"\"\"\n",
        "        self.samples = samples\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "        self.riskset_total.reset()\n",
        "\n",
        "        time_starts, stop_times, events = y[:, 0], y[:, 1], y[:, 2]\n",
        "        ids_for_update = [self.ids[idx] for idx in samples[start:end]]\n",
        "        time_starts_for_update = [time_starts[idx] for idx in samples[start:end]]\n",
        "        stop_times_for_update = [stop_times[idx] for idx in samples[start:end]]\n",
        "        events_for_update = [events[idx] for idx in samples[start:end]]\n",
        "\n",
        "        self.riskset_total.update(ids_for_update, time_starts_for_update, stop_times_for_update, events_for_update)\n",
        "\n",
        "    def update(self, new_pos, split_feature, split_threshold):\n",
        "        \"\"\"\n",
        "        Update the criterion with a potential new split.\n",
        "\n",
        "        Parameters:\n",
        "            - new_pos: The position of the split in the sorted feature values.\n",
        "            - split_feature: The feature index on which the split is made.\n",
        "            - split_threshold: The threshold value for the split.\n",
        "        \"\"\"\n",
        "        # Determine which samples fall to the left and right of the split\n",
        "        is_left = self.x[:, split_feature] <= split_threshold\n",
        "        is_right = np.logical_not(is_left)\n",
        "\n",
        "        # Get the relevant ids, start, stop, and event data for left and right splits\n",
        "        ids_left = np.array(self.ids)[is_left]\n",
        "        ids_right = np.array(self.ids)[is_right]\n",
        "\n",
        "        start_left = np.array(self.time_start)[is_left]\n",
        "        start_right = np.array(self.time_stop)[is_right]\n",
        "\n",
        "        stop_left = np.array(self.time_stop)[is_left]\n",
        "        stop_right = np.array(self.time_stop)[is_right]\n",
        "\n",
        "        event_left = np.array(self.event)[is_left]\n",
        "        event_right = np.array(self.event)[is_right]\n",
        "\n",
        "        # 한 번의 메서드 호출로 모든 값을 업데이트합니다.\n",
        "        self.riskset_left.update(ids_left, start_left, stop_left, event_left)\n",
        "        self.riskset_right.update(ids_right, start_right, stop_right, event_right)\n",
        "\n",
        "    #@lru_cache(maxsize=None)\n",
        "        \"\"\"\n",
        "        Functions returning the risk set value and event value for the given ID and time index from the respective risk set (left or right)\n",
        "        \"\"\"\n",
        "    def Y_left_value(self, id_, t):\n",
        "        return self.riskset_left.Y_i(id_, t)\n",
        "\n",
        "    #@lru_cache(maxsize=None)\n",
        "    def Y_right_value(self, id_, t):\n",
        "        return self.riskset_right.Y_i(id_, t)\n",
        "\n",
        "    #@lru_cache(maxsize=None)\n",
        "    def dN_bar_left_value(self, id_, t):\n",
        "        return self.riskset_left.dN_bar_i(id_, t)\n",
        "\n",
        "    #@lru_cache(maxsize=None)\n",
        "    def dN_bar_right_value(self, id_, t):\n",
        "        return self.riskset_right.dN_bar_i(id_, t)\n",
        "\n",
        "    def calculate_variance_estimate(self):\n",
        "        \"\"\"\n",
        "        Functions to compute the variance estimate for the split\n",
        "        \"\"\"\n",
        "        left_n_at_risk = self.riskset_left.n_at_risk + 1e-7\n",
        "        right_n_at_risk = self.riskset_right.n_at_risk + 1e-7\n",
        "\n",
        "        w = (left_n_at_risk * right_n_at_risk) / (left_n_at_risk + right_n_at_risk)\n",
        "\n",
        "        # Expand w and n_at_risk arrays to match the size of Y_left and Y_right\n",
        "        w_expanded = np.tile(w, len(self.unique_ids))\n",
        "        left_n_at_risk_expanded = np.tile(left_n_at_risk, len(self.unique_ids))\n",
        "        right_n_at_risk_expanded = np.tile(right_n_at_risk, len(self.unique_ids))\n",
        "\n",
        "        Y_left, Y_right, term_left, term_right = [], [], [], []\n",
        "\n",
        "        for id_ in self.unique_ids:\n",
        "            for t in range(self.riskset_left.n_unique_times):\n",
        "                Y_left_val = self.Y_left_value(id_, t)\n",
        "                Y_right_val = self.Y_right_value(id_, t)\n",
        "\n",
        "                dN_bar_left_val = self.dN_bar_left_value(id_, t)\n",
        "                dN_bar_right_val = self.dN_bar_right_value(id_, t)\n",
        "\n",
        "                term_left_val = (dN_bar_left_val - (self.riskset_left.n_events[t] / left_n_at_risk[t])) ** 2\n",
        "                term_right_val = (dN_bar_right_val - (self.riskset_right.n_events[t] / right_n_at_risk[t])) ** 2\n",
        "\n",
        "                Y_left.append(Y_left_val)\n",
        "                Y_right.append(Y_right_val)\n",
        "                term_left.append(term_left_val)\n",
        "                term_right.append(term_right_val)\n",
        "\n",
        "        Y_left = np.array(Y_left)\n",
        "        Y_right = np.array(Y_right)\n",
        "        term_left = np.array(term_left)\n",
        "        term_right = np.array(term_right)\n",
        "\n",
        "        var_estimate_L = np.sum(w_expanded * (Y_left / left_n_at_risk_expanded) * term_left)\n",
        "        var_estimate_R = np.sum(w_expanded * (Y_right / right_n_at_risk_expanded) * term_right)\n",
        "\n",
        "        return var_estimate_L + var_estimate_R\n",
        "\n",
        "\n",
        "    def proxy_impurity_improvement(self):\n",
        "        \"\"\"\n",
        "        Functions that calculates the pseudo impurity improvement of the split\n",
        "        This value represents the reduction in pseudo impurity in the risk sets after the split\n",
        "        \"\"\"\n",
        "        left_n_at_risk = self.riskset_left.n_at_risk + 1e-7\n",
        "        right_n_at_risk = self.riskset_right.n_at_risk + 1e-7\n",
        "\n",
        "        w = (left_n_at_risk * right_n_at_risk) / (left_n_at_risk + right_n_at_risk)\n",
        "        term = (self.riskset_left.n_events / left_n_at_risk) - (self.riskset_right.n_events / right_n_at_risk)\n",
        "        numer = np.sum(w * term)\n",
        "        var_estimate = self.calculate_variance_estimate()\n",
        "\n",
        "        return numer / (np.sqrt(var_estimate) + 1e-7)\n",
        "\n",
        "    def node_value(self):\n",
        "        \"\"\"\n",
        "        Returns the expected risk value of the node\n",
        "        \"\"\"\n",
        "        total_n_at_risk = self.riskset_left.n_at_risk + self.riskset_right.n_at_risk + 1e-7\n",
        "        return np.cumsum(self.riskset_left.n_events + self.riskset_right.n_events) / total_n_at_risk\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Functions to reset all risk set counters\n",
        "        \"\"\"\n",
        "        self.riskset_total.reset()\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "        #self.Y_left_value.cache_clear()\n",
        "        #self.Y_right_value.cache_clear()\n",
        "        #self.dN_bar_left_value.cache_clear()\n",
        "        #self.dN_bar_right_value.cache_clear()\n",
        "\n",
        "    def copy(self):\n",
        "        \"\"\"\n",
        "        Creates and returns a copy of the current object.\n",
        "        \"\"\"\n",
        "        new_criterion = PseudoScoreCriterion(self.n_outputs, self.n_samples, self.unique_times,\n",
        "                                                     self.x, self.ids, self.time_start, self.time_stop,\n",
        "                                                     self.event)\n",
        "        new_criterion.riskset_left = self.riskset_left.copy()\n",
        "        new_criterion.riskset_right = self.riskset_right.copy()\n",
        "        new_criterion.riskset_total = self.riskset_total.copy()\n",
        "        new_criterion.samples_time_idx = self.samples_time_idx.copy()\n",
        "        if hasattr(self, 'samples'):\n",
        "            new_criterion.samples = self.samples.copy()\n",
        "\n",
        "        return new_criterion\n",
        "\n",
        "# 주어진 코드를 기반으로 수정된 PseudoScoreCriterion 클래스를 정의하였습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVDmDgI2Jx8a"
      },
      "outputs": [],
      "source": [
        "import cProfile\n",
        "import pstats\n",
        "\n",
        "n_samples = 500\n",
        "n_features = 3\n",
        "unique_times = np.unique(time_stop)\n",
        "\n",
        "# PseudoScoreCriterion 초기화\n",
        "criterion = PseudoScoreCriterion(n_outputs=n_features, n_samples=n_samples,\n",
        "                                 unique_times=np.unique(np.concatenate([time_start, time_stop])),\n",
        "                                 x=x, ids=ids, time_start=time_start, time_stop=time_stop, event=event)\n",
        "\n",
        "# init 메서드를 사용하여 criterion 초기화\n",
        "y = np.column_stack([time_start, time_stop, event])\n",
        "samples = np.arange(n_samples)\n",
        "\n",
        "# 프로파일링 시작\n",
        "profiler = cProfile.Profile()\n",
        "profiler.enable()\n",
        "\n",
        "\n",
        "criterion.init(y=y, sample_weight=None, n_samples=n_samples, samples=samples, start=0, end=n_samples)\n",
        "\n",
        "profiler.disable()\n",
        "\n",
        "# 프로파일링 결과 출력\n",
        "stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
        "stats.print_stats(10)  # 상위 10개 항목 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ONtZAy3Jx8a"
      },
      "outputs": [],
      "source": [
        "# 임의의 분할을 위해 새로운 위치와 임계값 설정\n",
        "# 프로파일링 시작\n",
        "profiler = cProfile.Profile()\n",
        "profiler.enable()\n",
        "\n",
        "new_pos = n_samples // 2\n",
        "split_feature = 1\n",
        "split_threshold = np.median(x[:, split_feature])\n",
        "\n",
        "# update 메서드를 사용하여 분할 업데이트\n",
        "criterion.update(new_pos, split_feature, split_threshold)\n",
        "\n",
        "profiler.disable()\n",
        "\n",
        "# 프로파일링 결과 출력\n",
        "stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
        "stats.print_stats(10)  # 상위 10개 항목 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am3w0x1MJx8a"
      },
      "outputs": [],
      "source": [
        "# update 메서드를 사용하여 분할 업데이트\n",
        "criterion.update(new_pos, split_feature, split_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeT2y81gJx8a"
      },
      "outputs": [],
      "source": [
        "criterion.proxy_impurity_improvement()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIdKIqGwJx8a"
      },
      "outputs": [],
      "source": [
        "criterion.node_value()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUasJ9h45a0N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "def check_random_state(seed):\n",
        "    \"\"\"\n",
        "    Ensures a consistent random state based on the given 'seed'\n",
        "    If the seed is 'None', an integer, or an instance of 'np.integer', a new random state is created.\n",
        "    If the seed is an instance of 'np.random.RandomState', it's returned as is.\n",
        "    Otherwise, a 'ValueError' is raised.\n",
        "    \"\"\"\n",
        "    if seed is None or isinstance(seed, (int, np.integer)):\n",
        "        return np.random.RandomState(seed)\n",
        "    elif isinstance(seed, np.random.RandomState):\n",
        "        return seed\n",
        "    else:\n",
        "        raise ValueError(\"seed must be None, int or np.random.RandomState\")\n",
        "\n",
        "class PseudoScoreTreeBuilder:\n",
        "    \"\"\"\n",
        "    Class designed to build a decision tree based on the pseudo-score test statistics criterion, typically used in recurrent events data analysis.\n",
        "    \"\"\"\n",
        "    TREE_UNDEFINED = -1  # Placeholder\n",
        "\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
        "                 max_features=None, max_thresholds=None, min_impurity_decrease=0,\n",
        "                 random_state=None):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        Initializes the hyperparameters and settings of the tree, such as 'max_depth','min_samples_split','max_features', and the others.\n",
        "        The 'random_state' is checked and stored.\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "    def split_indices(self, X_column, threshold):\n",
        "        \"\"\"Efficiently splits the data based on the given threshold for a specific feature column (X_column).\"\"\"\n",
        "        return np.where(X_column <= threshold)[0], np.where(X_column > threshold)[0]\n",
        "\n",
        "    def _split(self, X, criterion, start, end):\n",
        "        \"\"\"\n",
        "        Finds the best feature and threshold to split on for the data in the node defined by the range [start, end].\n",
        "        Iterates over features and possible thresholds to determine the best split based on the pseudo-score test statistics criterion.\n",
        "        Returns the feature index, threshold, and improvement of the best split.\n",
        "        \"\"\"\n",
        "        best_split = {\n",
        "            'feature_index': None,\n",
        "            'threshold': None,\n",
        "            'improvement': -np.inf\n",
        "        }\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        if self.max_features is None:\n",
        "            features_to_consider = np.arange(n_features)\n",
        "        else:\n",
        "            features_to_consider = self.random_state.choice(n_features, self.max_features, replace=False)\n",
        "\n",
        "        for feature_index in features_to_consider:\n",
        "            sorted_indices = np.argsort(X[start:end, feature_index])\n",
        "            X_sorted = X[start:end][sorted_indices]\n",
        "            unique_thresholds = np.unique(X_sorted[:, feature_index])\n",
        "\n",
        "            # Modify the handling for max_thresholds\n",
        "            if self.max_thresholds is not None:\n",
        "                n_thresholds = len(unique_thresholds)\n",
        "                if isinstance(self.max_thresholds, float):\n",
        "                    n_sample_thresholds = int(n_thresholds * self.max_thresholds)\n",
        "                else:\n",
        "                    n_sample_thresholds = self.max_thresholds\n",
        "\n",
        "                if n_thresholds > n_sample_thresholds:\n",
        "                    unique_thresholds = self.random_state.choice(unique_thresholds, n_sample_thresholds, replace=False)\n",
        "\n",
        "            for threshold in unique_thresholds:\n",
        "                new_pos = np.searchsorted(X_sorted[:, feature_index], threshold, side='right')\n",
        "                criterion.update(new_pos=new_pos, split_feature=feature_index, split_threshold=threshold)\n",
        "                improvement = criterion.proxy_impurity_improvement()\n",
        "\n",
        "                if improvement > best_split['improvement']:\n",
        "                    best_split = {\n",
        "                        'feature_index': feature_index,\n",
        "                        'threshold': threshold,\n",
        "                        'improvement': improvement\n",
        "                    }\n",
        "\n",
        "                if improvement < self.min_impurity_decrease:\n",
        "                    break\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def _build(self, X, y, criterion, depth=0, start=0, end=None):\n",
        "        \"\"\"\n",
        "        Recursively builds the decision tree.\n",
        "        If the current node meets the termination criteria (e.g., maximum depth, minimum samples in the node), it returns a terminal node.\n",
        "        Otherwise, it finds the best split for the current node, splits the data accordingly, and recursively constructs the left and right subtrees.\n",
        "        Returns a dictionary representing the node and its children.\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        if end is None:\n",
        "            end = n_samples\n",
        "\n",
        "        # Conditions for terminal node\n",
        "        node_value = criterion.node_value()\n",
        "        if depth == self.max_depth or (end - start) <= self.min_samples_leaf or (end - start) < self.min_samples_split:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value\n",
        "            }\n",
        "\n",
        "        # Initialize the criterion with the samples in the current node\n",
        "        criterion.init(y, None, n_samples, np.arange(start, end), start, end)\n",
        "\n",
        "        # Find the best split\n",
        "        best_split = self._split(X, criterion, start, end)\n",
        "        if best_split['improvement'] == -np.inf:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value\n",
        "            }\n",
        "\n",
        "        # Split the data based on the best split\n",
        "        left_indices = np.where(X[start:end, best_split['feature_index']] <= best_split['threshold'])[0]\n",
        "        right_indices = np.where(X[start:end, best_split['feature_index']] > best_split['threshold'])[0]\n",
        "\n",
        "        # Recursively build the left and right subtrees\n",
        "        left_child = self._build(X[left_indices], y[left_indices], criterion, depth=depth+1)\n",
        "        right_child = self._build(X[right_indices], y[right_indices], criterion, depth=depth+1)\n",
        "\n",
        "        return {\n",
        "            'feature': best_split['feature_index'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left_child': left_child,\n",
        "            'right_child': right_child,\n",
        "            'node_value': node_value\n",
        "        }\n",
        "\n",
        "    def build(self, X, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        The main method to invoke the tree building process.\n",
        "        Initializes the pseudo-likelihood criterion using the input data and constructs the tree using the _build method.\n",
        "        Finally, converts the resulting tree dictionary into a pandas DataFrame and returns it.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        y = np.c_[time_start, time_stop, event]\n",
        "\n",
        "        unique_times = np.unique(np.concatenate([time_start, time_stop]))\n",
        "        criterion = PseudoScoreCriterion(n_outputs=n_features, n_samples=n_samples,\n",
        "                                         unique_times=unique_times, x=X, ids=ids,\n",
        "                                         time_start=time_start, time_stop=time_stop, event=event)\n",
        "\n",
        "        # Adjusting the samples_time_idx value based on PseudoScoreCriterion logic.\n",
        "        for i in range(n_samples - 1):  # Adjusted the range to prevent IndexError\n",
        "            criterion.samples_time_idx[i] = np.searchsorted(unique_times, time_stop[i])\n",
        "\n",
        "        # Build the tree\n",
        "        tree = self._build(X, y, criterion)\n",
        "\n",
        "        # Convert tree dictionary to dataframe for consistency\n",
        "        tree_df = pd.DataFrame([tree])\n",
        "        return tree_df\n",
        "# Since the PseudoScoreTreeBuilder is not directly testable (it's dependent on the state of the object),\n",
        "# we will assume the refactoring is correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEtFt_345a0O"
      },
      "outputs": [],
      "source": [
        "tree_builder=PseudoScoreTreeBuilder(max_depth=3, min_samples_leaf=5, max_thresholds=0.5, min_impurity_decrease=0.5, random_state=1190)\n",
        "tree_df = tree_builder.build(x, ids, time_start, time_stop, event)\n",
        "\n",
        "# Display the tree dataframe\n",
        "tree_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLDxcKUx5a0O"
      },
      "outputs": [],
      "source": [
        "class RecurrentTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
        "                 max_features=None, max_thresholds=None, min_impurity_decrease=0,\n",
        "                 random_state=None):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        Initializes the tree's hyperparameters and settings\n",
        "        Parameters like 'max_deth','min_samples_split','and 'max_features' define the tree's growth conditions.\n",
        "        'random_state' ensures reproducibility\n",
        "        'tree_' will later store the built tree\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.max_features = max_features\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.random_state = random_state\n",
        "        self.tree_ = None\n",
        "\n",
        "    def fit(self, X, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        Trains the recurrent tree using the input data\n",
        "        'X' represents the feature matrix, while 'ids','time_start','time_stop',and 'event' are specific to recurrent event data\n",
        "        Uses 'PseudoScoreTreeBuilder' to construct the tree based on the input data\n",
        "        The resulting tree is stored in the 'tree_' attribute.\n",
        "        \"\"\"\n",
        "        self.riskset_counter = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        # Ensure input is in the expected format\n",
        "        X = np.array(X)\n",
        "        ids = np.array(ids)\n",
        "        time_start = np.array(time_start)\n",
        "        time_stop = np.array(time_stop)\n",
        "        event = np.array(event)\n",
        "\n",
        "        # Use the PseudoScoreTreeBuilder to build the tree\n",
        "        builder = PseudoScoreTreeBuilder(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            max_features=self.max_features,\n",
        "            max_thresholds=self.max_thresholds,\n",
        "            min_impurity_decrease=self.min_impurity_decrease,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_ = builder.build(X, ids=ids, time_start=time_start, time_stop=time_stop, event=event).iloc[0]\n",
        "        return self\n",
        "\n",
        "    def get_tree(self):\n",
        "        \"\"\"Return the tree as a dictionary.\"\"\"\n",
        "        return self.tree_\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"Traverse the tree to find the terminal node for a given sample.\"\"\"\n",
        "\n",
        "        # Check if it's a terminal node\n",
        "        if node[\"threshold\"] is None:\n",
        "            return node\n",
        "\n",
        "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "            return self._traverse_tree(x, node[\"left_child\"])  # Navigate to the left child\n",
        "        else:\n",
        "            return self._traverse_tree(x, node[\"right_child\"])  # Navigate to the right child\n",
        "\n",
        "    def predict_rate_function(self, X):\n",
        "        \"\"\"\n",
        "        Predict the nonparametric estimates of dμ(t) = ρ(t)dt for given samples.\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        rate_functions = []\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            # Traverse the tree to find the terminal node for the current sample\n",
        "            terminal_node = self._traverse_tree(X[i], self.tree_)\n",
        "\n",
        "            # Extract the cumulative hazard (mean function) for the terminal node\n",
        "            mean_function = terminal_node.get('node_value', np.array([]))\n",
        "            if not mean_function.size:\n",
        "                warnings.warn(f\"No rate function found for sample {i}. Using a zero array as a placeholder.\")\n",
        "                rate_function = np.zeros_like(X[i])\n",
        "            else:\n",
        "                # Compute the rate function as the difference in consecutive values of the cumulative hazard\n",
        "                rate_function = np.diff(mean_function, prepend=0)\n",
        "\n",
        "            rate_functions.append(rate_function)\n",
        "\n",
        "        return rate_functions\n",
        "\n",
        "    def predict_mean_function(self, X):\n",
        "        \"\"\"\n",
        "        Predict the Nelson-Aalen estimator of the mean function for given samples.\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        n_samples = X.shape[0]\n",
        "        mean_functions = []\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            # Traverse the tree to find the terminal node for the current sample\n",
        "            terminal_node = self._traverse_tree(X[i], self.tree_)\n",
        "\n",
        "            # Extract the cumulative hazard (mean function) for the terminal node\n",
        "            mean_function = terminal_node.get('node_value', np.array([]))\n",
        "            if not mean_function.size:\n",
        "                warnings.warn(f\"No mean function found for sample {i}. Using a zero array as a placeholder.\")\n",
        "                mean_function = np.zeros_like(X[i])\n",
        "\n",
        "            mean_functions.append(mean_function)\n",
        "\n",
        "        return mean_functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL0G16an23L4"
      },
      "outputs": [],
      "source": [
        "# 2. RecurrentTree 학습 및 예측\n",
        "tree_model = RecurrentTree(max_depth=3, min_samples_leaf=5, max_thresholds=0.5, min_impurity_decrease=0.5, random_state=1190)\n",
        "tree_model.fit(x, ids, time_start, time_stop, event)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvAqImfSJx8b"
      },
      "outputs": [],
      "source": [
        "# 비율 함수 예측\n",
        "rate_functions = tree_model.predict_rate_function(x)\n",
        "\n",
        "# 평균 함수 예측\n",
        "mean_functions = tree_model.predict_mean_function(x)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"Sample Rate Functions:\", rate_functions[:5])\n",
        "print(\"Sample Mean Functions:\", mean_functions[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzKLsKYn-uPj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 첫 번째 샘플에 대한 rate function과 mean function 시각화\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(rate_functions[0], label=\"Predicted Rate Function\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Rate\")\n",
        "plt.title(\"Rate Function\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(mean_functions[0], label=\"Predicted Mean Function\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Mean\")\n",
        "plt.title(\"Mean Function\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIJxLbdSRzOJ"
      },
      "outputs": [],
      "source": [
        "%pip install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6C0ZZw7u4VM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numbers import Integral, Real\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "def _get_n_samples_bootstrap(n_ids, max_samples):\n",
        "    \"\"\"\n",
        "    Modified for recurrent events. Get the number of IDs in a bootstrap sample.\n",
        "    \"\"\"\n",
        "    if max_samples is None:\n",
        "        return n_ids\n",
        "\n",
        "    if isinstance(max_samples, Integral):\n",
        "        if max_samples > n_ids:\n",
        "            msg = \"`max_samples` must be <= n_ids={} but got value {}\"\n",
        "            raise ValueError(msg.format(n_ids, max_samples))\n",
        "        return max_samples\n",
        "\n",
        "    if isinstance(max_samples, Real):\n",
        "        return max(round(n_ids * max_samples), 1)\n",
        "\n",
        "def _generate_sample_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Sample unique IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    random_instance = check_random_state(random_state)\n",
        "    sampled_ids = np.random.choice(ids, n_ids_bootstrap, replace=True)\n",
        "    return sampled_ids\n",
        "\n",
        "def _generate_unsampled_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Determine unsampled IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    unique_ids = np.unique(ids)\n",
        "    sampled_ids = _generate_sample_indices(random_state, unique_ids, n_ids_bootstrap)\n",
        "    unique_sampled_ids = np.unique(sampled_ids)  # Ensure sampled IDs are unique\n",
        "    unsampled_ids = np.setdiff1d(unique_ids, unique_sampled_ids)\n",
        "\n",
        "    # Expand these unsampled IDs to include all their associated events.\n",
        "    # Again, this will depend on your data structure.\n",
        "    # As an example:\n",
        "    # unsampled_indices = np.concatenate([events_by_id[id] for id in unsampled_ids])\n",
        "\n",
        "    return unsampled_ids  # or return unsampled_indices based on your data structure\n",
        "\n",
        "\n",
        "from warnings import catch_warnings, simplefilter\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "def _parallel_build_trees(\n",
        "    tree,\n",
        "    bootstrap,\n",
        "    X,\n",
        "    y,\n",
        "    ids,  # New parameter: a list/array of IDs corresponding to each event in X and y\n",
        "    sample_weight,\n",
        "    tree_idx,\n",
        "    n_trees,\n",
        "    verbose=0,\n",
        "    class_weight=None,\n",
        "    n_ids_bootstrap=None,  # Instead of n_samples_bootstrap\n",
        "):\n",
        "    \"\"\"\n",
        "    Private function used to fit a single tree in parallel for recurrent events.\"\"\"\n",
        "\n",
        "    # Clear the cache of RisksetCounter before fitting the tree\n",
        "    if hasattr(tree, 'risk_set_counter'):\n",
        "        tree.risk_set_counter.Y_i.cache_clear()\n",
        "        tree.risk_set_counter.dN_bar_i.cache_clear()\n",
        "\n",
        "    if verbose > 1:\n",
        "        print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n",
        "\n",
        "    if bootstrap:\n",
        "        unique_ids = np.unique(ids)\n",
        "        n_ids = len(unique_ids)\n",
        "\n",
        "        # Generate bootstrap samples using IDs\n",
        "        sampled_ids = _generate_sample_indices(\n",
        "            tree.random_state, unique_ids, n_ids_bootstrap\n",
        "        )\n",
        "\n",
        "        # Expand sampled IDs to all their associated events\n",
        "        indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "\n",
        "        if sample_weight is None:\n",
        "            curr_sample_weight = np.ones((X.shape[0],), dtype=np.float64)\n",
        "        else:\n",
        "            curr_sample_weight = sample_weight.copy()\n",
        "\n",
        "        # Adjust the sample weight based on how many times each ID was sampled\n",
        "        sample_counts_for_ids = np.bincount(np.searchsorted(unique_ids, sampled_ids), minlength=n_ids)\n",
        "        curr_sample_weight *= sample_counts_for_ids[np.searchsorted(unique_ids, ids)]\n",
        "\n",
        "        if class_weight == \"subsample\":\n",
        "            with catch_warnings():\n",
        "                simplefilter(\"ignore\", DeprecationWarning)\n",
        "                curr_sample_weight *= compute_sample_weight(\"auto\", y, indices=indices)\n",
        "        elif class_weight == \"balanced_subsample\":\n",
        "            curr_sample_weight *= compute_sample_weight(\"balanced\", y, indices=indices)\n",
        "\n",
        "        tree.fit(X[indices], y[indices], sample_weight=curr_sample_weight[indices], check_input=False)\n",
        "    else:\n",
        "        tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
        "\n",
        "    # Clear the cache of RisksetCounter after fitting the tree\n",
        "    if hasattr(tree, 'risk_set_counter'):\n",
        "        tree.risk_set_counter.Y_i.cache_clear()\n",
        "        tree.risk_set_counter.dN_bar_i.cache_clear()\n",
        "\n",
        "    return tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nfxib_49tgG"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.utils import check_array\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "class RecurrentRandomForest(BaseEstimator):\n",
        "    \"\"\"\n",
        "    Constructor of the class\n",
        "    Initializes the forest's hyperparameters and settings.\n",
        "    Parameters like 'n_estimators', 'max_depth', and 'max_features' define the forest's construction conditions\n",
        "    The 'estimators_' attribute is initialized with a list of individual trees ('RecurrentTree' instance) based on the number of estimators specified.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2,\n",
        "                 min_samples_leaf=1, bootstrap=True, oob_score=False, n_jobs=None,\n",
        "                 random_state=None, verbose=0, warm_start=False, max_samples=None,\n",
        "                 min_impurity_decrease=0.0, max_features=None):  # Add new parameters\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.bootstrap = bootstrap\n",
        "        self.oob_score = oob_score\n",
        "        self.n_jobs = n_jobs\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.warm_start = warm_start\n",
        "        self.max_samples = max_samples\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.max_features = max_features\n",
        "        self.estimators_ = [self._make_estimator(random_state=i) for i in range(self.n_estimators)]\n",
        "\n",
        "    def _make_estimator(self, random_state=None):\n",
        "        \"\"\"\n",
        "        Constructs a new instances of the 'RecurrentTree' with the specified hyperparameters\n",
        "        Allows for creating each tree with a different 'random_state' for randomness\n",
        "        \"\"\"\n",
        "        return RecurrentTree(\n",
        "            max_depth=self.max_depth,\n",
        "            min_samples_split=self.min_samples_split,\n",
        "            min_samples_leaf=self.min_samples_leaf,\n",
        "            random_state=random_state,\n",
        "            min_impurity_decrease=self.min_impurity_decrease,  # Pass the new parameter\n",
        "            max_features=self.max_features  # Pass the new parameter\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Trains the random forest using the input data\n",
        "        'X' represents the feature matrix, while 'y' is expected to be a constructed array or a dictionary with keys: 'id' ,'time_start', 'time_stop', and 'event'.\n",
        "        It builds each tree in the forest, either on the full dataset or a bootstrap sample depending on the 'bootstrap' attribute.\n",
        "        If 'oob_score' is set to 'True', it calculates the out-of-bag (OOB) score once all trees are trained.\n",
        "        \"\"\"\n",
        "        X = self._validate_data(X, accept_sparse='csc', ensure_min_samples=2)\n",
        "        ids = y['id']\n",
        "        time_start = y['time_start']\n",
        "        time_stop = y['time_stop']\n",
        "        event = y['event']\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "\n",
        "        n_samples_bootstrap = _get_n_samples_bootstrap(len(np.unique(ids)), self.max_samples)\n",
        "\n",
        "        def _fit_tree(tree):\n",
        "            if self.bootstrap:\n",
        "                unique_ids = np.unique(ids)\n",
        "                sampled_ids = _generate_sample_indices(tree.random_state, unique_ids, n_samples_bootstrap)\n",
        "                bootstrap_indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "                X_bootstrap = X[bootstrap_indices]\n",
        "                ids_bootstrap = np.array(ids)[bootstrap_indices]\n",
        "                time_start_bootstrap = np.array(time_start)[bootstrap_indices]\n",
        "                time_stop_bootstrap = np.array(time_stop)[bootstrap_indices]\n",
        "                event_bootstrap = np.array(event)[bootstrap_indices]\n",
        "            else:\n",
        "                X_bootstrap = X\n",
        "                ids_bootstrap = ids\n",
        "                time_start_bootstrap = time_start\n",
        "                time_stop_bootstrap = time_stop\n",
        "                event_bootstrap = event\n",
        "            tree.fit(X_bootstrap, ids_bootstrap, time_start_bootstrap, time_stop_bootstrap, event_bootstrap)\n",
        "            return tree\n",
        "\n",
        "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(_fit_tree)(tree) for tree in self.estimators_\n",
        "        )\n",
        "\n",
        "        for estimator in self.estimators_:\n",
        "            if hasattr(estimator, 'riskset_counter') and estimator.riskset_counter is not None:\n",
        "                estimator.riskset_counter.reset()\n",
        "\n",
        "        if self.oob_score:\n",
        "            self._set_oob_score_and_attributes(X, y)\n",
        "        return self\n",
        "\n",
        "    def _set_oob_score_and_attributes(self, X, y):\n",
        "        \"\"\"\n",
        "        Calculates the out-of-bag (OOB) scores using the ensemble's predictions for the training data samples that were not seen during the training of a given tree\n",
        "        Also sets the 'oob_prediction_' and 'oob_score_' attributes of the class\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Assuming y is a structured array with these keys.\n",
        "        ids = y['id']\n",
        "        time_start = y['time_start']\n",
        "        time_stop = y['time_stop']\n",
        "        event = y['event']\n",
        "\n",
        "        # Calculate total number of events for each ID\n",
        "        total_events = {}\n",
        "        for i in range(n_samples):\n",
        "            total_events[ids[i]] = total_events.get(ids[i], 0) + event[i]\n",
        "\n",
        "        predictions = np.zeros(n_samples)\n",
        "        n_predictions = np.zeros(n_samples)\n",
        "\n",
        "        n_samples_bootstrap = _get_n_samples_bootstrap(len(np.unique(ids)), self.max_samples)\n",
        "\n",
        "        for estimator in self.estimators_:\n",
        "            unsampled_indices = _generate_unsampled_indices(estimator.random_state, np.unique(ids), n_samples_bootstrap)\n",
        "\n",
        "            p_estimator_result = estimator.predict_mean_function(X[unsampled_indices, :])\n",
        "\n",
        "            # Debug print\n",
        "            print(f\"Type of p_estimator_result: {type(p_estimator_result)}, Length: {len(p_estimator_result)}, Type of first element: {type(p_estimator_result[0]) if p_estimator_result else None}\")\n",
        "\n",
        "            # Check if p_estimator_result is not empty\n",
        "            if p_estimator_result:\n",
        "                p_estimator = np.array(p_estimator_result).mean(axis=1)\n",
        "                predictions[unsampled_indices] += p_estimator\n",
        "                n_predictions[unsampled_indices] += 1\n",
        "\n",
        "        if (n_predictions == 0).any():\n",
        "            warnings.warn(\n",
        "              \"Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\",\n",
        "            stacklevel=3,\n",
        "            )\n",
        "            n_predictions[n_predictions == 0] = 1\n",
        "\n",
        "        predictions /= n_predictions\n",
        "\n",
        "        self.oob_prediction_ = predictions\n",
        "\n",
        "        # Pass the calculated total_events to the method\n",
        "        self.oob_score_ = self._estimate_recurrent_concordance_index(predictions, X, event, ids, total_events)\n",
        "\n",
        "\n",
        "\n",
        "    def _estimate_recurrent_concordance_index(self, predictions, X, event, ids, total_events):\n",
        "        \"\"\"\n",
        "        Estimate the C-index for recurrent events using OOB ensemble estimates for right-censored data.\n",
        "\n",
        "        Parameters:\n",
        "        - predictions: Predicted mean functions for all samples using OOB.\n",
        "        - X: The data matrix.\n",
        "        - event: Observed recurrent events for all samples.\n",
        "        - ids: IDs for each event.\n",
        "        - total_events: Total number of events for each ID.\n",
        "\n",
        "        Returns:\n",
        "        - C-index estimate.\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        concordant_pairs = 0\n",
        "        permissible_pairs = 0\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            for j in range(i+1, n_samples):\n",
        "                if ids[i] != ids[j]:  # Only consider pairs with different IDs\n",
        "                    # Check if one of the pair is right-censored\n",
        "                    right_censored_i = event[i] < total_events[ids[i]]\n",
        "                    right_censored_j = event[j] < total_events[ids[j]]\n",
        "\n",
        "                    if not right_censored_i and not right_censored_j:  # Both are not right-censored\n",
        "                        if event[i] > event[j]:\n",
        "                            permissible_pairs += 1\n",
        "                            if predictions[i] > predictions[j] and event[i] > event[j]:\n",
        "                                concordant_pairs += 1\n",
        "                    else:  # At least one is right-censored\n",
        "                        if not right_censored_i:  # i is not right-censored but j is\n",
        "                            permissible_pairs += 1\n",
        "                            if predictions[i] > predictions[j]:\n",
        "                                concordant_pairs += 1\n",
        "                        elif not right_censored_j:  # j is not right-censored but i is\n",
        "                            permissible_pairs += 1\n",
        "                            if predictions[i] < predictions[j]:\n",
        "                                concordant_pairs += 1\n",
        "\n",
        "        c_index = concordant_pairs / permissible_pairs if permissible_pairs > 0 else 0\n",
        "        return 2 * c_index\n",
        "\n",
        "        \"\"\"Validate input data('X') to ensure it's in the correct format and meets the necessary conditions for processing.\"\"\"\n",
        "    def _validate_data(self, X, accept_sparse=False, ensure_min_samples=1):\n",
        "        \"\"\"Validate input data('X') to ensure it's in the correct format and meets the necessary conditions for processing.\"\"\"\n",
        "        return check_array(X, accept_sparse=accept_sparse, ensure_min_samples=ensure_min_samples)\n",
        "\n",
        "    def _validate_X_predict(self, X):\n",
        "        \"\"\"Validate X whenever one tries to predict.\"\"\"\n",
        "        X = check_array(X)\n",
        "        if X.shape[1] != self.n_features_in_:\n",
        "            raise ValueError(\"Number of features of the model must match the input. Model n_features is {} and input n_features is {}.\"\n",
        "                             .format(self.n_features_in_, X.shape[1]))\n",
        "        return X\n",
        "\n",
        "    def _predict_function(self, X, prediction_type):\n",
        "        \"\"\"\n",
        "        A general method that handles predictions. The type of prediction (either rate function or mean function) is determined by the 'prediction_type' argument\n",
        "        For each tree in the forest, it gets the desired prediction and then averages them.\n",
        "        \"\"\"\n",
        "        check_is_fitted(self, \"estimators_\")\n",
        "        X = self._validate_X_predict(X)\n",
        "\n",
        "        def _tree_predict(tree):\n",
        "            if prediction_type == \"rate\":\n",
        "                return tree.predict_rate_function(X)\n",
        "            elif prediction_type == \"mean\":\n",
        "                return tree.predict_mean_function(X)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid prediction_type provided. Valid options are 'rate' or 'mean'.\")\n",
        "\n",
        "        all_results = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(_tree_predict)(tree) for tree in self.estimators_\n",
        "        )\n",
        "\n",
        "        max_shape = max((np.array(result).shape for result in all_results), key=lambda x: np.prod(x))\n",
        "        accumulator = np.zeros(max_shape)\n",
        "\n",
        "        for result in all_results:\n",
        "            result_array = np.array(result)\n",
        "            if result_array.shape != max_shape:\n",
        "                padded_result = np.zeros(max_shape)\n",
        "                slices = tuple(slice(0, dim) for dim in result_array.shape)\n",
        "                padded_result[slices] = result_array\n",
        "            else:\n",
        "                padded_result = result_array\n",
        "            accumulator += padded_result\n",
        "        averaged_results = accumulator / len(self.estimators_)\n",
        "\n",
        "        return averaged_results\n",
        "\n",
        "    def predict_rate_function(self, X):\n",
        "        return self._predict_function(X, \"rate\")\n",
        "\n",
        "    def predict_mean_function(self, X):\n",
        "        return self._predict_function(X, \"mean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly9w9VroGTOB"
      },
      "outputs": [],
      "source": [
        "rrf = RecurrentRandomForest(n_estimators=10, max_depth=3, min_samples_leaf=5, min_impurity_decrease=0.2, random_state=42, oob_score=True, n_jobs=6)\n",
        "y = {\n",
        "    'id': ids,\n",
        "    'time_start': time_start,\n",
        "    'time_stop': time_stop,\n",
        "    'event': event\n",
        "}\n",
        "rrf.fit(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufi_AWHUG4ap"
      },
      "outputs": [],
      "source": [
        "# Predict the rate function and mean function for the samples\n",
        "rate_function_predictions = rrf.predict_rate_function(x)\n",
        "mean_function_predictions = rrf.predict_mean_function(x)\n",
        "\n",
        "rate_function_predictions, mean_function_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCOCSJ9PbJpf"
      },
      "outputs": [],
      "source": [
        "rrf.oob_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMSFu9jJedyB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "보류\n",
        "\"\"\"\n",
        "\n",
        "class PermutationImportance:\n",
        "    def __init__(self, model, n_repeats=30, random_state=None):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        'model': The trained model for which we want to compute feature importances\n",
        "        'n_repeats': Number of times to repeat the permutation for each feature to get a reliable estimate.\n",
        "        'random_state': Seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.n_repeats = n_repeats\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _compute_baseline_cindex(self, X, event, time_stop, ids):\n",
        "        \"\"\"\n",
        "        Computes the baseline C-index using the original (non-permuted) data.\n",
        "        The C-index is a metric to evaluate the model's predictions, especially for recurrent events.\n",
        "        This function first predicts the mean function using the model, averages the predictions, and then calculates the C-index.\n",
        "        \"\"\"\n",
        "        # Get predictions using the model\n",
        "        predictions = self.model.predict_mean_function(X)\n",
        "\n",
        "        # Take the mean of the predictions for each individual for the C-index computation\n",
        "        mean_predictions = np.mean(predictions, axis=1)\n",
        "\n",
        "        # Compute total number of events for each ID\n",
        "        unique_ids, total_events = np.unique(ids, return_counts=True)\n",
        "        total_events_dict = dict(zip(unique_ids, total_events))\n",
        "        total_events_arr = np.array([total_events_dict[id_] for id_ in ids])\n",
        "\n",
        "        return self._estimate_concordance_index_recurrent(time_stop, event, mean_predictions, ids, total_events_arr)\n",
        "\n",
        "    def _estimate_concordance_index_recurrent(self, time_stop, event, predictions, ids, total_events):\n",
        "        \"\"\"\n",
        "        A wrapper around the model's method to estimate the recurrent C-index.\n",
        "        It's used for convenience and to make the code more readable.\n",
        "        \"\"\"\n",
        "        return self.model._estimate_recurrent_concordance_index(predictions, time_stop, event, ids, total_events)\n",
        "\n",
        "    def compute_importance(self, X, event, time_stop, ids):\n",
        "        \"\"\"\n",
        "        The main function that computes the feature importances\n",
        "        For each feature:\n",
        "            It permutes (shuffles) the feature's values a number of times (specified by 'n_repeats')\n",
        "            For each permutation, it calculates the drop in C-index (compared to the baseline C-index) due to the permutation\n",
        "            The drop in performance (C-index) due to the permutation gives an indicartion of the feature's importance.\n",
        "        It returns the computed importances matrix where each row corresponds to a feature and each column to a permutation repitition\n",
        "        \"\"\"\n",
        "        baseline_cindex = self._compute_baseline_cindex(X, event, time_stop, ids)\n",
        "\n",
        "        rng = check_random_state(self.random_state)\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        importances = np.zeros((n_features, self.n_repeats))\n",
        "\n",
        "        unique_ids = np.unique(ids)\n",
        "\n",
        "        for feature in range(n_features):\n",
        "            for repeat in range(self.n_repeats):\n",
        "                X_permuted = X.copy()\n",
        "\n",
        "                permuted_ids = rng.permutation(unique_ids)  # ID를 섞습니다.\n",
        "\n",
        "                # ID에 따라 값을 변경합니다.\n",
        "                for orig_id, new_id in zip(unique_ids, permuted_ids):\n",
        "                    orig_idx = np.where(ids == orig_id)[0]\n",
        "                    new_idx = np.where(ids == new_id)[0]\n",
        "                    X_permuted[orig_idx, feature] = X[new_idx, feature]\n",
        "\n",
        "                # Calculate c-index for permuted X\n",
        "                permuted_cindex = self._compute_baseline_cindex(X_permuted, event, time_stop, ids)\n",
        "\n",
        "                # The importance is the drop in c-index\n",
        "                importances[feature, repeat] = baseline_cindex - permuted_cindex\n",
        "\n",
        "        return importances\n",
        "\n",
        "    def report_importance(self, X, event, time_stop, ids):\n",
        "        \"\"\"\n",
        "        Computes the feature importances and then calculates the mean and standard deviation of the importances for each feature across all the repeats.\n",
        "        The mean gives an average measure of the importance of each feature, while the standard deviation provides an estimate of the variability or uncertainty in the importance estimates.\n",
        "        It returns the mean and standard deviation of the feature importances.\n",
        "        \"\"\"\n",
        "        importances = self.compute_importance(X, event, time_stop, ids)\n",
        "\n",
        "        # Compute mean and std of importances\n",
        "        importance_mean = np.mean(importances, axis=1)\n",
        "        importance_std = np.std(importances, axis=1)\n",
        "\n",
        "        return importance_mean, importance_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9JHyEydNere"
      },
      "outputs": [],
      "source": [
        "permutation_importance = PermutationImportance(rrf, n_repeats=10, random_state=42)\n",
        "mean_importances, std_importances = permutation_importance.report_importance(x, event, time_stop, ids)\n",
        "\n",
        "print(\"Mean Importances:\", mean_importances)\n",
        "print(\"STD Importances:\", std_importances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsxwplcEJx8d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}