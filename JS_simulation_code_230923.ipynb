{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSK2022/RandomForest-for-Recurrent-Events/blob/Thesis-Code/JS_simulation_code_230923.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GThv_VDEEWx0"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-survival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVx50x1_Kw-C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "path=\"/Users/jeongsookim/Downloads\"\n",
        "data = pd.read_csv(f\"{path}/simuDat.csv\")\n",
        "\n",
        "ids = data['id'].values\n",
        "time_start = data['start'].values\n",
        "time_stop = data['stop'].values\n",
        "event = data['event'].values\n",
        "x = data[['group','x1','gender']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXJ9Vf-6Kw-C"
      },
      "source": [
        "### RisksetCounter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2GflIJTKw-D"
      },
      "outputs": [],
      "source": [
        "class RisksetCounter:\n",
        "    def __init__(self, ids, time_start, time_stop, event):\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "\n",
        "        self.all_unique_times = np.unique(time_stop)\n",
        "        self.n_unique_times = len(self.all_unique_times)\n",
        "\n",
        "        self.n_at_risk = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.n_events = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.set_data()\n",
        "\n",
        "        self.state_stack = []\n",
        "\n",
        "    def set_data(self):\n",
        "        unique_ids = set(self.ids)\n",
        "        for t_idx, t in enumerate(self.all_unique_times):\n",
        "            self.n_at_risk[t_idx] = sum([self.Y_i(id_, t_idx) for id_ in unique_ids])\n",
        "            self.n_events[t_idx] = sum([self.dN_bar_i(id_, t_idx) for id_ in unique_ids])\n",
        "\n",
        "    def Y_i(self, id_, t_idx):\n",
        "        if t_idx >= len(self.all_unique_times):\n",
        "            return 0\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        indices = (self.ids == id_) & (time_at_t_idx <= self.time_stop)\n",
        "        return np.any(indices)\n",
        "\n",
        "    def dN_bar_i(self, id_, t_idx):\n",
        "        if t_idx >= len(self.all_unique_times):\n",
        "            return 0\n",
        "        time_at_t_idx = self.all_unique_times[t_idx]\n",
        "        indices = (self.ids == id_) & (time_at_t_idx == self.time_stop) & (self.event == 1)\n",
        "        return np.any(indices)\n",
        "\n",
        "    def save_state(self):\n",
        "        self.state_stack.append((self.ids.copy(), self.time_start.copy(), self.time_stop.copy(), self.event.copy(), self.n_at_risk.copy(), self.n_events.copy()))\n",
        "\n",
        "    def load_state(self):\n",
        "        if self.state_stack:\n",
        "            self.ids, self.time_start, self.time_stop, self.event, self.n_at_risk, self.n_events = self.state_stack.pop()\n",
        "\n",
        "    def update(self, new_ids, new_time_start, new_time_stop, new_event):\n",
        "        # Save the current state\n",
        "        self.save_state()\n",
        "\n",
        "        # Compute the intersection of data\n",
        "        mask = np.isin(self.ids, new_ids)\n",
        "\n",
        "        # Extract data of the intersection\n",
        "        updated_ids = self.ids[mask]\n",
        "        updated_time_start = self.time_start[mask]\n",
        "        updated_time_stop = self.time_stop[mask]\n",
        "        updated_event = self.event[mask]\n",
        "\n",
        "        # Update object variables based on the intersection data\n",
        "        self.ids = updated_ids\n",
        "        self.time_start = updated_time_start\n",
        "        self.time_stop = updated_time_stop\n",
        "        self.event = updated_event\n",
        "\n",
        "        # Recalculate unique times based on the updated data\n",
        "        self.all_unique_times = np.unique(np.concatenate([self.time_start, self.time_stop]))\n",
        "        self.n_unique_times = len(self.all_unique_times)\n",
        "\n",
        "        # Resize the n_at_risk and n_events arrays based on the updated unique times\n",
        "        self.n_at_risk = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "        self.n_events = np.zeros(self.n_unique_times, dtype=np.int64)\n",
        "\n",
        "        # Update the n_at_risk and n_events arrays\n",
        "        unique_ids = set(self.ids)  # Extract unique IDs to avoid redundant calculations\n",
        "        for t_idx, t in enumerate(self.all_unique_times):\n",
        "            self.n_at_risk[t_idx] = sum([self.Y_i(id_, t_idx) for id_ in unique_ids])\n",
        "            self.n_events[t_idx] = sum([self.dN_bar_i(id_, t_idx) for id_ in unique_ids])\n",
        "\n",
        "    def reset(self):\n",
        "        self.load_state()\n",
        "\n",
        "    def copy(self):\n",
        "        return RisksetCounter(self.ids.copy(), self.time_start.copy(), self.time_stop.copy(), self.event.copy())\n",
        "\n",
        "    def __reduce__(self):\n",
        "        return (self.__class__, (self.ids, self.time_start, self.time_stop, self.event))\n",
        "\n",
        "# Let's check if the updated RisksetCounter works\n",
        "riskset_test = RisksetCounter(ids, time_start, time_stop, event)\n",
        "riskset_test.n_at_risk, riskset_test.n_events, len(riskset_test.n_at_risk), len(riskset_test.n_events)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIP3_oNoKw-E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "path=\"/Users/jeongsookim/Downloads\"\n",
        "simuDat= pd.read_csv(f\"{path}/simuDat.csv\")\n",
        "simuDat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L35Q1-XVKw-E"
      },
      "outputs": [],
      "source": [
        "riskset_counter = RisksetCounter(ids=simuDat[\"id\"].values,\n",
        "                                       time_start=simuDat[\"start\"].values,\n",
        "                                       time_stop=simuDat[\"stop\"].values,\n",
        "                                       event=simuDat[\"event\"].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlGUN3CuKw-E"
      },
      "source": [
        "#### Group==Contr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMSpQRm7Kw-E"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat[\"group\"] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMktKtDBKw-E"
      },
      "outputs": [],
      "source": [
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xGTkc76Kw-E"
      },
      "outputs": [],
      "source": [
        "riskset_counter.n_at_risk, len(riskset_counter.n_at_risk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W37rev68Kw-E"
      },
      "outputs": [],
      "source": [
        "riskset_counter.n_events, len(riskset_counter.n_events)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHe580cMKw-F"
      },
      "outputs": [],
      "source": [
        "np.unique(riskset_counter.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeIxgsruKw-F"
      },
      "outputs": [],
      "source": [
        "riskset_counter.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOD0NwEKKw-F"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat[\"group\"] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG1F1KR6Kw-F"
      },
      "outputs": [],
      "source": [
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zuesy8nAKw-F"
      },
      "outputs": [],
      "source": [
        "riskset_counter.n_at_risk, len(riskset_counter.n_at_risk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMqW1cpSKw-F"
      },
      "outputs": [],
      "source": [
        "riskset_counter.n_events, len(riskset_counter.n_events)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWPnM_aFKw-F"
      },
      "outputs": [],
      "source": [
        "np.unique(riskset_counter.ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VEI-hOcKw-F"
      },
      "source": [
        "#### Male"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RsXBVfKKw-F"
      },
      "outputs": [],
      "source": [
        "riskset_counter.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tlrl-HbEKw-F"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat[\"gender\"] == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYhwagmZKw-F"
      },
      "outputs": [],
      "source": [
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2WByKwNKw-F"
      },
      "outputs": [],
      "source": [
        "riskset_counter.n_at_risk, len(riskset_counter.n_at_risk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4homuJ4Kw-F"
      },
      "outputs": [],
      "source": [
        "riskset_counter.n_events, len(riskset_counter.n_events)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFZFDS5TKw-F"
      },
      "outputs": [],
      "source": [
        "np.unique(riskset_counter.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LA3IBOmKw-G"
      },
      "outputs": [],
      "source": [
        "np.unique(simuDat_male['id'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KWqspE0Kw-G"
      },
      "source": [
        "#### Female"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrtFstPFKw-G"
      },
      "outputs": [],
      "source": [
        "riskset_counter.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn63LRVoKw-G"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat[\"gender\"] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rojKjjl5Kw-G"
      },
      "outputs": [],
      "source": [
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYCfBBZWKw-G"
      },
      "outputs": [],
      "source": [
        "riskset_counter.n_events, len(riskset_counter.n_events)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUTC1e48Kw-G"
      },
      "outputs": [],
      "source": [
        "riskset_counter.n_at_risk, len(riskset_counter.n_at_risk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcHKHpbwKw-G"
      },
      "outputs": [],
      "source": [
        "riskset_counter.n_events, len(riskset_counter.n_events)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74xpaxGvKw-G"
      },
      "outputs": [],
      "source": [
        "np.unique(riskset_counter.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCLvqy3iKw-G"
      },
      "outputs": [],
      "source": [
        "np.unique(simuDat_male['id'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVVIySyXKw-G"
      },
      "source": [
        "#### Contr&male"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZCEwA-YKw-G"
      },
      "outputs": [],
      "source": [
        "simuDat_sub1 = simuDat[(simuDat[\"group\"] == 0)&(simuDat['gender']==0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JRN7cTLKw-G"
      },
      "outputs": [],
      "source": [
        "riskset_counter.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv3e1S0kKw-G"
      },
      "outputs": [],
      "source": [
        "np.unique(riskset_counter.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbs2Ktw3Kw-H"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat[\"group\"] == 0]\n",
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKieyF-GKw-H"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat['gender'] == 0]\n",
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkSIGLtgKw-H"
      },
      "outputs": [],
      "source": [
        "np.unique(riskset_counter.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo94qnoRKw-H"
      },
      "outputs": [],
      "source": [
        "np.unique(simuDat_sub1['id'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-NX6i5MKw-H"
      },
      "source": [
        "#### Contr & female"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sRlbB5BKw-H"
      },
      "outputs": [],
      "source": [
        "simuDat_sub1 = simuDat[(simuDat[\"group\"] == 0)&(simuDat['gender']==1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiRwJaeMKw-H"
      },
      "outputs": [],
      "source": [
        "riskset_counter.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWZqBpAAKw-H"
      },
      "outputs": [],
      "source": [
        "np.unique(riskset_counter.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlsb0YJ7Kw-H"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat[\"group\"] == 0]\n",
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4-BRaWkKw-H"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat[\"gender\"] == 1]\n",
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MYGV3EzKw-I"
      },
      "outputs": [],
      "source": [
        "np.unique(riskset_counter.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huUxUC_tKw-I"
      },
      "outputs": [],
      "source": [
        "np.unique(simuDat_sub1['id'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtXAi4lOKw-I"
      },
      "source": [
        "#### Treat&male"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woHcOFs-Kw-I"
      },
      "outputs": [],
      "source": [
        "simuDat_sub1 = simuDat[(simuDat[\"group\"] == 1)&(simuDat['gender']==0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPYrfs8ZKw-I"
      },
      "outputs": [],
      "source": [
        "riskset_counter.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMrB24YJKw-I"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat['group']==1]\n",
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMhrRbxhKw-I"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat['gender']==0]\n",
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6MiLdbEKw-I"
      },
      "outputs": [],
      "source": [
        "np.unique(riskset_counter.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_vzzw-GKw-I"
      },
      "outputs": [],
      "source": [
        "np.unique(simuDat_sub1['id'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVdSMuUwKw-I"
      },
      "source": [
        "#### Treat&female"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9lKrLCuKw-I"
      },
      "outputs": [],
      "source": [
        "simuDat_sub1 = simuDat[(simuDat[\"group\"] == 1)&(simuDat['gender']==1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GGehc3-Kw-I"
      },
      "outputs": [],
      "source": [
        "riskset_counter.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIFW5zuVKw-I"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat['group']==1]\n",
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG42WE-eKw-I"
      },
      "outputs": [],
      "source": [
        "simuDat_male = simuDat[simuDat['gender']==1]\n",
        "riskset_counter.update(simuDat_male['id'].values, simuDat_male['start'].values, simuDat_male['stop'].values, simuDat_male['event'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU-BcXJQKw-J"
      },
      "outputs": [],
      "source": [
        "np.unique(riskset_counter.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMO-L2PDKw-J"
      },
      "outputs": [],
      "source": [
        "np.unique(simuDat_sub1['id'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AEBAF1au4VD"
      },
      "outputs": [],
      "source": [
        "def argbinsearch(arr, key_val):\n",
        "    arr_len = len(arr)\n",
        "    min_idx = 0\n",
        "    max_idx = arr_len\n",
        "\n",
        "    while min_idx < max_idx:\n",
        "        mid_idx = min_idx + ((max_idx - min_idx) // 2)\n",
        "\n",
        "        if mid_idx < 0 or mid_idx >= arr_len:\n",
        "            return -1\n",
        "\n",
        "        mid_val = arr[mid_idx]\n",
        "        if mid_val <= key_val:  # Change the condition to <=\n",
        "            min_idx = mid_idx + 1\n",
        "        else:\n",
        "            max_idx = mid_idx\n",
        "\n",
        "    return min_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH11s9dDCZLK"
      },
      "source": [
        "이 함수는 argbinsearch라는 이름의 함수로, 배열에서 주어진 키 값보다 크거나 같은 첫 번째 원소의 인덱스를 이진 탐색으로 찾아 반환합니다.\n",
        "\n",
        "자세한 코드 설명을 아래에 제공합니다:\n",
        "\n",
        "1. 입력:\n",
        "\n",
        "  * arr: 탐색 대상인 정렬된 배열\n",
        "  key_val: 찾고자 하는 키 값\n",
        "\n",
        "2. 초기 변수 설정:\n",
        "\n",
        "  * arr_len: 배열의 길이를 저장합니다.\n",
        "  * min_idx: 탐색 범위의 최솟값으로, 처음에는 배열의 시작 인덱스인 0으로 설정됩니다.\n",
        "  * max_idx: 탐색 범위의 최댓값으로, 처음에는 배열의 길이로 설정됩니다.\n",
        "\n",
        "3. 이진 탐색:\n",
        "\n",
        "  * while 루프를 사용하여 min_idx가 max_idx보다 작은 동안 탐색을 반복합니다.\n",
        "  * mid_idx: 현재 탐색 범위의 중간 인덱스를 계산합니다.\n",
        "  * mid_val: 중간 인덱스에 해당하는 배열의 원소 값을 가져옵니다.\n",
        "\n",
        "4. 키 값과 중간 값을 비교합니다:\n",
        "  * 만약 중간 값이 키 값보다 작거나 같으면, min_idx를 mid_idx + 1로 업데이트합니다. 이렇게 하면 탐색 범위의 왼쪽 부분을 제외하게 됩니다.\n",
        "  * 그렇지 않으면, max_idx를 mid_idx로 업데이트합니다. 이렇게 하면 탐색 범위의 오른쪽 부분을 제외하게 됩니다.\n",
        "\n",
        "5. 결과 반환:\n",
        "\n",
        "  * 루프가 종료되면, min_idx는 키 값보다 크거나 같은 첫 번째 원소의 인덱스를 가리키게 됩니다. 따라서 min_idx를 반환합니다.\n",
        "\n",
        "이 함수는 정렬된 배열에서 주어진 키 값보다 크거나 같은 첫 번째 원소의 위치를 효율적으로 찾기 위해 사용됩니다. 이진 탐색은 배열의 중간 값을 반복적으로 확인하면서 탐색 범위를 절반씩 줄여나가므로, 큰 배열에서도 빠르게 원하는 값을 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D85rJQwBKw-J"
      },
      "source": [
        "### PseudoScoreCriterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEViRgp-Kw-J"
      },
      "outputs": [],
      "source": [
        "class PseudoScoreCriterion:\n",
        "    def __init__(self, n_outputs, n_samples, unique_times, x, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        Initialize instance variables using the provided input parameters\n",
        "        Objects 'riskset_left', 'riskset_right', and 'riskset_total' are initialized using the 'RisksetCounter' class\n",
        "        \"\"\"\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_samples = n_samples\n",
        "        self.unique_times = unique_times\n",
        "        self.x = x\n",
        "        self.ids = ids\n",
        "        self.time_start = time_start\n",
        "        self.time_stop = time_stop\n",
        "        self.event = event\n",
        "\n",
        "        self.unique_ids = set(self.ids)  # Store unique ids for later use\n",
        "        self.unique_times = unique_times\n",
        "\n",
        "        self.riskset_left = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        self.riskset_right = RisksetCounter(ids, time_start, time_stop, event)\n",
        "        self.riskset_total = RisksetCounter(ids, time_start, time_stop, event)\n",
        "\n",
        "        self.samples_time_idx = np.searchsorted(unique_times, time_stop)\n",
        "\n",
        "        self.split_pos = 0\n",
        "        self.split_time_idx = 0\n",
        "\n",
        "        self._riskset_counter = RisksetCounter(ids, time_start, time_stop, event)  # 새로 추가\n",
        "\n",
        "    def init(self, y, sample_weight, n_samples, samples, start, end):\n",
        "        \"\"\"\n",
        "        Initialization function\n",
        "        Reset the risk set counters ('riskset_left','riskset_right','riskset_total') and updates 'riskset_total' with new data\n",
        "        \"\"\"\n",
        "        self.samples = samples\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "        self.riskset_total.reset()\n",
        "\n",
        "        time_starts, stop_times, events = y[:, 0], y[:, 1], y[:, 2]\n",
        "        ids_for_update = [self.ids[idx] for idx in samples[start:end]]\n",
        "        time_starts_for_update = [time_starts[idx] for idx in samples[start:end]]\n",
        "        stop_times_for_update = [stop_times[idx] for idx in samples[start:end]]\n",
        "        events_for_update = [events[idx] for idx in samples[start:end]]\n",
        "\n",
        "        # Combine unique times from both datasets\n",
        "        self.unique_times = np.unique(np.concatenate([self.unique_times, stop_times_for_update]))\n",
        "\n",
        "        self.riskset_total.update(ids_for_update, time_starts_for_update, stop_times_for_update, events_for_update)\n",
        "\n",
        "    def set_unique_times(self, unique_times):\n",
        "        \"\"\"Sets the unique times for the current node.\"\"\"\n",
        "        self.unique_times = unique_times\n",
        "\n",
        "## Group Indicator만으로 나누기...\n",
        "\n",
        "    # Functions returning the risk set value and event value for the given ID and time index from the respective risk set (left or right)\n",
        "    def Y_left_value(self, id_, t):\n",
        "        return self.riskset_left.Y_i(id_, t)\n",
        "\n",
        "    def Y_right_value(self, id_, t):\n",
        "        return self.riskset_right.Y_i(id_, t)\n",
        "\n",
        "    def dN_bar_left_value(self, id_, t):\n",
        "        return self.riskset_left.dN_bar_i(id_, t)\n",
        "\n",
        "    def dN_bar_right_value(self, id_, t):\n",
        "        return self.riskset_right.dN_bar_i(id_, t)\n",
        "\n",
        "    def calculate_variance_estimate(self):\n",
        "        \"\"\"\n",
        "        Functions to compute the variance estimate for the split\n",
        "        \"\"\"\n",
        "        n_unique_times = len(self.unique_times)\n",
        "\n",
        "        left_n_at_risk = np.pad(self.riskset_left.n_at_risk, (0, n_unique_times - len(self.riskset_left.n_at_risk)), 'edge')\n",
        "        right_n_at_risk = np.pad(self.riskset_right.n_at_risk, (0, n_unique_times - len(self.riskset_right.n_at_risk)), 'edge')\n",
        "\n",
        "        left_n_events = np.pad(self.riskset_left.n_events, (0, n_unique_times - len(self.riskset_left.n_events)), 'constant')\n",
        "        right_n_events = np.pad(self.riskset_right.n_events, (0, n_unique_times - len(self.riskset_right.n_events)), 'constant')\n",
        "\n",
        "        w = (left_n_at_risk * right_n_at_risk) / (left_n_at_risk + right_n_at_risk)\n",
        "\n",
        "        w_expanded = np.tile(w, len(self.unique_ids))\n",
        "        left_n_at_risk_expanded = np.tile(left_n_at_risk, len(self.unique_ids))\n",
        "        right_n_at_risk_expanded = np.tile(right_n_at_risk, len(self.unique_ids))\n",
        "\n",
        "        Y_left, Y_right, term_left, term_right = [], [], [], []\n",
        "\n",
        "        for id_ in self.unique_ids:\n",
        "            for t in range(n_unique_times):  # Ensure we don't exceed the bounds of the array\n",
        "                Y_left_val = self.Y_left_value(id_, t)\n",
        "                Y_right_val = self.Y_right_value(id_, t)\n",
        "\n",
        "                dN_bar_left_val = self.dN_bar_left_value(id_, t)\n",
        "                dN_bar_right_val = self.dN_bar_right_value(id_, t)\n",
        "\n",
        "                term_left_val = (dN_bar_left_val - (left_n_events[t] / left_n_at_risk[t])) ** 2\n",
        "                term_right_val = (dN_bar_right_val - (right_n_events[t] / right_n_at_risk[t])) ** 2\n",
        "\n",
        "                Y_left.append(Y_left_val)\n",
        "                Y_right.append(Y_right_val)\n",
        "                term_left.append(term_left_val)\n",
        "                term_right.append(term_right_val)\n",
        "\n",
        "        Y_left = np.array(Y_left)\n",
        "        Y_right = np.array(Y_right)\n",
        "        term_left = np.array(term_left)\n",
        "        term_right = np.array(term_right)\n",
        "\n",
        "        var_estimate_L = np.sum(w_expanded * (Y_left / left_n_at_risk_expanded) * term_left)\n",
        "        var_estimate_R = np.sum(w_expanded * (Y_right / right_n_at_risk_expanded) * term_right)\n",
        "\n",
        "        return var_estimate_L + var_estimate_R\n",
        "\n",
        "    def proxy_impurity_improvement(self):\n",
        "        \"\"\"\n",
        "        Functions that calculates the pseudo impurity improvement of the split.\n",
        "        This value represents the reduction in pseudo impurity in the risk sets after the split.\n",
        "        \"\"\"\n",
        "        # If either risk set is empty, return -np.inf\n",
        "        if len(self.riskset_left.n_at_risk) == 0 or len(self.riskset_right.n_at_risk) == 0:\n",
        "            return -np.inf\n",
        "\n",
        "        left_n_at_risk = np.pad(self.riskset_left.n_at_risk,\n",
        "                                (0, max(0, len(self.unique_times) - len(self.riskset_left.n_at_risk))),\n",
        "                                'edge')\n",
        "        right_n_at_risk = np.pad(self.riskset_right.n_at_risk,\n",
        "                                 (0, max(0, len(self.unique_times) - len(self.riskset_right.n_at_risk))),\n",
        "                                 'edge')\n",
        "\n",
        "        w = (left_n_at_risk * right_n_at_risk) / (left_n_at_risk + right_n_at_risk)\n",
        "\n",
        "        left_n_events = np.pad(self.riskset_left.n_events,\n",
        "                               (0, max(0, len(self.unique_times) - len(self.riskset_left.n_events))),\n",
        "                               'constant')\n",
        "        right_n_events = np.pad(self.riskset_right.n_events,\n",
        "                                (0, max(0, len(self.unique_times) - len(self.riskset_right.n_events))),\n",
        "                                'constant')\n",
        "\n",
        "        term = (left_n_events / left_n_at_risk) - (right_n_events / right_n_at_risk)\n",
        "        numer = np.sum(w * term)\n",
        "        var_estimate = self.calculate_variance_estimate()\n",
        "\n",
        "        return numer / (np.sqrt(var_estimate) + 1e-7)\n",
        "\n",
        "    def update_riskset(self, ids_subset):\n",
        "        # Update the riskset based on the subset of IDs at the current node\n",
        "        unique_ids_subset = np.unique(ids_subset)\n",
        "        self.riskset_counter.update(unique_ids_subset, self.time_start, self.time_stop, self.event)\n",
        "\n",
        "    def node_value(self):\n",
        "        \"\"\"\n",
        "        Returns the Nelson-Aalen estimator of the mean function μ(t) for the entities in the current node.\n",
        "        \"\"\"\n",
        "        return self.node_value_from_riskset(self.riskset_total)\n",
        "\n",
        "    def node_value_from_riskset(self, riskset_counter):\n",
        "        \"\"\"\n",
        "        Returns the Nelson-Aalen estimator of the mean function μ(t) for the entities based on provided riskset_counter.\n",
        "        \"\"\"\n",
        "        mu_hat_values = []\n",
        "\n",
        "        # Initialize the cumulative sum of the Nelson-Aalen estimator\n",
        "        cumsum_Nelson_Aalen = 0\n",
        "\n",
        "        for t_idx, t in enumerate(self.unique_times):\n",
        "            # Use n_at_risk and n_events from the riskset_counter\n",
        "            n_at_risk_t = riskset_counter.n_at_risk[t_idx] if t_idx < len(riskset_counter.n_at_risk) else 0\n",
        "            n_events_t = riskset_counter.n_events[t_idx] if t_idx < len(riskset_counter.n_events) else 0\n",
        "\n",
        "            cumsum_Nelson_Aalen += n_events_t / (n_at_risk_t + 1e-7)  # Avoiding division by zero\n",
        "            mu_hat_values.append(cumsum_Nelson_Aalen)\n",
        "\n",
        "        return mu_hat_values\n",
        "\n",
        "    # RisksetCounter의 상태를 저장하고 복원하기 위한 메서드를 추가합니다.\n",
        "    def save_riskset_state(self):\n",
        "        self._riskset_counter.save_state()\n",
        "\n",
        "    def reset_riskset_state(self):\n",
        "        self._riskset_counter.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Functions to reset all risk set counters\n",
        "        \"\"\"\n",
        "        self.riskset_total.reset()\n",
        "        self.riskset_left.reset()\n",
        "        self.riskset_right.reset()\n",
        "\n",
        "    def copy(self):\n",
        "        \"\"\"\n",
        "        Creates and returns a copy of the current object.\n",
        "        \"\"\"\n",
        "        new_criterion = PseudoScoreCriterion(self.n_outputs, self.n_samples, self.unique_times,\n",
        "                                             self.x, self.ids, self.time_start, self.time_stop,\n",
        "                                             self.event)\n",
        "        new_criterion.riskset_left = self.riskset_left.copy()\n",
        "        new_criterion.riskset_right = self.riskset_right.copy()\n",
        "        new_criterion.riskset_total = self.riskset_total.copy()\n",
        "        new_criterion.samples_time_idx = self.samples_time_idx.copy()\n",
        "        if hasattr(self, 'samples'):\n",
        "            new_criterion.samples = self.samples.copy()\n",
        "\n",
        "        return new_criterion\n",
        "\n",
        "def update_with_group_indicator(self, feature_index, group_indicator):\n",
        "    \"\"\"\n",
        "    Update the criterion based on a specified feature and group indicator.\n",
        "    This will split the data into left and right nodes based on the provided feature and group indicator.\n",
        "    \"\"\"\n",
        "    # Reset the riskset counters for the left and right nodes\n",
        "    self.riskset_left.reset()\n",
        "    self.riskset_right.reset()\n",
        "\n",
        "    # Determine the split by the feature and group indicator\n",
        "    left_mask = self.x[:, feature_index] <= group_indicator  # Changed to <= for continuous features\n",
        "    right_mask = ~left_mask\n",
        "\n",
        "    # Create empty lists to store the ids, start times, stop times, and events for both left and right splits\n",
        "    ids_left, start_left, stop_left, event_left = [], [], [], []\n",
        "    ids_right, start_right, stop_right, event_right = [], [], [], []\n",
        "\n",
        "    # For each unique ID, decide whether to assign it to the left or right node based on the mask\n",
        "    for id_ in self.unique_ids:\n",
        "        id_indices = np.where(self.ids == id_)[0]  # Get all indices for this ID\n",
        "        if left_mask[id_indices[0]]:\n",
        "            ids_left.extend([self.ids[i] for i in id_indices])\n",
        "            start_left.extend([self.time_start[i] for i in id_indices])\n",
        "            stop_left.extend([self.time_stop[i] for i in id_indices])\n",
        "            event_left.extend([self.event[i] for i in id_indices])\n",
        "        else:\n",
        "            ids_right.extend([self.ids[i] for i in id_indices])\n",
        "            start_right.extend([self.time_start[i] for i in id_indices])\n",
        "            stop_right.extend([self.time_stop[i] for i in id_indices])\n",
        "            event_right.extend([self.event[i] for i in id_indices])\n",
        "\n",
        "    # Set the all_unique_times for the risk sets of left and right nodes to the current node's unique times\n",
        "    self.riskset_left.all_unique_times = self.unique_times\n",
        "    self.riskset_right.all_unique_times = self.unique_times\n",
        "\n",
        "    # Also, adjust the lengths of n_at_risk and n_events in both riskset_left and riskset_right to match unique_times\n",
        "    self.riskset_left.n_at_risk = np.zeros(len(self.unique_times), dtype=np.int64)\n",
        "    self.riskset_left.n_events = np.zeros(len(self.unique_times), dtype=np.int64)\n",
        "    self.riskset_right.n_at_risk = np.zeros(len(self.unique_times), dtype=np.int64)\n",
        "    self.riskset_right.n_events = np.zeros(len(self.unique_times), dtype=np.int64)\n",
        "\n",
        "    # Update the risk sets for the left and right nodes\n",
        "    self.riskset_left.update(ids_left, start_left, stop_left, event_left)\n",
        "    self.riskset_right.update(ids_right, start_right, stop_right, event_right)\n",
        "\n",
        "# 이 함수를 PseudoScoreCriterion 클래스에 추가합니다.\n",
        "setattr(PseudoScoreCriterion, 'update', update_with_group_indicator)\n",
        "\n",
        "# 추가로, left node와 right node의 데이터를 반환하는 메소드를 추가합니다.\n",
        "def get_left_node_data(self):\n",
        "    return self.riskset_left.ids, self.riskset_left.n_at_risk, self.riskset_left.n_events\n",
        "\n",
        "def get_right_node_data(self):\n",
        "    return self.riskset_right.ids, self.riskset_right.n_at_risk, self.riskset_right.n_events\n",
        "\n",
        "setattr(PseudoScoreCriterion, 'get_left_node_data', get_left_node_data)\n",
        "setattr(PseudoScoreCriterion, 'get_right_node_data', get_right_node_data)\n",
        "\n",
        "def calculate_node_value_updated(self, side=\"left\"):\n",
        "    \"\"\"\n",
        "    Calculate the node value based on the updated RisksetCounter using get_left_node_data and get_right_node_data.\n",
        "\n",
        "    Parameters:\n",
        "        - side (str): Either \"left\" or \"right\" to determine which riskset to use for calculation.\n",
        "    \"\"\"\n",
        "    if side == \"left\":\n",
        "        ids, n_at_risk, n_events = self.get_left_node_data()\n",
        "    elif side == \"right\":\n",
        "        ids, n_at_risk, n_events = self.get_right_node_data()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid side value. Expected 'left' or 'right'.\")\n",
        "\n",
        "    mask = np.isin(self.ids, ids)\n",
        "\n",
        "    time_start_filtered = self.time_start[mask]\n",
        "    time_stop_filtered = self.time_stop[mask]\n",
        "    event_filtered = self.event[mask]\n",
        "\n",
        "    riskset_temp = RisksetCounter(ids, time_start_filtered, time_stop_filtered, event_filtered)\n",
        "    riskset_temp.n_at_risk = n_at_risk\n",
        "    riskset_temp.n_events = n_events\n",
        "\n",
        "    return self.node_value_from_riskset(riskset_temp)\n",
        "\n",
        "# PseudoScoreCriterion 클래스에 위에서 정의한 함수를 추가합니다.\n",
        "setattr(PseudoScoreCriterion, 'calculate_node_value', calculate_node_value_updated)\n",
        "\n",
        "\n",
        "\n",
        "PseudoScoreCriterion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_C0xezJ-Kw-K"
      },
      "outputs": [],
      "source": [
        "riskset_counter.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw0WIaeWKw-K"
      },
      "outputs": [],
      "source": [
        "n_samples = len(ids)\n",
        "\n",
        "# Create an instance of the RisksetCounter and PseudoScoreCriterion classes\n",
        "riskset = RisksetCounter(ids, time_start, time_stop, event)\n",
        "criterion = PseudoScoreCriterion(n_outputs=1, n_samples=n_samples, unique_times=np.unique(time_stop), x=x, ids=ids, time_start=time_start, time_stop=time_stop, event=event)\n",
        "\n",
        "criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emqn3yRYKw-K"
      },
      "outputs": [],
      "source": [
        "feature_index = 0\n",
        "group_indicator = 0\n",
        "\n",
        "criterion.update(feature_index, group_indicator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB1CUgnNKw-K"
      },
      "outputs": [],
      "source": [
        "criterion.get_left_node_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ3rwaRTKw-K"
      },
      "outputs": [],
      "source": [
        "criterion.get_right_node_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONkrBVHmKw-K"
      },
      "outputs": [],
      "source": [
        "criterion.proxy_impurity_improvement()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Org7SWJ9Kw-K"
      },
      "outputs": [],
      "source": [
        "criterion.calculate_node_value('left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr0CRDOMKw-K"
      },
      "outputs": [],
      "source": [
        "criterion.calculate_node_value('right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms5RCa_IKw-K"
      },
      "outputs": [],
      "source": [
        "riskset_counter.reset()\n",
        "\n",
        "n_samples = len(ids)\n",
        "\n",
        "# Create an instance of the RisksetCounter and PseudoScoreCriterion classes\n",
        "riskset = RisksetCounter(ids, time_start, time_stop, event)\n",
        "criterion = PseudoScoreCriterion(n_outputs=1, n_samples=n_samples, unique_times=np.unique(time_stop), x=x, ids=ids, time_start=time_start, time_stop=time_stop, event=event)\n",
        "\n",
        "criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuGNG4YLKw-K"
      },
      "outputs": [],
      "source": [
        "feature_index=2\n",
        "group_indicator = 0\n",
        "\n",
        "criterion.update(feature_index, group_indicator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es48lBDYKw-K"
      },
      "outputs": [],
      "source": [
        "criterion.get_left_node_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "No93dj56Kw-K"
      },
      "outputs": [],
      "source": [
        "criterion.get_right_node_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIq_Hdm8Kw-K"
      },
      "outputs": [],
      "source": [
        "criterion.proxy_impurity_improvement()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e_qjpLoKw-L"
      },
      "outputs": [],
      "source": [
        "criterion.node_value()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjMo_rG9Kw-L"
      },
      "outputs": [],
      "source": [
        "criterion.calculate_node_value('left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjlujnRKKw-L"
      },
      "outputs": [],
      "source": [
        "criterion.calculate_node_value('right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN1FTW8NKw-L"
      },
      "source": [
        "### 이전 버전에서 수정한 것!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_UL_R4iKw-L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "class PseudoScoreTreeBuilder:\n",
        "    \"\"\"\n",
        "    Class designed to build a decision tree based on the pseudo-score test statistics criterion,\n",
        "    typically used in recurrent events data analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    TREE_UNDEFINED = -1  # Placeholder\n",
        "\n",
        "    def __init__(self, max_depth=None, min_ids_split=2, min_ids_leaf=1,\n",
        "                 max_features=None, max_thresholds=None, min_impurity_decrease=0,\n",
        "                 random_state=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_ids_split = min_ids_split\n",
        "        self.min_ids_leaf = min_ids_leaf\n",
        "        self.max_features = max_features\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "    def split_indices(self, X_column, threshold, criterion, start, end):\n",
        "        \"\"\"Efficiently splits the data based on the given threshold for a specific feature column.\"\"\"\n",
        "        left_indices = np.where(X_column <= threshold)[0]\n",
        "        right_indices = np.where(X_column > threshold)[0]\n",
        "\n",
        "        # Convert local indices to global indices\n",
        "        left_indices = np.arange(start, end)[left_indices]\n",
        "        right_indices = np.arange(start, end)[right_indices]\n",
        "\n",
        "        return left_indices, right_indices\n",
        "\n",
        "    def _split(self, X, criterion, start, end):\n",
        "        best_split = {\n",
        "            'feature_index': None,\n",
        "            'threshold': None,\n",
        "            'improvement': -np.inf\n",
        "        }\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        for feature_index in range(n_features):\n",
        "            unique_thresholds = np.unique(X[start:end, feature_index])\n",
        "            if len(unique_thresholds) <= 1:\n",
        "                continue  # Skip if the feature has a single unique value\n",
        "\n",
        "            # If max_thresholds specified and unique_thresholds is larger, randomly sample\n",
        "            if self.max_thresholds and len(unique_thresholds) > self.max_thresholds:\n",
        "                unique_thresholds = self.random_state.choice(unique_thresholds, self.max_thresholds, replace=False)\n",
        "\n",
        "            for threshold in unique_thresholds:\n",
        "                criterion.update(feature_index, threshold)\n",
        "                improvement = criterion.proxy_impurity_improvement()\n",
        "\n",
        "                if improvement > best_split['improvement']:\n",
        "                    best_split = {\n",
        "                        'feature_index': feature_index,\n",
        "                        'threshold': threshold,\n",
        "                        'improvement': improvement\n",
        "                    }\n",
        "\n",
        "        return best_split\n",
        "\n",
        "    def _build(self, X, y, criterion, depth=0, start=0, end=None):\n",
        "        if end is None:\n",
        "            end = X.shape[0]\n",
        "\n",
        "        ids = y[start:end, 0]\n",
        "        unique_ids = np.unique(ids)\n",
        "\n",
        "        # Initialize RisksetCounter for the current node and compute the n_at_risk and n_events arrays\n",
        "        riskset_counter = RisksetCounter(ids, y[start:end, 1], y[start:end, 2], y[start:end, 3])\n",
        "        node_value = criterion.node_value_from_riskset(riskset_counter)\n",
        "\n",
        "        # Get unique times for the current node\n",
        "        node_unique_times = riskset_counter.all_unique_times.tolist()\n",
        "\n",
        "        # Adjust node_value length based on unique times of the node\n",
        "        node_value = node_value[:len(node_unique_times)]\n",
        "\n",
        "        # If current depth is equal to or greater than max_depth, stop further splits\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value,\n",
        "                'unique_times': node_unique_times,\n",
        "                'ids': unique_ids.tolist()\n",
        "            }\n",
        "\n",
        "        # If the number of unique IDs in the current node is less than min_ids_split, stop further splits\n",
        "        if len(unique_ids) < self.min_ids_split:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value,\n",
        "                'unique_times': node_unique_times,\n",
        "                'ids': unique_ids.tolist()\n",
        "            }\n",
        "\n",
        "        best_split = self._split(X, criterion, start, end)\n",
        "\n",
        "        # If improvement is less than min_impurity_decrease, stop further splits\n",
        "        if best_split['improvement'] < self.min_impurity_decrease:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value,\n",
        "                'unique_times': node_unique_times,\n",
        "                'ids': unique_ids.tolist()\n",
        "            }\n",
        "\n",
        "        left_indices, right_indices = self.split_indices(X[start:end, best_split['feature_index']], best_split['threshold'], criterion, start, end)\n",
        "\n",
        "        # Check if there's data in both the left and right child nodes\n",
        "        if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "            return {\n",
        "                'feature': None,\n",
        "                'threshold': None,\n",
        "                'left_child': None,\n",
        "                'right_child': None,\n",
        "                'node_value': node_value,\n",
        "                'unique_times': node_unique_times,\n",
        "                'ids': unique_ids.tolist()\n",
        "            }\n",
        "\n",
        "        # Build the left child\n",
        "        left_child = self._build(X[left_indices], y[left_indices], criterion, depth=depth+1)\n",
        "\n",
        "        # Build the right child\n",
        "        right_child = self._build(X[right_indices], y[right_indices], criterion, depth=depth+1)\n",
        "\n",
        "        return {\n",
        "            'feature': best_split['feature_index'],\n",
        "            'threshold': best_split['threshold'],\n",
        "            'left_child': left_child,\n",
        "            'right_child': right_child,\n",
        "            'node_value': node_value,\n",
        "            'unique_times': node_unique_times,\n",
        "            'ids': unique_ids.tolist()\n",
        "        }\n",
        "\n",
        "    def build(self, X, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        The main method to invoke the tree building process.\n",
        "        Initializes the pseudo-likelihood criterion using the input data and constructs the tree using the _build method.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        y = np.c_[ids, time_start, time_stop, event]\n",
        "\n",
        "        unique_times = np.unique(np.concatenate([time_start, time_stop]))\n",
        "        criterion = PseudoScoreCriterion(n_outputs=1, n_samples=n_samples,\n",
        "                                         unique_times=unique_times, x=X, ids=ids,\n",
        "                                         time_start=time_start, time_stop=time_stop, event=event)\n",
        "\n",
        "        tree = self._build(X, y, criterion)\n",
        "        return tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AF8dbS7YKw-L"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4MlKU8MKw-L"
      },
      "outputs": [],
      "source": [
        "ids = data['id'].values\n",
        "time_start = data['start'].values\n",
        "time_stop = data['stop'].values\n",
        "event = data['event'].values\n",
        "x = data[['gender','group']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEtFt_345a0O"
      },
      "outputs": [],
      "source": [
        "# Initialize and build the tree using PseudoScoreTreeBuilder\n",
        "tree_builder = PseudoScoreTreeBuilder(max_depth=2, min_ids_leaf=10, random_state=1190)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp8RnbiBKw-L"
      },
      "outputs": [],
      "source": [
        "tree_df = tree_builder.build(x, ids, time_start, time_stop, event)\n",
        "\n",
        "# Display the tree dataframe\n",
        "tree_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oecyAoIhKw-L"
      },
      "outputs": [],
      "source": [
        "class RecurrentTree:\n",
        "    def __init__(self, max_depth=None, min_ids_split=2, min_ids_leaf=1,\n",
        "                 max_features=None, max_thresholds=None, min_impurity_decrease=0,\n",
        "                 random_state=None):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        Initializes the tree's hyperparameters and settings\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_ids_split = min_ids_split\n",
        "        self.min_ids_leaf = min_ids_leaf\n",
        "        self.max_features = max_features\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.random_state = random_state\n",
        "        self.tree_ = None\n",
        "\n",
        "    def fit(self, X, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        Trains the recurrent tree using the input data\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        ids = np.array(ids)\n",
        "        time_start = np.array(time_start)\n",
        "        time_stop = np.array(time_stop)\n",
        "        event = np.array(event)\n",
        "\n",
        "        # Use the PseudoScoreTreeBuilder to build the tree\n",
        "        builder = PseudoScoreTreeBuilder(\n",
        "            max_depth=self.max_depth,\n",
        "            min_ids_split=self.min_ids_split,\n",
        "            min_ids_leaf=self.min_ids_leaf,\n",
        "            max_features=self.max_features,\n",
        "            max_thresholds=self.max_thresholds,\n",
        "            min_impurity_decrease=self.min_impurity_decrease,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_ = builder.build(X, ids, time_start, time_stop, event)\n",
        "        return self\n",
        "\n",
        "    def get_tree(self):\n",
        "        \"\"\"Return the tree as a dictionary.\"\"\"\n",
        "        return self.tree_\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"Traverse the tree to find the terminal node for a given sample.\"\"\"\n",
        "        if node[\"feature\"] is None:\n",
        "            return node\n",
        "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "            return self._traverse_tree(x, node[\"left_child\"])\n",
        "        else:\n",
        "            return self._traverse_tree(x, node[\"right_child\"])\n",
        "\n",
        "    def predict_mean_function(self, X, ids):\n",
        "        \"\"\"\n",
        "        Predict the node_value of the terminal node for given samples.\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        ids = np.array(ids)\n",
        "\n",
        "        mean_functions = [self._traverse_tree(sample, self.tree_)[\"node_value\"] for sample in X]\n",
        "        return mean_functions\n",
        "\n",
        "    def predict_rate_function(self, X, ids):\n",
        "        \"\"\"\n",
        "        Predict the rate function as the difference between unique time points for the terminal node.\n",
        "        \"\"\"\n",
        "        mean_functions = self.predict_mean_function(X, ids)\n",
        "        rate_functions = []\n",
        "\n",
        "        for func in mean_functions:\n",
        "            # Calculate rate function as the difference between consecutive time points\n",
        "            rate_function = np.diff(func, prepend=func[0])\n",
        "            rate_functions.append(rate_function)\n",
        "\n",
        "        return rate_functions\n",
        "\n",
        "    def apply(self, X, ids):\n",
        "        \"\"\"Return the index of the leaf that each unique ID is predicted as.\"\"\"\n",
        "        X = np.array(X, dtype=np.float32)\n",
        "        ids = np.array(ids)\n",
        "\n",
        "        unique_ids = np.unique(ids)\n",
        "        leaf_indices = []\n",
        "\n",
        "        for uid in unique_ids:\n",
        "            idx = np.where(ids == uid)[0]\n",
        "            representative_sample = X[idx[0]]  # Using the first sample of the group\n",
        "            leaf_index = self._get_leaf_index(representative_sample, self.tree_)\n",
        "            leaf_indices.append(leaf_index)\n",
        "\n",
        "        return np.array(leaf_indices)\n",
        "\n",
        "    def _get_leaf_index(self, x, node, current_index=0):\n",
        "        \"\"\"Traverse the tree to find the leaf index for a given sample.\"\"\"\n",
        "        if node[\"feature\"] is None:\n",
        "            return current_index\n",
        "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "            return self._get_leaf_index(x, node[\"left_child\"], current_index*2 + 1)\n",
        "        else:\n",
        "            return self._get_leaf_index(x, node[\"right_child\"], current_index*2 + 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfQHSY4MKw-M"
      },
      "source": [
        "### RecurrentTree V2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1hmzt4rKw-M"
      },
      "outputs": [],
      "source": [
        "class RecurrentTree:\n",
        "    def __init__(self, max_depth=None, min_ids_split=2, min_ids_leaf=1,\n",
        "                 max_features=None, max_thresholds=None, min_impurity_decrease=0,\n",
        "                 random_state=None):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        Initializes the tree's hyperparameters and settings\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_ids_split = min_ids_split\n",
        "        self.min_ids_leaf = min_ids_leaf\n",
        "        self.max_features = max_features\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.random_state = random_state\n",
        "        self.tree_ = None\n",
        "\n",
        "    def fit(self, X, ids, time_start, time_stop, event):\n",
        "        \"\"\"\n",
        "        Trains the recurrent tree using the input data\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        ids = np.array(ids)\n",
        "        time_start = np.array(time_start)\n",
        "        time_stop = np.array(time_stop)\n",
        "        event = np.array(event)\n",
        "\n",
        "        # Use the PseudoScoreTreeBuilder to build the tree\n",
        "        builder = PseudoScoreTreeBuilder(\n",
        "            max_depth=self.max_depth,\n",
        "            min_ids_split=self.min_ids_split,\n",
        "            min_ids_leaf=self.min_ids_leaf,\n",
        "            max_features=self.max_features,\n",
        "            max_thresholds=self.max_thresholds,\n",
        "            min_impurity_decrease=self.min_impurity_decrease,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.tree_ = builder.build(X, ids, time_start, time_stop, event)\n",
        "        return self\n",
        "\n",
        "    def get_tree(self):\n",
        "        \"\"\"Return the tree as a dictionary.\"\"\"\n",
        "        return self.tree_\n",
        "\n",
        "    def traverse_tree_for_id(self, X_id_samples, node):\n",
        "        \"\"\"\n",
        "        Traverse the tree for a specific ID based on its samples.\n",
        "\n",
        "        Args:\n",
        "        - X_id_samples (list of arrays): The samples corresponding to a specific ID.\n",
        "        - node (dict): The current node being evaluated in the tree.\n",
        "\n",
        "        Returns:\n",
        "        - node (dict): The terminal node for the specific ID.\n",
        "        \"\"\"\n",
        "        if node[\"feature\"] is None:  # Terminal node\n",
        "            return node\n",
        "\n",
        "        # Traverse the tree for each sample and collect the terminal nodes\n",
        "        terminal_nodes = []\n",
        "        for sample in X_id_samples:\n",
        "            if sample[node[\"feature\"]] <= node[\"threshold\"]:\n",
        "                terminal_nodes.append(self.traverse_tree_for_id([sample], node[\"left_child\"]))\n",
        "            else:\n",
        "                terminal_nodes.append(self.traverse_tree_for_id([sample], node[\"right_child\"]))\n",
        "\n",
        "        # Check if all samples lead to the same terminal node\n",
        "        first_terminal = terminal_nodes[0]\n",
        "        if all(node == first_terminal for node in terminal_nodes):\n",
        "            return first_terminal\n",
        "\n",
        "        # If samples lead to different terminal nodes, it's ambiguous. For simplicity, return the first terminal node.\n",
        "        # In a real-world scenario, this might need more sophisticated handling.\n",
        "        return first_terminal\n",
        "\n",
        "    def predict_mean_function(self, X, ids):\n",
        "        \"\"\"\n",
        "        Predict the node_value of the terminal node for given samples.\n",
        "        \"\"\"\n",
        "\n",
        "        # Ensure X is a list of samples\n",
        "        X = np.array(X)\n",
        "\n",
        "        mean_function_predictions = {}\n",
        "\n",
        "        for sample_id in ids:\n",
        "            samples_for_id = [X[i] for i, uid in enumerate(ids) if uid == sample_id]\n",
        "            terminal_node_for_id = self.traverse_tree_for_id(samples_for_id, self.tree_)\n",
        "\n",
        "            mean_function_predictions[sample_id] = {\n",
        "                \"unique_times\": terminal_node_for_id.get('unique_times', []),\n",
        "                \"mean_function\": terminal_node_for_id[\"node_value\"]\n",
        "            }\n",
        "\n",
        "        return mean_function_predictions\n",
        "\n",
        "    def predict_rate_function(self, X, ids):\n",
        "        \"\"\"\n",
        "        Predict the rate function as the difference between unique time points for the terminal node.\n",
        "        \"\"\"\n",
        "\n",
        "        if np.isscalar(ids):\n",
        "            ids = [ids]\n",
        "\n",
        "        mean_function_predictions = self.predict_mean_function(X, ids)\n",
        "\n",
        "        rate_function_predictions = {}\n",
        "        for sample_id in ids:\n",
        "            unique_times = mean_function_predictions[sample_id][\"unique_times\"]\n",
        "            mean_function_values = mean_function_predictions[sample_id][\"mean_function\"]\n",
        "\n",
        "            # Calculate rate function as the difference between consecutive mean function values\n",
        "            rate_function = np.diff(mean_function_values, prepend=mean_function_values[0])\n",
        "\n",
        "            rate_function_predictions[sample_id] = {\n",
        "                \"times\": unique_times,\n",
        "                \"rates\": rate_function\n",
        "            }\n",
        "\n",
        "        return rate_function_predictions\n",
        "\n",
        "    def _map_terminal_nodes(self, node):\n",
        "        \"\"\"\n",
        "        Recursively traverse the tree and assign unique integers to each terminal node.\n",
        "        \"\"\"\n",
        "        if node[\"feature\"] is None:  # Terminal node\n",
        "            if __builtins__.id(node) not in self.terminal_node_mapping:\n",
        "                self.terminal_node_mapping[__builtins__.id(node)] = len(self.terminal_node_mapping)\n",
        "            return\n",
        "\n",
        "        self._map_terminal_nodes(node[\"left_child\"])\n",
        "        self._map_terminal_nodes(node[\"right_child\"])\n",
        "\n",
        "    def apply(self, X, ids=None):\n",
        "        \"\"\"Return the index of the leaf that each unique ID is predicted as.\"\"\"\n",
        "        X = np.array(X, dtype=np.float32)\n",
        "        if ids is None:\n",
        "            ids = np.array([i for i in range(X.shape[0])])\n",
        "        else:\n",
        "            ids = np.array(ids)\n",
        "\n",
        "        terminal_nodes = {}\n",
        "        self.terminal_node_mapping = {}  # Reset the mapping\n",
        "        self._map_terminal_nodes(self.tree_)\n",
        "\n",
        "        for sample_id in np.unique(ids):\n",
        "            samples_for_id = [X[i] for i, uid in enumerate(ids) if uid == sample_id]\n",
        "            terminal_node_for_id = self.traverse_tree_for_id(samples_for_id, self.tree_)\n",
        "            terminal_nodes[sample_id] = self.terminal_node_mapping[__builtins__.id(terminal_node_for_id)]\n",
        "\n",
        "        return terminal_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_rUQE4yKw-M"
      },
      "outputs": [],
      "source": [
        "x = data[['group','gender']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL0G16an23L4"
      },
      "outputs": [],
      "source": [
        "# 2. RecurrentTree 학습 및 예측\n",
        "tree_model = RecurrentTree(max_depth=2, random_state=1190)\n",
        "tree_model.fit(x, ids, time_start, time_stop, event)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIJxLbdSRzOJ"
      },
      "outputs": [],
      "source": [
        "%pip install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbIzVrVlKw-M"
      },
      "outputs": [],
      "source": [
        "tree_model.get_tree()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B9VDxduKw-M"
      },
      "outputs": [],
      "source": [
        "tree = tree_model.get_tree()\n",
        "tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQBBvOUoKw-M"
      },
      "outputs": [],
      "source": [
        "predict_mean_function=tree_model.predict_mean_function(x,ids=ids)\n",
        "print(\"ID 1 predicted mean function:\",predict_mean_function[1])\n",
        "print(\"ID 2 predicted mean function:\",predict_mean_function[2])\n",
        "print(\"ID 26 predicted mean function:\",predict_mean_function[26])\n",
        "print(\"ID 27 predicted mean function:\",predict_mean_function[27])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMkQvmBQKw-M"
      },
      "outputs": [],
      "source": [
        "tree_model.apply(x,ids=ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DE0eWD_Kw-M"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "\n",
        "def visualize_tree_simple(tree):\n",
        "    \"\"\"\n",
        "    Visualize the tree using graphviz.\n",
        "    This function only displays the structure of the tree and the threshold values for each node.\n",
        "    \"\"\"\n",
        "    graph = graphviz.Digraph()\n",
        "\n",
        "    def traverse_tree(node, parent_name=None, decision=None):\n",
        "        nonlocal node_counter\n",
        "\n",
        "        if node is None:\n",
        "            return\n",
        "\n",
        "        node_name = f\"node{node_counter}\"\n",
        "        node_counter += 1\n",
        "\n",
        "        # If the current node is a leaf node\n",
        "        if node[\"feature\"] is None:\n",
        "            leaf_info = \"\\n\".join([f\"t{idx}: {value:.2f}\" for idx, value in enumerate(node['node_value'])])\n",
        "            graph.node(node_name, label=leaf_info, shape=\"box\")\n",
        "        else:\n",
        "            decision_info = f\"Feature {node['feature']} <= {node['threshold']:.2f}\"\n",
        "            graph.node(node_name, label=decision_info)\n",
        "            traverse_tree(node['left_child'], node_name, decision=\"True\")\n",
        "            traverse_tree(node['right_child'], node_name, decision=\"False\")\n",
        "\n",
        "        # Connect the parent node to the current node\n",
        "        if parent_name:\n",
        "            graph.edge(parent_name, node_name, label=decision)\n",
        "\n",
        "    node_counter = 0\n",
        "    traverse_tree(tree)\n",
        "\n",
        "    return graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8YF091GKw-M"
      },
      "outputs": [],
      "source": [
        "dot = visualize_tree_simple(tree)\n",
        "dot.view()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YblgOR6lKw-N"
      },
      "outputs": [],
      "source": [
        "def visualize_tree_with_data(tree):\n",
        "    \"\"\"\n",
        "    Visualize the tree using graphviz.\n",
        "    This function displays the structure of the tree, the threshold values for each node,\n",
        "    and the unique IDs at leaf nodes.\n",
        "    \"\"\"\n",
        "    import graphviz\n",
        "\n",
        "    graph = graphviz.Digraph()\n",
        "\n",
        "    def traverse_tree(node, parent_name=None, decision=None):\n",
        "        nonlocal node_counter\n",
        "\n",
        "        if node is None:\n",
        "            return\n",
        "\n",
        "        node_name = f\"node{node_counter}\"\n",
        "        node_counter += 1\n",
        "\n",
        "        # If the current node is a leaf node\n",
        "        if node[\"feature\"] is None:\n",
        "            leaf_info = f\"Unique IDs: {node['ids']}\"\n",
        "            graph.node(node_name, label=leaf_info, shape=\"box\")\n",
        "        else:\n",
        "            decision_info = f\"Feature {node['feature']} <= {node['threshold']:.2f}\"\n",
        "            graph.node(node_name, label=decision_info)\n",
        "\n",
        "            traverse_tree(node['left_child'], node_name, decision=\"True\")\n",
        "            traverse_tree(node['right_child'], node_name, decision=\"False\")\n",
        "\n",
        "        # Connect the parent node to the current node\n",
        "        if parent_name:\n",
        "            graph.edge(parent_name, node_name, label=decision)\n",
        "\n",
        "    node_counter = 0\n",
        "    traverse_tree(tree)\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi8vFRG7Kw-N"
      },
      "outputs": [],
      "source": [
        "dot = visualize_tree_with_data(tree)\n",
        "dot.view()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWUHgbdhKw-N"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "\n",
        "def visualize_tree_simple(tree):\n",
        "    \"\"\"\n",
        "    Visualize the tree using graphviz.\n",
        "    This function only displays the structure of the tree and the threshold values for each node.\n",
        "    \"\"\"\n",
        "    graph = graphviz.Digraph()\n",
        "\n",
        "    def traverse_tree(node, parent_name=None, decision=None):\n",
        "        nonlocal node_counter\n",
        "\n",
        "        if node is None:\n",
        "            return\n",
        "\n",
        "        node_name = f\"node{node_counter}\"\n",
        "        node_counter += 1\n",
        "\n",
        "        # If the current node is a leaf node\n",
        "        if node[\"feature\"] is None:\n",
        "            graph.node(node_name, label=\"Leaf Node\", shape=\"box\")\n",
        "        else:\n",
        "            decision_info = f\"Feature {node['feature']} <= {node['threshold']:.2f}\"\n",
        "            graph.node(node_name, label=decision_info)\n",
        "            traverse_tree(node['left_child'], node_name, decision=\"True\")\n",
        "            traverse_tree(node['right_child'], node_name, decision=\"False\")\n",
        "\n",
        "        # Connect the parent node to the current node\n",
        "        if parent_name:\n",
        "            graph.edge(parent_name, node_name, label=decision)\n",
        "\n",
        "    node_counter = 0\n",
        "    traverse_tree(tree)\n",
        "\n",
        "    return graph\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7Qwc1h1Kw-N"
      },
      "outputs": [],
      "source": [
        "dot = visualize_tree_simple(tree)\n",
        "dot.view()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25p7wJrWKw-N"
      },
      "source": [
        "### Mission Clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TIqgsVOKw-N"
      },
      "outputs": [],
      "source": [
        "x = data[['group','x1','gender']].values\n",
        "# 2. RecurrentTree 학습 및 예측\n",
        "tree_model = RecurrentTree(max_depth=3, random_state=1190)\n",
        "tree_model.fit(x, ids, time_start, time_stop, event)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuR7PY8KKw-N"
      },
      "outputs": [],
      "source": [
        "tree=tree_model.get_tree()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgjyus0aKw-N"
      },
      "outputs": [],
      "source": [
        "predict_mean_function=tree_model.predict_mean_function(x,ids)\n",
        "predict_mean_function[64]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpW4WjtcKw-N"
      },
      "outputs": [],
      "source": [
        "tree_model.apply(x,ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHcE99SpKw-N"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "\n",
        "def visualize_tree_simple(tree):\n",
        "    \"\"\"\n",
        "    Visualize the tree using graphviz.\n",
        "    This function only displays the structure of the tree and the threshold values for each node.\n",
        "    \"\"\"\n",
        "    graph = graphviz.Digraph()\n",
        "\n",
        "    def traverse_tree(node, parent_name=None, decision=None):\n",
        "        nonlocal node_counter\n",
        "\n",
        "        if node is None:\n",
        "            return\n",
        "\n",
        "        node_name = f\"node{node_counter}\"\n",
        "        node_counter += 1\n",
        "\n",
        "        # If the current node is a leaf node\n",
        "        if node[\"feature\"] is None:\n",
        "            graph.node(node_name, label=\"Leaf Node\", shape=\"box\")\n",
        "        else:\n",
        "            decision_info = f\"Feature {node['feature']} <= {node['threshold']:.2f}\"\n",
        "            graph.node(node_name, label=decision_info)\n",
        "            traverse_tree(node['left_child'], node_name, decision=\"True\")\n",
        "            traverse_tree(node['right_child'], node_name, decision=\"False\")\n",
        "\n",
        "        # Connect the parent node to the current node\n",
        "        if parent_name:\n",
        "            graph.edge(parent_name, node_name, label=decision)\n",
        "\n",
        "    node_counter = 0\n",
        "    traverse_tree(tree)\n",
        "\n",
        "    return graph\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgOKgk6oKw-N"
      },
      "outputs": [],
      "source": [
        "dot = visualize_tree_simple(tree)\n",
        "dot.view()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_DGcR2TKw-N"
      },
      "outputs": [],
      "source": [
        "import graphviz\n",
        "\n",
        "def visualize_tree_simple(tree):\n",
        "    \"\"\"\n",
        "    Visualize the tree using graphviz.\n",
        "    This function only displays the structure of the tree and the threshold values for each node.\n",
        "    \"\"\"\n",
        "    graph = graphviz.Digraph()\n",
        "\n",
        "    def traverse_tree(node, parent_name=None, decision=None):\n",
        "        nonlocal node_counter\n",
        "\n",
        "        if node is None:\n",
        "            return\n",
        "\n",
        "        node_name = f\"node{node_counter}\"\n",
        "        node_counter += 1\n",
        "\n",
        "        # If the current node is a leaf node\n",
        "        if node[\"feature\"] is None:\n",
        "            leaf_info = \"\\n\".join([f\"t{idx}: {value:.2f}\" for idx, value in enumerate(node['node_value'])])\n",
        "            graph.node(node_name, label=leaf_info, shape=\"box\")\n",
        "        else:\n",
        "            decision_info = f\"Feature {node['feature']} <= {node['threshold']:.2f}\"\n",
        "            graph.node(node_name, label=decision_info)\n",
        "            traverse_tree(node['left_child'], node_name, decision=\"True\")\n",
        "            traverse_tree(node['right_child'], node_name, decision=\"False\")\n",
        "\n",
        "        # Connect the parent node to the current node\n",
        "        if parent_name:\n",
        "            graph.edge(parent_name, node_name, label=decision)\n",
        "\n",
        "    node_counter = 0\n",
        "    traverse_tree(tree)\n",
        "\n",
        "    return graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5CVa-NOKw-N"
      },
      "outputs": [],
      "source": [
        "dot = visualize_tree_simple(tree)\n",
        "dot.view()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyazQ6CPKw-N"
      },
      "outputs": [],
      "source": [
        "def visualize_tree_with_data(tree):\n",
        "    \"\"\"\n",
        "    Visualize the tree using graphviz.\n",
        "    This function displays the structure of the tree, the threshold values for each node,\n",
        "    and the unique IDs at leaf nodes.\n",
        "    \"\"\"\n",
        "    import graphviz\n",
        "\n",
        "    graph = graphviz.Digraph()\n",
        "\n",
        "    def traverse_tree(node, parent_name=None, decision=None):\n",
        "        nonlocal node_counter\n",
        "\n",
        "        if node is None:\n",
        "            return\n",
        "\n",
        "        node_name = f\"node{node_counter}\"\n",
        "        node_counter += 1\n",
        "\n",
        "        # If the current node is a leaf node\n",
        "        if node[\"feature\"] is None:\n",
        "            leaf_info = f\"Unique IDs: {node['ids']}\"\n",
        "            graph.node(node_name, label=leaf_info, shape=\"box\")\n",
        "        else:\n",
        "            decision_info = f\"Feature {node['feature']} <= {node['threshold']:.2f}\"\n",
        "            graph.node(node_name, label=decision_info)\n",
        "\n",
        "            traverse_tree(node['left_child'], node_name, decision=\"True\")\n",
        "            traverse_tree(node['right_child'], node_name, decision=\"False\")\n",
        "\n",
        "        # Connect the parent node to the current node\n",
        "        if parent_name:\n",
        "            graph.edge(parent_name, node_name, label=decision)\n",
        "\n",
        "    node_counter = 0\n",
        "    traverse_tree(tree)\n",
        "\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40Yf1bxSKw-O"
      },
      "outputs": [],
      "source": [
        "dot = visualize_tree_with_data(tree)\n",
        "dot.view()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qytB7-0Kw-O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teHlBzwZKw-O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJB-7zrSKw-O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_md8-ygKw-O"
      },
      "source": [
        "### RecurrentRandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6C0ZZw7u4VM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numbers import Integral, Real\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "def _get_n_samples_bootstrap(n_ids, max_samples):\n",
        "    \"\"\"\n",
        "    Modified for recurrent events. Get the number of IDs in a bootstrap sample.\n",
        "    \"\"\"\n",
        "    if max_samples is None:\n",
        "        return n_ids\n",
        "\n",
        "    if isinstance(max_samples, Integral):\n",
        "        if max_samples > n_ids:\n",
        "            msg = \"`max_samples` must be <= n_ids={} but got value {}\"\n",
        "            raise ValueError(msg.format(n_ids, max_samples))\n",
        "        return max_samples\n",
        "\n",
        "    if isinstance(max_samples, Real):\n",
        "        return max(round(n_ids * max_samples), 1)\n",
        "\n",
        "def _generate_sample_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Sample unique IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    random_instance = check_random_state(random_state)\n",
        "    sampled_ids = np.random.choice(ids, n_ids_bootstrap, replace=True)\n",
        "    return sampled_ids\n",
        "\n",
        "def _generate_unsampled_indices(random_state, ids, n_ids_bootstrap):\n",
        "    \"\"\"\n",
        "    Determine unsampled IDs and then expand to all associated events.\n",
        "    \"\"\"\n",
        "    unique_ids = np.unique(ids)\n",
        "    sampled_ids = _generate_sample_indices(random_state, unique_ids, n_ids_bootstrap)\n",
        "    unique_sampled_ids = np.unique(sampled_ids)  # Ensure sampled IDs are unique\n",
        "    unsampled_ids = np.setdiff1d(unique_ids, unique_sampled_ids)\n",
        "\n",
        "    # Expand these unsampled IDs to include all their associated events.\n",
        "    unsampled_indices = np.where(np.isin(ids, unsampled_ids))[0]\n",
        "\n",
        "    return unsampled_indices\n",
        "\n",
        "from warnings import catch_warnings, simplefilter\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "def _parallel_build_trees(\n",
        "    tree,\n",
        "    bootstrap,\n",
        "    X,\n",
        "    y,  # Now, y is expected to be a dictionary with 'id', 'time_start', 'time_stop', and 'event' as keys\n",
        "    sample_weight,\n",
        "    tree_idx,\n",
        "    n_trees,\n",
        "    verbose=0,\n",
        "    class_weight=None,\n",
        "    n_ids_bootstrap=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Private function used to fit a single tree in parallel for recurrent events.\n",
        "    \"\"\"\n",
        "    # Extract necessary data from y\n",
        "    ids = y['id']\n",
        "    time_start = y['time_start']\n",
        "    time_stop = y['time_stop']\n",
        "    event = y['event']\n",
        "\n",
        "    if verbose > 1:\n",
        "        print(\"building tree %d of %d\" % (tree_idx + 1, n_trees))\n",
        "\n",
        "    if bootstrap:\n",
        "        unique_ids = np.unique(ids)\n",
        "        n_ids = len(unique_ids)\n",
        "\n",
        "        # Generate bootstrap samples using IDs\n",
        "        sampled_ids = _generate_sample_indices(\n",
        "            tree.random_state, unique_ids, n_ids_bootstrap\n",
        "        )\n",
        "\n",
        "        # Expand sampled IDs to all their associated events\n",
        "        indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "\n",
        "        if sample_weight is None:\n",
        "            curr_sample_weight = np.ones((X.shape[0],), dtype=np.float64)\n",
        "        else:\n",
        "            curr_sample_weight = sample_weight.copy()\n",
        "\n",
        "        # Adjust the sample weight based on how many times each ID was sampled\n",
        "        sample_counts_for_ids = np.bincount(np.searchsorted(unique_ids, sampled_ids), minlength=n_ids)\n",
        "        curr_sample_weight *= sample_counts_for_ids[np.searchsorted(unique_ids, ids)]\n",
        "\n",
        "        if class_weight == \"subsample\":\n",
        "            with catch_warnings():\n",
        "                simplefilter(\"ignore\", DeprecationWarning)\n",
        "                curr_sample_weight *= compute_sample_weight(\"auto\", event, indices=indices)\n",
        "        elif class_weight == \"balanced_subsample\":\n",
        "            curr_sample_weight *= compute_sample_weight(\"balanced\", event, indices=indices)\n",
        "\n",
        "        tree.fit(X[indices], {'id': ids[indices], 'time_start': time_start[indices], 'time_stop': time_stop[indices], 'event': event[indices]}, sample_weight=curr_sample_weight[indices], check_input=False)\n",
        "    else:\n",
        "        tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
        "\n",
        "    return tree\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtVL15B-Kw-O"
      },
      "outputs": [],
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "tree = RecurrentTree()\n",
        "dump(tree, 'test.pkl')  # 직렬화 시도\n",
        "loaded_tree = load('test.pkl')  # 역직렬화 시도"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8krFjeytKw-O"
      },
      "outputs": [],
      "source": [
        "loaded_tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nfxib_49tgG"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.utils import check_array\n",
        "from joblib import Parallel, delayed\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "class RecurrentRandomForest(BaseEstimator):\n",
        "    \"\"\"\n",
        "    A Random Forest model designed for recurrent event data.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, max_depth=None, min_ids_split=2,\n",
        "                 min_ids_leaf=1, bootstrap=True, oob_score=False, n_jobs=None,\n",
        "                 random_state=None, verbose=0, warm_start=False, max_samples=None,\n",
        "                 min_impurity_decrease=0.0, max_features=None, max_thresholds=None):  # Add new parameters\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_ids_split = min_ids_split\n",
        "        self.min_ids_leaf = min_ids_leaf\n",
        "        self.bootstrap = bootstrap\n",
        "        self.oob_score = oob_score\n",
        "        self.n_jobs = n_jobs\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.warm_start = warm_start\n",
        "        self.max_samples = max_samples\n",
        "        self.min_impurity_decrease = min_impurity_decrease\n",
        "        self.max_features = max_features\n",
        "        self.max_thresholds = max_thresholds\n",
        "        self.estimators_ = [self._make_estimator(random_state=i) for i in range(self.n_estimators)]\n",
        "\n",
        "    def _make_estimator(self, random_state=None):\n",
        "        \"\"\"\n",
        "        Constructs a new instances of the 'RecurrentTree' with the specified hyperparameters\n",
        "        Allows for creating each tree with a different 'random_state' for randomness\n",
        "        \"\"\"\n",
        "        return RecurrentTree(\n",
        "            max_depth=self.max_depth,\n",
        "            min_ids_split=self.min_ids_split,  # Note: changed the parameter name\n",
        "            min_ids_leaf=self.min_ids_leaf,    # Note: changed the parameter name\n",
        "            random_state=random_state,\n",
        "            min_impurity_decrease=self.min_impurity_decrease,\n",
        "            max_features=self.max_features,\n",
        "            max_thresholds=self.max_thresholds  # New parameter\n",
        "        )\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Trains the random forest using the input data\n",
        "        \"\"\"\n",
        "        X = self._validate_data(X, accept_sparse='csc', ensure_min_samples=2)\n",
        "        ids = y['id']\n",
        "        time_start = y['time_start']\n",
        "        time_stop = y['time_stop']\n",
        "        event = y['event']\n",
        "        self.n_features_in_ = X.shape[1]\n",
        "\n",
        "        n_samples_bootstrap = _get_n_samples_bootstrap(len(np.unique(ids)), self.max_samples)\n",
        "\n",
        "        def _fit_tree(tree):\n",
        "            if self.bootstrap:\n",
        "                unique_ids = np.unique(ids)\n",
        "                sampled_ids = _generate_sample_indices(tree.random_state, unique_ids, n_samples_bootstrap)\n",
        "                bootstrap_indices = np.where(np.isin(ids, sampled_ids))[0]\n",
        "                X_bootstrap = X[bootstrap_indices]\n",
        "                ids_bootstrap = np.array(ids)[bootstrap_indices]\n",
        "                time_start_bootstrap = np.array(time_start)[bootstrap_indices]\n",
        "                time_stop_bootstrap = np.array(time_stop)[bootstrap_indices]\n",
        "                event_bootstrap = np.array(event)[bootstrap_indices]\n",
        "            else:\n",
        "                X_bootstrap = X\n",
        "                ids_bootstrap = ids\n",
        "                time_start_bootstrap = time_start\n",
        "                time_stop_bootstrap = time_stop\n",
        "                event_bootstrap = event\n",
        "\n",
        "            tree.fit(X_bootstrap, ids_bootstrap, time_start_bootstrap, time_stop_bootstrap, event_bootstrap)\n",
        "            return tree\n",
        "\n",
        "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(_fit_tree)(tree) for tree in self.estimators_\n",
        "        )\n",
        "\n",
        "        if self.oob_score:\n",
        "            self._set_oob_score_and_attributes(X, y)\n",
        "        return self\n",
        "\n",
        "    def _set_oob_score_and_attributes(self, X, y):\n",
        "        \"\"\"\n",
        "        Calculates the out-of-bag (OOB) scores using the ensemble's predictions for the training data samples that were not seen during the training of a given tree.\n",
        "        Also sets the 'oob_prediction_' and 'oob_score_' attributes of the class.\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Assuming y is a structured array with these keys.\n",
        "        ids = y['id']\n",
        "        time_start = y['time_start']\n",
        "        time_stop = y['time_stop']\n",
        "        event = y['event']\n",
        "\n",
        "        all_predictions = []\n",
        "        n_samples_bootstrap = _get_n_samples_bootstrap(len(np.unique(ids)), self.max_samples)\n",
        "\n",
        "        for estimator in self.estimators_:\n",
        "            unsampled_indices = _generate_unsampled_indices(estimator.random_state, np.unique(ids), n_samples_bootstrap)\n",
        "            p_estimator_result = estimator.predict_mean_function(X[unsampled_indices, :], ids[unsampled_indices])\n",
        "            all_predictions.extend(p_estimator_result)\n",
        "\n",
        "        # Aggregate the predictions for each unique ID\n",
        "        aggregated_predictions = self._aggregate_predictions(all_predictions, ids)\n",
        "\n",
        "        self.oob_prediction_ = aggregated_predictions\n",
        "\n",
        "        # Assuming the presence of a utility function to calculate the C-index based on aggregated predictions\n",
        "        self.oob_score_ = self._estimate_recurrent_concordance_index(aggregated_predictions, X, event, ids)\n",
        "\n",
        "\n",
        "    def _estimate_recurrent_concordance_index(self, predictions, X, event, ids, total_events):\n",
        "        \"\"\"\n",
        "        Estimate the C-index for recurrent events using OOB ensemble estimates for right-censored data.\n",
        "\n",
        "        Parameters:\n",
        "        - predictions: Predicted mean functions for all samples using OOB.\n",
        "        - X: The data matrix.\n",
        "        - event: Observed recurrent events for all samples.\n",
        "        - ids: IDs for each event.\n",
        "        - total_events: Total number of events for each ID.\n",
        "\n",
        "        Returns:\n",
        "        - C-index estimate.\n",
        "        \"\"\"\n",
        "        unique_ids = np.unique(ids)\n",
        "        n_unique_ids = len(unique_ids)\n",
        "\n",
        "        id_to_avg_prediction = {}\n",
        "        for uid in unique_ids:\n",
        "            uid_indices = np.where(ids == uid)[0]\n",
        "            uid_predictions = [predictions[i] for i in uid_indices]\n",
        "\n",
        "            # 각 원소별로 평균을 계산\n",
        "            max_length = max(map(len, uid_predictions))\n",
        "            avg_prediction = []\n",
        "            for i in range(max_length):\n",
        "                avg_prediction.append(np.mean([pred[i] for pred in uid_predictions if i < len(pred)]))\n",
        "\n",
        "            id_to_avg_prediction[uid] = avg_prediction\n",
        "\n",
        "        concordant_pairs = 0\n",
        "        permissible_pairs = 0\n",
        "\n",
        "        for i in range(n_unique_ids):\n",
        "            for j in range(i+1, n_unique_ids):\n",
        "                uid_i = unique_ids[i]\n",
        "                uid_j = unique_ids[j]\n",
        "\n",
        "                right_censored_i = event[ids == uid_i][0] < total_events[uid_i]\n",
        "                right_censored_j = event[ids == uid_j][0] < total_events[uid_j]\n",
        "\n",
        "                if not right_censored_i and not right_censored_j:  # Both are not right-censored\n",
        "                    if event[ids == uid_i][0] > event[ids == uid_j][0]:\n",
        "                        permissible_pairs += 1\n",
        "                        if id_to_avg_prediction[uid_i] > id_to_avg_prediction[uid_j]:\n",
        "                            concordant_pairs += 1\n",
        "                else:  # At least one is right-censored\n",
        "                    if not right_censored_i:  # i is not right-censored but j is\n",
        "                        permissible_pairs += 1\n",
        "                        if id_to_avg_prediction[uid_i] > id_to_avg_prediction[uid_j]:\n",
        "                            concordant_pairs += 1\n",
        "                    elif not right_censored_j:  # j is not right-censored but i is\n",
        "                        permissible_pairs += 1\n",
        "                        if id_to_avg_prediction[uid_i] < id_to_avg_prediction[uid_j]:\n",
        "                            concordant_pairs += 1\n",
        "\n",
        "        c_index = concordant_pairs / permissible_pairs if permissible_pairs > 0 else 0\n",
        "        return 2 * c_index\n",
        "\n",
        "        \"\"\"Validate input data('X') to ensure it's in the correct format and meets the necessary conditions for processing.\"\"\"\n",
        "    def _validate_data(self, X, accept_sparse=False, ensure_min_samples=1):\n",
        "        \"\"\"Validate input data('X') to ensure it's in the correct format and meets the necessary conditions for processing.\"\"\"\n",
        "        return check_array(X, accept_sparse=accept_sparse, ensure_min_samples=ensure_min_samples)\n",
        "\n",
        "    def _validate_X_predict(self, X):\n",
        "        \"\"\"Validate X whenever one tries to predict.\"\"\"\n",
        "        X = check_array(X)\n",
        "        if X.shape[1] != self.n_features_in_:\n",
        "            raise ValueError(\"Number of features of the model must match the input. Model n_features is {} and input n_features is {}.\"\n",
        "                             .format(self.n_features_in_, X.shape[1]))\n",
        "        return X\n",
        "\n",
        "    def _aggregate_predictions(self, all_predictions, ids):\n",
        "        \"\"\"Aggregate predictions for each unique ID.\"\"\"\n",
        "        # Initialize aggregated predictions dictionary\n",
        "        aggregated_predictions = {}\n",
        "\n",
        "        # Iterate over each unique ID\n",
        "        for uid in np.unique(ids):\n",
        "            uid_indices = np.where(ids == uid)[0]\n",
        "            uid_predictions = [all_predictions[i] for i in uid_indices]\n",
        "\n",
        "            # Aggregate unique times, n_at_risk, and n_events\n",
        "            unique_times = sorted(np.unique(np.concatenate([pred['unique_times'] for pred in uid_predictions])))\n",
        "            n_at_risk = np.zeros(len(unique_times))\n",
        "            n_events = np.zeros(len(unique_times))\n",
        "\n",
        "            for pred in uid_predictions:\n",
        "                for i, t in enumerate(unique_times):\n",
        "                    if t in pred['unique_times']:\n",
        "                        idx = pred['unique_times'].index(t)\n",
        "                        n_at_risk[i] += pred['n_at_risk'][idx]\n",
        "                        n_events[i] += pred['n_events'][idx]\n",
        "\n",
        "            # Calculate the mean function\n",
        "            aggregated_predictions[uid] = self._calculate_mean_function(unique_times, n_at_risk, n_events)\n",
        "\n",
        "        return aggregated_predictions\n",
        "\n",
        "    def _calculate_mean_function(self, unique_times, n_at_risk, n_events):\n",
        "        \"\"\"Calculate the mean function based on unique times, n_at_risk, and n_events.\"\"\"\n",
        "        mean_function_values = []\n",
        "        cumulative_hazard = 0\n",
        "        for i in range(len(unique_times)):\n",
        "            hazard = n_events[i] / n_at_risk[i]\n",
        "            cumulative_hazard += hazard\n",
        "            mean_function_values.append(cumulative_hazard)\n",
        "        return {\n",
        "            'unique_times': unique_times,\n",
        "            'mean_function': mean_function_values\n",
        "        }\n",
        "\n",
        "    def predict_mean_function(self, X, ids):\n",
        "        \"\"\"\n",
        "        Predict the Nelson-Aalen estimator of the mean function for given samples.\n",
        "        \"\"\"\n",
        "        X = self._validate_X_predict(X)\n",
        "\n",
        "        # Ensure ids is iterable\n",
        "        if not isinstance(ids, (list, np.ndarray)):\n",
        "            ids = [ids]\n",
        "\n",
        "        # Initialize all_predictions list\n",
        "        all_predictions = []\n",
        "\n",
        "        for tree in self.estimators_:\n",
        "            mean_predictions = tree.predict_mean_function(X, ids)\n",
        "            for idx, func in enumerate(mean_predictions):\n",
        "                predictions.append({\n",
        "                    'unique_times': list(range(len(func))),\n",
        "                    'mean_function': func,\n",
        "                    # For now, assuming n_at_risk and n_events to be ones, this might need to be adjusted based on actual logic\n",
        "                    'n_at_risk': [1] * len(func),\n",
        "                    'n_events': [1] * len(func)\n",
        "                })\n",
        "            all_predictions.extend(mean_predictions)\n",
        "\n",
        "        # Aggregate the predictions for each unique ID\n",
        "        aggregated_predictions = self._aggregate_predictions(all_predictions, ids)\n",
        "\n",
        "        return aggregated_predictions\n",
        "\n",
        "    def predict_rate_function(self, X, ids):\n",
        "        \"\"\"Predict the rate function using the derivative of the mean function.\"\"\"\n",
        "        mean_functions = self.predict_mean_function(X, ids)\n",
        "        rate_functions = {}\n",
        "\n",
        "        for uid, mean_function in mean_functions.items():\n",
        "            unique_times = mean_function['unique_times']\n",
        "            rates = np.diff(mean_function['mean_function']) / np.diff(unique_times)\n",
        "            rate_functions[uid] = {\n",
        "                'unique_times': unique_times[:-1],  # 마지막 시간 포인트는 제외합니다.\n",
        "                'rates': rates\n",
        "            }\n",
        "\n",
        "        return rate_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkkmrI9XKw-P"
      },
      "outputs": [],
      "source": [
        "x = data[['group','x1','gender']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly9w9VroGTOB"
      },
      "outputs": [],
      "source": [
        "rrf = RecurrentRandomForest(n_estimators=100, max_depth=3, min_ids_leaf=10, min_impurity_decrease=0.2, random_state=1190, oob_score=True, n_jobs=6)\n",
        "y = {\n",
        "    'id': ids,\n",
        "    'time_start': time_start,\n",
        "    'time_stop': time_stop,\n",
        "    'event': event\n",
        "}\n",
        "rrf.fit(x,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufi_AWHUG4ap"
      },
      "outputs": [],
      "source": [
        "rrf.predict_mean_function(X=x, ids=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOT5bxSqKw-P"
      },
      "outputs": [],
      "source": [
        "rrf.predict_mean_function(X=x, ids=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCOCSJ9PbJpf"
      },
      "outputs": [],
      "source": [
        "rrf.oob_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMSFu9jJedyB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "보류\n",
        "\"\"\"\n",
        "\n",
        "class PermutationImportance:\n",
        "    def __init__(self, model, n_repeats=30, random_state=None):\n",
        "        \"\"\"\n",
        "        Constructor of the class\n",
        "        'model': The trained model for which we want to compute feature importances\n",
        "        'n_repeats': Number of times to repeat the permutation for each feature to get a reliable estimate.\n",
        "        'random_state': Seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.n_repeats = n_repeats\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _compute_baseline_cindex(self, X, event, time_stop, ids):\n",
        "        \"\"\"\n",
        "        Computes the baseline C-index using the original (non-permuted) data.\n",
        "        The C-index is a metric to evaluate the model's predictions, especially for recurrent events.\n",
        "        This function first predicts the mean function using the model, averages the predictions, and then calculates the C-index.\n",
        "        \"\"\"\n",
        "        # Get predictions using the model\n",
        "        predictions = self.model.predict_mean_function(X)\n",
        "\n",
        "        # Take the mean of the predictions for each individual for the C-index computation\n",
        "        mean_predictions = np.mean(predictions, axis=1)\n",
        "\n",
        "        # Compute total number of events for each ID\n",
        "        unique_ids, total_events = np.unique(ids, return_counts=True)\n",
        "        total_events_dict = dict(zip(unique_ids, total_events))\n",
        "        total_events_arr = np.array([total_events_dict[id_] for id_ in ids])\n",
        "\n",
        "        return self._estimate_concordance_index_recurrent(time_stop, event, mean_predictions, ids, total_events_arr)\n",
        "\n",
        "    def _estimate_concordance_index_recurrent(self, time_stop, event, predictions, ids, total_events):\n",
        "        \"\"\"\n",
        "        A wrapper around the model's method to estimate the recurrent C-index.\n",
        "        It's used for convenience and to make the code more readable.\n",
        "        \"\"\"\n",
        "        return self.model._estimate_recurrent_concordance_index(predictions, time_stop, event, ids, total_events)\n",
        "\n",
        "    def compute_importance(self, X, event, time_stop, ids):\n",
        "        \"\"\"\n",
        "        The main function that computes the feature importances\n",
        "        For each feature:\n",
        "            It permutes (shuffles) the feature's values a number of times (specified by 'n_repeats')\n",
        "            For each permutation, it calculates the drop in C-index (compared to the baseline C-index) due to the permutation\n",
        "            The drop in performance (C-index) due to the permutation gives an indicartion of the feature's importance.\n",
        "        It returns the computed importances matrix where each row corresponds to a feature and each column to a permutation repitition\n",
        "        \"\"\"\n",
        "        baseline_cindex = self._compute_baseline_cindex(X, event, time_stop, ids)\n",
        "\n",
        "        rng = check_random_state(self.random_state)\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        importances = np.zeros((n_features, self.n_repeats))\n",
        "\n",
        "        unique_ids = np.unique(ids)\n",
        "\n",
        "        for feature in range(n_features):\n",
        "            for repeat in range(self.n_repeats):\n",
        "                X_permuted = X.copy()\n",
        "\n",
        "                permuted_ids = rng.permutation(unique_ids)  # ID를 섞습니다.\n",
        "\n",
        "                # ID에 따라 값을 변경합니다.\n",
        "                for orig_id, new_id in zip(unique_ids, permuted_ids):\n",
        "                    orig_idx = np.where(ids == orig_id)[0]\n",
        "                    new_idx = np.where(ids == new_id)[0]\n",
        "                    X_permuted[orig_idx, feature] = X[new_idx, feature]\n",
        "\n",
        "                # Calculate c-index for permuted X\n",
        "                permuted_cindex = self._compute_baseline_cindex(X_permuted, event, time_stop, ids)\n",
        "\n",
        "                # The importance is the drop in c-index\n",
        "                importances[feature, repeat] = baseline_cindex - permuted_cindex\n",
        "\n",
        "        return importances\n",
        "\n",
        "    def report_importance(self, X, event, time_stop, ids):\n",
        "        \"\"\"\n",
        "        Computes the feature importances and then calculates the mean and standard deviation of the importances for each feature across all the repeats.\n",
        "        The mean gives an average measure of the importance of each feature, while the standard deviation provides an estimate of the variability or uncertainty in the importance estimates.\n",
        "        It returns the mean and standard deviation of the feature importances.\n",
        "        \"\"\"\n",
        "        importances = self.compute_importance(X, event, time_stop, ids)\n",
        "\n",
        "        # Compute mean and std of importances\n",
        "        importance_mean = np.mean(importances, axis=1)\n",
        "        importance_std = np.std(importances, axis=1)\n",
        "\n",
        "        return importance_mean, importance_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9JHyEydNere"
      },
      "outputs": [],
      "source": [
        "permutation_importance = PermutationImportance(rrf, n_repeats=10, random_state=42)\n",
        "mean_importances, std_importances = permutation_importance.report_importance(x, event, time_stop, ids)\n",
        "\n",
        "print(\"Mean Importances:\", mean_importances)\n",
        "print(\"STD Importances:\", std_importances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIwh8hD-Kw-Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}