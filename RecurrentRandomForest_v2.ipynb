{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path=\"/Users/jeongsookim/Downloads\"\n",
    "data = pd.read_csv(f\"{path}/simuDat.csv\")\n",
    "\n",
    "ids = data['id'].values\n",
    "time_start = data['time_start'].values\n",
    "time_stop = data['time_stop'].values\n",
    "event = data['event'].values\n",
    "x = data[['group','x1','gender']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RisksetCounter:\n",
    "    def __init__(self, ids, time_start, time_stop, event):\n",
    "        self.ids = ids\n",
    "        self.time_start = time_start\n",
    "        self.time_stop = time_stop\n",
    "        self.event = event\n",
    "\n",
    "        self.all_unique_times = np.unique(time_stop)\n",
    "        self.n_unique_times = len(self.all_unique_times)\n",
    "\n",
    "        self.n_at_risk = np.zeros(self.n_unique_times, dtype=np.int64)\n",
    "        self.n_events = np.zeros(self.n_unique_times, dtype=np.int64)\n",
    "        self.set_data()\n",
    "\n",
    "        self.state_stack = []\n",
    "\n",
    "    def set_data(self):\n",
    "        unique_ids = set(self.ids)\n",
    "        for t_idx, t in enumerate(self.all_unique_times):\n",
    "            self.n_at_risk[t_idx] = sum([self.Y_i(id_, t_idx) for id_ in unique_ids])\n",
    "            self.n_events[t_idx] = sum([self.dN_bar_i(id_, t_idx) for id_ in unique_ids])\n",
    "\n",
    "    def Y_i(self, id_, t_idx):\n",
    "        if t_idx >= len(self.all_unique_times):\n",
    "            return 0\n",
    "        time_at_t_idx = self.all_unique_times[t_idx]\n",
    "        indices = (self.ids == id_) & (time_at_t_idx <= self.time_stop)\n",
    "        return np.any(indices)\n",
    "\n",
    "    def dN_bar_i(self, id_, t_idx):\n",
    "        if t_idx >= len(self.all_unique_times):\n",
    "            return 0\n",
    "        time_at_t_idx = self.all_unique_times[t_idx]\n",
    "        indices = (self.ids == id_) & (time_at_t_idx == self.time_stop) & (self.event == 1)\n",
    "        return np.any(indices)\n",
    "\n",
    "    def save_state(self):\n",
    "        self.state_stack.append((self.ids.copy(), self.time_start.copy(), self.time_stop.copy(), self.event.copy(), self.n_at_risk.copy(), self.n_events.copy()))\n",
    "\n",
    "    def load_state(self):\n",
    "        if self.state_stack:\n",
    "            self.ids, self.time_start, self.time_stop, self.event, self.n_at_risk, self.n_events = self.state_stack.pop()\n",
    "\n",
    "    def update(self, new_ids, new_time_start, new_time_stop, new_event): \n",
    "        # Save the current state\n",
    "        self.save_state()    \n",
    "\n",
    "        # Compute the intersection of data\n",
    "        mask = np.isin(self.ids, new_ids)\n",
    "    \n",
    "        # Extract data of the intersection\n",
    "        updated_ids = self.ids[mask]\n",
    "        updated_time_start = self.time_start[mask]\n",
    "        updated_time_stop = self.time_stop[mask]\n",
    "        updated_event = self.event[mask]\n",
    "\n",
    "        # Update object variables based on the intersection data\n",
    "        self.ids = updated_ids\n",
    "        self.time_start = updated_time_start\n",
    "        self.time_stop = updated_time_stop\n",
    "        self.event = updated_event\n",
    "\n",
    "        # Recalculate unique times based on the updated data\n",
    "        self.all_unique_times = np.unique(np.concatenate([self.time_start, self.time_stop]))\n",
    "        self.n_unique_times = len(self.all_unique_times)\n",
    "    \n",
    "        # Resize the n_at_risk and n_events arrays based on the updated unique times\n",
    "        self.n_at_risk = np.zeros(self.n_unique_times, dtype=np.int64)\n",
    "        self.n_events = np.zeros(self.n_unique_times, dtype=np.int64)\n",
    "\n",
    "        # Update the n_at_risk and n_events arrays\n",
    "        unique_ids = set(self.ids)  # Extract unique IDs to avoid redundant calculations\n",
    "        for t_idx, t in enumerate(self.all_unique_times):\n",
    "            self.n_at_risk[t_idx] = sum([self.Y_i(id_, t_idx) for id_ in unique_ids])\n",
    "            self.n_events[t_idx] = sum([self.dN_bar_i(id_, t_idx) for id_ in unique_ids])\n",
    "\n",
    "    def reset(self):\n",
    "        self.load_state()\n",
    "\n",
    "    def copy(self):\n",
    "        return RisksetCounter(self.ids.copy(), self.time_start.copy(), self.time_stop.copy(), self.event.copy())\n",
    "\n",
    "    def __reduce__(self):\n",
    "        return (self.__class__, (self.ids, self.time_start, self.time_stop, self.event))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argbinsearch(arr, key_val):\n",
    "    arr_len = len(arr)\n",
    "    min_idx = 0\n",
    "    max_idx = arr_len\n",
    "\n",
    "    while min_idx < max_idx:\n",
    "        mid_idx = min_idx + ((max_idx - min_idx) // 2)\n",
    "\n",
    "        if mid_idx < 0 or mid_idx >= arr_len:\n",
    "            return -1\n",
    "\n",
    "        mid_val = arr[mid_idx]\n",
    "        if mid_val <= key_val:  # Change the condition to <=\n",
    "            min_idx = mid_idx + 1\n",
    "        else:\n",
    "            max_idx = mid_idx\n",
    "\n",
    "    return min_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoScoreCriterion:\n",
    "    def __init__(self, n_outputs, n_samples, unique_times, x, ids, time_start, time_stop, event):\n",
    "        \"\"\"ㅊ\n",
    "        Constructor of the class\n",
    "        Initialize instance variables using the provided input parameters\n",
    "        Objects 'riskset_left', 'riskset_right', and 'riskset_total' are initialized using the 'RisksetCounter' class\n",
    "        \"\"\"\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_samples = n_samples\n",
    "        self.unique_times = unique_times\n",
    "        self.x = x\n",
    "        self.ids = ids\n",
    "        self.time_start = time_start\n",
    "        self.time_stop = time_stop\n",
    "        self.event = event\n",
    "\n",
    "        self.unique_ids = set(self.ids)  # Store unique ids for later use\n",
    "        self.unique_times = unique_times\n",
    "\n",
    "        self.riskset_left = RisksetCounter(ids, time_start, time_stop, event)\n",
    "        self.riskset_right = RisksetCounter(ids, time_start, time_stop, event)\n",
    "        self.riskset_total = RisksetCounter(ids, time_start, time_stop, event)\n",
    "\n",
    "        self.samples_time_idx = np.searchsorted(unique_times, time_stop)\n",
    "\n",
    "        self.split_pos = 0\n",
    "        self.split_time_idx = 0\n",
    "\n",
    "        self._riskset_counter = RisksetCounter(ids, time_start, time_stop, event)  # 새로 추가\n",
    "\n",
    "    def init(self, y, sample_weight, n_samples, samples, start, end):\n",
    "        \"\"\"\n",
    "        Initialization function\n",
    "        Reset the risk set counters ('riskset_left','riskset_right','riskset_total') and updates 'riskset_total' with new data\n",
    "        \"\"\"\n",
    "        self.samples = samples\n",
    "        self.riskset_left.reset()\n",
    "        self.riskset_right.reset()\n",
    "        self.riskset_total.reset()\n",
    "\n",
    "        time_starts, stop_times, events = y[:, 0], y[:, 1], y[:, 2]\n",
    "        ids_for_update = [self.ids[idx] for idx in samples[start:end]]\n",
    "        time_starts_for_update = [time_starts[idx] for idx in samples[start:end]]\n",
    "        stop_times_for_update = [stop_times[idx] for idx in samples[start:end]]\n",
    "        events_for_update = [events[idx] for idx in samples[start:end]]\n",
    "\n",
    "        # Combine unique times from both datasets\n",
    "        self.unique_times = np.unique(np.concatenate([self.unique_times, stop_times_for_update]))\n",
    "\n",
    "        self.riskset_total.update(ids_for_update, time_starts_for_update, stop_times_for_update, events_for_update)\n",
    "\n",
    "    def set_unique_times(self, unique_times):\n",
    "        \"\"\"Sets the unique times for the current node.\"\"\"\n",
    "        self.unique_times = unique_times\n",
    "\n",
    "## Group Indicator만으로 나누기...\n",
    "\n",
    "    # Functions returning the risk set value and event value for the given ID and time index from the respective risk set (left or right)\n",
    "    def Y_left_value(self, id_, t):\n",
    "        return self.riskset_left.Y_i(id_, t)\n",
    "    \n",
    "    def Y_right_value(self, id_, t):\n",
    "        return self.riskset_right.Y_i(id_, t)\n",
    "\n",
    "    def dN_bar_left_value(self, id_, t):\n",
    "        return self.riskset_left.dN_bar_i(id_, t)\n",
    "\n",
    "    def dN_bar_right_value(self, id_, t):\n",
    "        return self.riskset_right.dN_bar_i(id_, t)\n",
    "\n",
    "    def temporary_update_riskset(self, riskset_counter, ids, time_start, time_stop, event):\n",
    "        # Combine and find unique stop times from both nodes\n",
    "        combined_time_stops = np.concatenate([self.riskset_left.time_stop, self.riskset_right.time_stop])\n",
    "        unique_time_stops = np.unique(combined_time_stops)\n",
    "\n",
    "        riskset_counter.all_unique_times = unique_time_stops\n",
    "\n",
    "        # Resize the n_at_risk and n_events arrays based on the updated unique times\n",
    "        riskset_counter.n_at_risk = np.zeros(len(unique_time_stops), dtype=np.int64)\n",
    "        riskset_counter.n_events = np.zeros(len(unique_time_stops), dtype=np.int64)\n",
    "\n",
    "        # Update the n_at_risk and n_events arrays\n",
    "        unique_ids = set(ids)  # Extract unique IDs to avoid redundant calculations\n",
    "        for t_idx, t in enumerate(unique_time_stops):\n",
    "            riskset_counter.n_at_risk[t_idx] = sum([riskset_counter.Y_i(id_, t_idx) for id_ in unique_ids])\n",
    "            riskset_counter.n_events[t_idx] = sum([riskset_counter.dN_bar_i(id_, t_idx) for id_ in unique_ids])\n",
    "\n",
    "    def calculate_numerator(self):\n",
    "        # Temporary update riskset\n",
    "        self.temporary_update_riskset(self.riskset_left, self.riskset_left.ids, self.riskset_left.time_start, self.riskset_left.time_stop, self.riskset_left.event)\n",
    "        self.temporary_update_riskset(self.riskset_right, self.riskset_right.ids, self.riskset_right.time_start, self.riskset_right.time_stop, self.riskset_right.event)\n",
    "    \n",
    "        w = (self.riskset_left.n_at_risk * self.riskset_right.n_at_risk) / (self.riskset_left.n_at_risk + self.riskset_right.n_at_risk)\n",
    "        term = (self.riskset_left.n_events / self.riskset_left.n_at_risk) - (self.riskset_right.n_events / self.riskset_right.n_at_risk)\n",
    "    \n",
    "        return np.sum(w * term)\n",
    "\n",
    "    def calculate_variance_estimate(self):\n",
    "        \"\"\"\n",
    "        Update the variance estimate to be compatible with the provided function.\n",
    "        \"\"\"\n",
    "    \n",
    "        def var_comp(riskset, id_, uniTimeVec, w_const, max_w_const):\n",
    "            \"\"\"\n",
    "            Compute the variance component for each observation, \n",
    "            similar to the var_comp function in the mcfDiff.test R code.\n",
    "            \"\"\"\n",
    "            y_i_tj = np.array([riskset.Y_i(id_, t_idx) for t_idx in range(len(uniTimeVec))])\n",
    "            yVec = riskset.n_at_risk\n",
    "            n_i_tj = np.array([riskset.dN_bar_i(id_, t_idx) for t_idx in range(len(uniTimeVec))])\n",
    "            dLambda = riskset.n_events / (riskset.n_at_risk + 1e-7)  # Avoid division by zero\n",
    "\n",
    "            res_ij = np.where(yVec > 0, y_i_tj / yVec * (n_i_tj - dLambda), 0)\n",
    "\n",
    "            max_res_ij = np.max(np.abs(res_ij))\n",
    "    \n",
    "            if max_res_ij > 0:\n",
    "                re_res_ij = res_ij / max_res_ij\n",
    "                reFactor = np.exp(np.log(max_res_ij) + np.log(max_w_const))\n",
    "            else:\n",
    "                re_res_ij = 0\n",
    "                reFactor = 1\n",
    "    \n",
    "            res_const = (w_const / max_w_const) * re_res_ij\n",
    "\n",
    "            return (np.sum(res_const) * reFactor) ** 2\n",
    "\n",
    "        # Temporary update riskset\n",
    "        self.temporary_update_riskset(self.riskset_left, self.riskset_left.ids, self.riskset_left.time_start, self.riskset_left.time_stop, self.riskset_left.event)\n",
    "        self.temporary_update_riskset(self.riskset_right, self.riskset_right.ids, self.riskset_right.time_start, self.riskset_right.time_stop, self.riskset_right.event)\n",
    "\n",
    "        # Extract required variables\n",
    "        uniTimeVec = self.riskset_total.all_unique_times\n",
    "        w_const = (self.riskset_left.n_at_risk * self.riskset_right.n_at_risk) / (self.riskset_left.n_at_risk + self.riskset_right.n_at_risk)\n",
    "        max_w_const = np.max(w_const)\n",
    "\n",
    "        # Calculate variance components for each ID in the left and right nodes\n",
    "        varList1 = [var_comp(self.riskset_left, id_, uniTimeVec, w_const, max_w_const) \n",
    "                    for id_ in np.unique(self.riskset_left.ids)]\n",
    "\n",
    "        varList2 = [var_comp(self.riskset_right, id_, uniTimeVec, w_const, max_w_const) \n",
    "                    for id_ in np.unique(self.riskset_right.ids)]\n",
    "    \n",
    "        # Sum the variance components\n",
    "        varU_1 = np.sum(varList1)\n",
    "        varU_2 = np.sum(varList2)\n",
    "    \n",
    "        return varU_1 + varU_2\n",
    "\n",
    "    \n",
    "    def calculate_denominator(self):\n",
    "        return self.calculate_variance_estimate()\n",
    "\n",
    "    def proxy_impurity_improvement(self):\n",
    "        if len(self.riskset_left.n_at_risk) == 0 or len(self.riskset_right.n_at_risk) == 0:\n",
    "            return -np.inf\n",
    "\n",
    "        numer = self.calculate_numerator() ** 2\n",
    "        denom = self.calculate_denominator()\n",
    "\n",
    "        return numer / (denom + 1e-7)\n",
    "    \n",
    "    def update_riskset(self, ids_subset):\n",
    "        # Update the riskset based on the subset of IDs at the current node\n",
    "        unique_ids_subset = np.unique(ids_subset)\n",
    "        self.riskset_counter.update(unique_ids_subset, self.time_start, self.time_stop, self.event)\n",
    "\n",
    "    def node_value(self):\n",
    "        \"\"\"\n",
    "        Returns the Nelson-Aalen estimator of the mean function μ(t) for the entities in the current node.\n",
    "        \"\"\"\n",
    "        return self.node_value_from_riskset(self.riskset_total)\n",
    "\n",
    "    def node_value_from_riskset(self, riskset_counter):\n",
    "        \"\"\"\n",
    "        Returns the Nelson-Aalen estimator of the mean function μ(t) for the entities based on provided riskset_counter.\n",
    "        \"\"\"\n",
    "        mu_hat_values = []\n",
    "    \n",
    "        # Initialize the cumulative sum of the Nelson-Aalen estimator\n",
    "        cumsum_Nelson_Aalen = 0\n",
    "    \n",
    "        for t_idx, t in enumerate(self.unique_times):\n",
    "            # Use n_at_risk and n_events from the riskset_counter\n",
    "            n_at_risk_t = riskset_counter.n_at_risk[t_idx] if t_idx < len(riskset_counter.n_at_risk) else 0\n",
    "            n_events_t = riskset_counter.n_events[t_idx] if t_idx < len(riskset_counter.n_events) else 0\n",
    "            \n",
    "            cumsum_Nelson_Aalen += n_events_t / (n_at_risk_t + 1e-7)  # Avoiding division by zero\n",
    "            mu_hat_values.append(cumsum_Nelson_Aalen)\n",
    "        \n",
    "        return mu_hat_values\n",
    "\n",
    "    # RisksetCounter의 상태를 저장하고 복원하기 위한 메서드를 추가합니다.\n",
    "    def save_riskset_state(self):\n",
    "        self._riskset_counter.save_state()\n",
    "\n",
    "    def reset_riskset_state(self):\n",
    "        self._riskset_counter.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Functions to reset all risk set counters\n",
    "        \"\"\"\n",
    "        self.riskset_total.reset()\n",
    "        self.riskset_left.reset()\n",
    "        self.riskset_right.reset()\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"\n",
    "        Creates and returns a copy of the current object.\n",
    "        \"\"\"\n",
    "        new_criterion = PseudoScoreCriterion(self.n_outputs, self.n_samples, self.unique_times,\n",
    "                                             self.x, self.ids, self.time_start, self.time_stop, \n",
    "                                             self.event)\n",
    "        new_criterion.riskset_left = self.riskset_left.copy()\n",
    "        new_criterion.riskset_right = self.riskset_right.copy()\n",
    "        new_criterion.riskset_total = self.riskset_total.copy()\n",
    "        new_criterion.samples_time_idx = self.samples_time_idx.copy()\n",
    "        if hasattr(self, 'samples'):\n",
    "            new_criterion.samples = self.samples.copy()\n",
    "\n",
    "        return new_criterion\n",
    "\n",
    "def update_with_group_indicator(self, feature_index, group_indicator):\n",
    "    \"\"\"\n",
    "    Update the criterion based on a specified feature and group indicator. \n",
    "    This will split the data into left and right nodes based on the provided feature and group indicator.\n",
    "    \"\"\"\n",
    "    # Reset the riskset counters for the left and right nodes\n",
    "    self.riskset_left.reset()\n",
    "    self.riskset_right.reset()\n",
    "\n",
    "    # Determine the split by the feature and group indicator\n",
    "    left_mask = self.x[:, feature_index] <= group_indicator  # Changed to <= for continuous features\n",
    "    right_mask = ~left_mask\n",
    "\n",
    "    # Create empty lists to store the ids, start times, stop times, and events for both left and right splits\n",
    "    ids_left, start_left, stop_left, event_left = [], [], [], []\n",
    "    ids_right, start_right, stop_right, event_right = [], [], [], []\n",
    "\n",
    "    # For each unique ID, decide whether to assign it to the left or right node based on the mask\n",
    "    for id_ in self.unique_ids:\n",
    "        id_indices = np.where(self.ids == id_)[0]  # Get all indices for this ID\n",
    "        if left_mask[id_indices[0]]:\n",
    "            ids_left.extend([self.ids[i] for i in id_indices])\n",
    "            start_left.extend([self.time_start[i] for i in id_indices])\n",
    "            stop_left.extend([self.time_stop[i] for i in id_indices])\n",
    "            event_left.extend([self.event[i] for i in id_indices])\n",
    "        else:\n",
    "            ids_right.extend([self.ids[i] for i in id_indices])\n",
    "            start_right.extend([self.time_start[i] for i in id_indices])\n",
    "            stop_right.extend([self.time_stop[i] for i in id_indices])\n",
    "            event_right.extend([self.event[i] for i in id_indices])\n",
    "\n",
    "    # Set the all_unique_times for the risk sets of left and right nodes to the current node's unique times\n",
    "    self.riskset_left.all_unique_times = self.unique_times\n",
    "    self.riskset_right.all_unique_times = self.unique_times\n",
    "\n",
    "    # Also, adjust the lengths of n_at_risk and n_events in both riskset_left and riskset_right to match unique_times\n",
    "    self.riskset_left.n_at_risk = np.zeros(len(self.unique_times), dtype=np.int64)\n",
    "    self.riskset_left.n_events = np.zeros(len(self.unique_times), dtype=np.int64)\n",
    "    self.riskset_right.n_at_risk = np.zeros(len(self.unique_times), dtype=np.int64)\n",
    "    self.riskset_right.n_events = np.zeros(len(self.unique_times), dtype=np.int64)\n",
    "\n",
    "    # Update the risk sets for the left and right nodes\n",
    "    self.riskset_left.update(ids_left, start_left, stop_left, event_left)\n",
    "    self.riskset_right.update(ids_right, start_right, stop_right, event_right)\n",
    "\n",
    "# 이 함수를 PseudoScoreCriterion 클래스에 추가합니다.\n",
    "setattr(PseudoScoreCriterion, 'update', update_with_group_indicator)\n",
    "\n",
    "# 추가로, left node와 right node의 데이터를 반환하는 메소드를 추가합니다.\n",
    "def get_left_node_data(self):\n",
    "    return self.riskset_left.ids, self.riskset_left.n_at_risk, self.riskset_left.n_events\n",
    "\n",
    "def get_right_node_data(self):\n",
    "    return self.riskset_right.ids, self.riskset_right.n_at_risk, self.riskset_right.n_events\n",
    "\n",
    "setattr(PseudoScoreCriterion, 'get_left_node_data', get_left_node_data)\n",
    "setattr(PseudoScoreCriterion, 'get_right_node_data', get_right_node_data)\n",
    "\n",
    "def calculate_node_value_updated(self, side=\"left\"):\n",
    "    \"\"\"\n",
    "    Calculate the node value based on the updated RisksetCounter using get_left_node_data and get_right_node_data.\n",
    "    \n",
    "    Parameters:\n",
    "        - side (str): Either \"left\" or \"right\" to determine which riskset to use for calculation.\n",
    "    \"\"\"\n",
    "    if side == \"left\":\n",
    "        ids, n_at_risk, n_events = self.get_left_node_data()\n",
    "    elif side == \"right\":\n",
    "        ids, n_at_risk, n_events = self.get_right_node_data()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid side value. Expected 'left' or 'right'.\")\n",
    "    \n",
    "    mask = np.isin(self.ids, ids)\n",
    "    \n",
    "    time_start_filtered = self.time_start[mask]\n",
    "    time_stop_filtered = self.time_stop[mask]\n",
    "    event_filtered = self.event[mask]\n",
    "    \n",
    "    riskset_temp = RisksetCounter(ids, time_start_filtered, time_stop_filtered, event_filtered)\n",
    "    riskset_temp.n_at_risk = n_at_risk\n",
    "    riskset_temp.n_events = n_events\n",
    "\n",
    "    return self.node_value_from_riskset(riskset_temp)\n",
    "\n",
    "# PseudoScoreCriterion 클래스에 위에서 정의한 함수를 추가합니다.\n",
    "setattr(PseudoScoreCriterion, 'calculate_node_value', calculate_node_value_updated)\n",
    "\n",
    "\n",
    "\n",
    "PseudoScoreCriterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "class PseudoScoreTreeBuilder:\n",
    "    \"\"\"\n",
    "    Class designed to build a decision tree based on the pseudo-score test statistics criterion,\n",
    "    typically used in recurrent events data analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    TREE_UNDEFINED = -1  # Placeholder\n",
    "\n",
    "    def __init__(self, max_depth=None, min_ids_split=2, min_ids_leaf=1,\n",
    "                 max_features=None, max_thresholds=None, min_impurity_decrease=0,\n",
    "                 random_state=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_ids_split = min_ids_split\n",
    "        self.min_ids_leaf = min_ids_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_thresholds = max_thresholds\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.random_state = check_random_state(random_state)\n",
    "\n",
    "    def split_indices(self, X_column, threshold, criterion, start, end):\n",
    "        \"\"\"Efficiently splits the data based on the given threshold for a specific feature column.\"\"\"\n",
    "        left_indices = np.where(X_column <= threshold)[0]\n",
    "        right_indices = np.where(X_column > threshold)[0]\n",
    "\n",
    "        # Convert local indices to global indices\n",
    "        left_indices = np.arange(start, end)[left_indices]\n",
    "        right_indices = np.arange(start, end)[right_indices]\n",
    "\n",
    "        return left_indices, right_indices\n",
    "\n",
    "    def _split(self, X, criterion, start, end):\n",
    "        best_split = {\n",
    "            'feature_index': None,\n",
    "            'threshold': None,\n",
    "            'improvement': -np.inf\n",
    "        }\n",
    "    \n",
    "        # 가능한 스플릿 후보들을 저장하기 위한 리스트\n",
    "        potential_splits = []\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            unique_thresholds = np.unique(X[start:end, feature_index])\n",
    "            if len(unique_thresholds) <= 1:\n",
    "                continue\n",
    "\n",
    "            if self.max_thresholds and len(unique_thresholds) > self.max_thresholds:\n",
    "                unique_thresholds = self.random_state.choice(unique_thresholds, self.max_thresholds, replace=False)\n",
    "\n",
    "            for threshold in unique_thresholds:\n",
    "                criterion.update(feature_index, threshold)\n",
    "                improvement = criterion.proxy_impurity_improvement()\n",
    "\n",
    "                left_indices, right_indices = self.split_indices(X[start:end, feature_index], threshold, criterion, start, end)\n",
    "\n",
    "                # Ensure that both child nodes will have at least min_ids_leaf samples\n",
    "                if len(left_indices) < self.min_ids_leaf or len(right_indices) < self.min_ids_leaf:\n",
    "                    continue\n",
    "\n",
    "                # self.min_impurity_decrease보다 큰 모든 스플릿 후보들을 저장\n",
    "                if improvement > self.min_impurity_decrease:\n",
    "                    potential_splits.append({\n",
    "                        'feature_index': feature_index,\n",
    "                        'threshold': threshold,\n",
    "                        'improvement': improvement\n",
    "                    })\n",
    "\n",
    "                if improvement > best_split['improvement']:\n",
    "                    best_split = {\n",
    "                        'feature_index': feature_index,\n",
    "                        'threshold': threshold,\n",
    "                        'improvement': improvement\n",
    "                    }\n",
    "\n",
    "        return best_split, potential_splits\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _build(self, X, y, criterion, depth=0, start=0, end=None):\n",
    "        if end is None:\n",
    "            end = X.shape[0]\n",
    "\n",
    "        ids = y[start:end, 0]\n",
    "        unique_ids = np.unique(ids)\n",
    "\n",
    "        riskset_counter = RisksetCounter(ids, y[start:end, 1], y[start:end, 2], y[start:end, 3])\n",
    "        node_value = criterion.node_value_from_riskset(riskset_counter)\n",
    "        node_unique_times = riskset_counter.all_unique_times.tolist()\n",
    "        node_value = node_value[:len(node_unique_times)]\n",
    "\n",
    "        # We need to access n_at_risk and n_events as attributes, not as methods\n",
    "        node_n_at_risk = riskset_counter.n_at_risk\n",
    "        node_n_events = riskset_counter.n_events\n",
    "\n",
    "        # Check depth and minimum ids required for split\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return {\n",
    "                'feature': None,\n",
    "                'threshold': None,\n",
    "                'left_child': None,\n",
    "                'right_child': None,\n",
    "                'node_value': node_value,\n",
    "                'unique_times': node_unique_times,\n",
    "                'n_at_risk': node_n_at_risk.tolist(),  # Store as list for consistency\n",
    "                'n_events': node_n_events.tolist(),    # Store as list for consistency\n",
    "                'ids': unique_ids.tolist()\n",
    "            }\n",
    "\n",
    "        if len(unique_ids) < self.min_ids_split:\n",
    "            return {\n",
    "                'feature': None,\n",
    "                'threshold': None,\n",
    "                'left_child': None,\n",
    "                'right_child': None,\n",
    "                'node_value': node_value,\n",
    "                'unique_times': node_unique_times,\n",
    "                'n_at_risk': node_n_at_risk.tolist(),  # Store as list for consistency\n",
    "                'n_events': node_n_events.tolist(),    # Store as list for consistency\n",
    "                'ids': unique_ids.tolist()\n",
    "            }\n",
    "\n",
    "        best_split, potential_splits = self._split(X, criterion, start, end)\n",
    "\n",
    "        for split in [best_split] + potential_splits:\n",
    "            if split['threshold'] is None:\n",
    "                continue\n",
    "\n",
    "            left_indices, right_indices = self.split_indices(X[start:end, split['feature_index']], split['threshold'], criterion, start, end)\n",
    "\n",
    "            # Check if there are enough unique ids in both left and right children after the split\n",
    "            if len(np.unique(ids[left_indices])) >= self.min_ids_leaf and len(np.unique(ids[right_indices])) >= self.min_ids_leaf:\n",
    "                best_split = split\n",
    "                break\n",
    "        else:  # No valid split found\n",
    "            return {\n",
    "                'feature': None,\n",
    "                'threshold': None,\n",
    "                'left_child': None,\n",
    "                'right_child': None,\n",
    "                'node_value': node_value,\n",
    "                'unique_times': node_unique_times,\n",
    "                'n_at_risk': node_n_at_risk.tolist(),  # Store as list for consistency\n",
    "                'n_events': node_n_events.tolist(),    # Store as list for consistency\n",
    "                'ids': unique_ids.tolist()\n",
    "            }\n",
    "\n",
    "        left_child = self._build(X[left_indices], y[left_indices], criterion, depth=depth+1)\n",
    "        right_child = self._build(X[right_indices], y[right_indices], criterion, depth=depth+1)\n",
    "\n",
    "        return {\n",
    "            'feature': best_split['feature_index'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left_child': left_child,\n",
    "            'right_child': right_child,\n",
    "            'node_value': node_value,\n",
    "            'unique_times': node_unique_times,\n",
    "            'n_at_risk': node_n_at_risk.tolist(),  # Store as list for consistency\n",
    "            'n_events': node_n_events.tolist(),    # Store as list for consistency\n",
    "            'ids': unique_ids.tolist()\n",
    "        }\n",
    "\n",
    "    def build(self, X, ids, time_start, time_stop, event):\n",
    "        \"\"\"\n",
    "        The main method to invoke the tree building process.\n",
    "        Initializes the pseudo-score criterion using the input data and constructs the tree using the _build method.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        y = np.c_[ids, time_start, time_stop, event]\n",
    "\n",
    "        unique_times = np.unique(np.concatenate([time_start, time_stop]))\n",
    "        criterion = PseudoScoreCriterion(n_outputs=1, n_samples=n_samples,\n",
    "                                         unique_times=unique_times, x=X, ids=ids,\n",
    "                                         time_start=time_start, time_stop=time_stop, event=event)\n",
    "\n",
    "        tree = self._build(X, y, criterion)\n",
    "        return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class RecurrentTree(BaseEstimator):\n",
    "    def __init__(self, max_depth=None, min_ids_split=2, min_ids_leaf=1, \n",
    "                 max_features=None, max_thresholds=None, min_impurity_decrease=0,\n",
    "                 random_state=None):\n",
    "        \"\"\"\n",
    "        Constructor of the class\n",
    "        Initializes the tree's hyperparameters and settings\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_ids_split = min_ids_split\n",
    "        self.min_ids_leaf = min_ids_leaf\n",
    "        self.max_features = max_features\n",
    "        self.max_thresholds = max_thresholds\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.random_state = random_state\n",
    "        self.tree_ = None\n",
    "\n",
    "    def fit(self, X, ids, time_start, time_stop, event):\n",
    "        \"\"\"\n",
    "        Trains the recurrent tree using the input data\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        ids = np.array(ids)\n",
    "        time_start = np.array(time_start)\n",
    "        time_stop = np.array(time_stop)\n",
    "        event = np.array(event)\n",
    "\n",
    "        # Use the PseudoScoreTreeBuilder to build the tree\n",
    "        builder = PseudoScoreTreeBuilder(\n",
    "            max_depth=self.max_depth,\n",
    "            min_ids_split=self.min_ids_split,\n",
    "            min_ids_leaf=self.min_ids_leaf,\n",
    "            max_features=self.max_features,\n",
    "            max_thresholds=self.max_thresholds,\n",
    "            min_impurity_decrease=self.min_impurity_decrease,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        self.tree_ = builder.build(X, ids, time_start, time_stop, event)\n",
    "        return self\n",
    "\n",
    "    def get_tree(self):\n",
    "        \"\"\"Return the tree as a dictionary.\"\"\"\n",
    "        return self.tree_\n",
    "\n",
    "    def traverse_tree_for_id(self, X_id_samples, node):\n",
    "        \"\"\"\n",
    "        Traverse the tree for a specific ID based on its samples.\n",
    "\n",
    "        Args:\n",
    "        - X_id_samples (list of arrays): The samples corresponding to a specific ID.\n",
    "        - node (dict): The current node being evaluated in the tree.\n",
    "\n",
    "        Returns:\n",
    "        - node (dict): The terminal node for the specific ID.\n",
    "        \"\"\"\n",
    "        if node[\"feature\"] is None:  # Terminal node\n",
    "            return node\n",
    "\n",
    "        # Traverse the tree for each sample and collect the terminal nodes\n",
    "        terminal_nodes = []\n",
    "        for sample in X_id_samples:\n",
    "            if sample[node[\"feature\"]] <= node[\"threshold\"]:\n",
    "                terminal_nodes.append(self.traverse_tree_for_id([sample], node[\"left_child\"]))\n",
    "            else:\n",
    "                terminal_nodes.append(self.traverse_tree_for_id([sample], node[\"right_child\"]))\n",
    "\n",
    "        # Check if all samples lead to the same terminal node\n",
    "        first_terminal = terminal_nodes[0]\n",
    "        if all(node == first_terminal for node in terminal_nodes):\n",
    "            return first_terminal\n",
    "\n",
    "        # If samples lead to different terminal nodes, it's ambiguous. For simplicity, return the first terminal node.\n",
    "        # In a real-world scenario, this might need more sophisticated handling.\n",
    "        return first_terminal\n",
    "\n",
    "    def predict_mean_function(self, X, ids):\n",
    "        \"\"\"\n",
    "        Predict the node_value, unique_times, n_at_risk, and n_events of the terminal node for given samples.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure X is a list of samples\n",
    "        X = np.array(X)\n",
    "\n",
    "        # If ids is a scalar, convert it to a list\n",
    "        if np.isscalar(ids):\n",
    "            ids = [ids]\n",
    "\n",
    "        mean_function_predictions = {}\n",
    "\n",
    "        for sample_id in ids:\n",
    "            samples_for_id = [X[i] for i, uid in enumerate(ids) if uid == sample_id]\n",
    "            terminal_node_for_id = self.traverse_tree_for_id(samples_for_id, self.tree_)\n",
    "\n",
    "            # Get node_value, unique_times, n_at_risk, and n_events from the terminal node\n",
    "            node_value = terminal_node_for_id.get(\"node_value\",[])\n",
    "            unique_times = terminal_node_for_id.get(\"unique_times\", [])\n",
    "            n_at_risk = terminal_node_for_id.get(\"n_at_risk\", [])\n",
    "            n_events = terminal_node_for_id.get(\"n_events\", [])\n",
    "\n",
    "            # Convert lists to dictionaries for easier access\n",
    "            n_at_risk = {unique_times[i]: n_at_risk[i] for i in range(len(unique_times))}\n",
    "            n_events = {unique_times[i]: n_events[i] for i in range(len(unique_times))}\n",
    "\n",
    "            mean_function_predictions[sample_id] = {\n",
    "                \"mean_function\": node_value,\n",
    "                \"unique_times\": unique_times,\n",
    "                \"n_at_risk\": n_at_risk,\n",
    "                \"n_events\": n_events\n",
    "            }\n",
    "\n",
    "        return mean_function_predictions, unique_times, n_at_risk, n_events\n",
    "\n",
    "\n",
    "    def predict_rate_function(self, X, ids):\n",
    "        \"\"\"\n",
    "        Predict the rate function as the difference between unique time points for the terminal node.\n",
    "        \"\"\"\n",
    "        \n",
    "        if np.isscalar(ids):\n",
    "            ids = [ids]\n",
    "            \n",
    "        mean_function_predictions = self.predict_mean_function(X, ids)\n",
    "    \n",
    "        rate_function_predictions = {}\n",
    "        for sample_id in ids:\n",
    "            mean_function_values = mean_function_predictions[sample_id]\n",
    "\n",
    "            # Calculate rate function as the difference between consecutive mean function values\n",
    "            rate_function = np.diff(mean_function_values, prepend=mean_function_values[0])\n",
    "\n",
    "            rate_function_predictions[sample_id] = rate_function\n",
    "\n",
    "        return rate_function_predictions\n",
    "    \n",
    "    def _map_terminal_nodes(self, node, current_id=[0]):\n",
    "        \"\"\"\n",
    "        Recursively traverse the tree and assign unique integers to each terminal node.\n",
    "        \"\"\"\n",
    "        if node[\"feature\"] is None:  # Terminal node\n",
    "            if \"id\" not in node:\n",
    "                node[\"id\"] = current_id[0]\n",
    "                current_id[0] += 1\n",
    "            return\n",
    "\n",
    "        self._map_terminal_nodes(node[\"left_child\"], current_id)\n",
    "        self._map_terminal_nodes(node[\"right_child\"], current_id)\n",
    "\n",
    "    def apply(self, X, ids=None):\n",
    "        \"\"\"Return the index of the leaf that each unique ID is predicted as.\"\"\"\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        if ids is None:\n",
    "            ids = np.array([i for i in range(X.shape[0])])\n",
    "        else:\n",
    "            ids = np.array(ids)\n",
    "\n",
    "        terminal_nodes = {}\n",
    "        self._map_terminal_nodes(self.tree_)  # Reset the mapping\n",
    "\n",
    "        for sample_id in np.unique(ids):\n",
    "            samples_for_id = [X[i] for i, uid in enumerate(ids) if uid == sample_id]\n",
    "            terminal_node_for_id = self.traverse_tree_for_id(samples_for_id, self.tree_)\n",
    "            terminal_nodes[sample_id] = terminal_node_for_id[\"id\"]\n",
    "\n",
    "        return terminal_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numbers import Integral, Real\n",
    "from sklearn.utils import check_random_state\n",
    "from numpy.random import RandomState\n",
    "\n",
    "def check_random_state(seed):\n",
    "    \"\"\"\n",
    "    Check if seed is a valid random state.\n",
    "    \"\"\"\n",
    "    if seed is None or isinstance(seed, int):\n",
    "        return np.random.default_rng(seed)\n",
    "    elif isinstance(seed, np.random.Generator):\n",
    "        return seed\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid seed: {seed}\")\n",
    "\n",
    "def _get_n_ids_bootstrap(n_ids, max_ids):\n",
    "    \"\"\"\n",
    "    Modified for recurrent events. Get the number of IDs in a bootstrap sample.\n",
    "    \"\"\"\n",
    "    if max_ids is None:\n",
    "        return n_ids\n",
    "\n",
    "    if isinstance(max_ids, Integral):\n",
    "        if max_ids > n_ids:\n",
    "            msg = \"`max_samples` must be <= n_ids={} but got value {}\"\n",
    "            raise ValueError(msg.format(n_ids, max_ids))\n",
    "        return max_ids\n",
    "\n",
    "    if isinstance(max_ids, Real):\n",
    "        return max(round(n_ids * max_ids), 1)\n",
    "\n",
    "def _generate_sampled_ids(random_state, unique_ids, max_ids):\n",
    "    \"\"\"\n",
    "    Generate bootstrap sample indices based on unique IDs.\n",
    "    \"\"\"\n",
    "    # Calculate the number of IDs to be sampled using the _get_n_ids_bootstrap function\n",
    "    n_ids_bootstrap = _get_n_ids_bootstrap(len(unique_ids), max_ids)\n",
    "\n",
    "    # Create a random instance with the given random_state\n",
    "    random_instance = check_random_state(random_state)\n",
    "\n",
    "    # Randomly select n_ids_bootstrap IDs from the unique_ids with replacement\n",
    "    sampled_ids_indices = random_instance.choice(len(unique_ids), n_ids_bootstrap, replace=True)\n",
    "    \n",
    "    # Get the actual IDs using the indices\n",
    "    sampled_ids = unique_ids[sampled_ids_indices]\n",
    "    \n",
    "    return sampled_ids\n",
    "\n",
    "def _generate_unsampled_ids(unique_ids, sampled_ids):\n",
    "    \"\"\"\n",
    "    Determine unsampled unique IDs from the entire set of IDs.\n",
    "    \"\"\"\n",
    "    # 중복 제거된 sampled_ids\n",
    "    unique_sampled_ids = np.unique(sampled_ids)\n",
    "    \n",
    "    # Find unsampled unique IDs\n",
    "    unsampled_unique_ids = np.setdiff1d(unique_ids, unique_sampled_ids)\n",
    "    return unsampled_unique_ids\n",
    "\n",
    "\n",
    "from warnings import catch_warnings, simplefilter\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "def _parallel_build_trees(tree, bootstrap, X, y, tree_idx, n_trees,\n",
    "                                    verbose=0, n_ids_bootstrap=None, random_state=None, max_ids=None):\n",
    "    \"\"\"\n",
    "    Private function used to fit a single tree in parallel with corrected y values extraction.\n",
    "    \"\"\"\n",
    "    if verbose > 1:\n",
    "        print(\"Building estimator %d of %d for this parallel run \"\n",
    "              \"(total %d)...\" % (tree_idx + 1, n_trees, n_trees))\n",
    "\n",
    "    ids = y['id']\n",
    "    time_start = y['time_start']\n",
    "    time_stop = y['time_stop']\n",
    "    event = y['event']\n",
    "\n",
    "    # If bootstrap is True, generate a bootstrap sample for training\n",
    "    if bootstrap:\n",
    "        unique_ids = np.unique(ids)\n",
    "        if isinstance(random_state, np.random.RandomState):\n",
    "            rnd = random_state\n",
    "        else:\n",
    "            rnd = np.random.RandomState(random_state)\n",
    "        \n",
    "        sampled_ids = _generate_sampled_ids(rnd, unique_ids, max_ids)\n",
    "        bootstrap_indices = np.where(np.isin(ids, sampled_ids))[0]\n",
    "        X_bootstrap = X[bootstrap_indices]\n",
    "        ids_bootstrap = ids[bootstrap_indices]\n",
    "        time_start_bootstrap = time_start[bootstrap_indices]\n",
    "        time_stop_bootstrap = time_stop[bootstrap_indices]\n",
    "        event_bootstrap = event[bootstrap_indices]\n",
    "        \n",
    "        tree.fit(X_bootstrap, ids_bootstrap, time_start_bootstrap, time_stop_bootstrap, event_bootstrap)\n",
    "    else:\n",
    "        tree.fit(X, ids, time_start, time_stop, event)\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_RE_nRE(data_dict):\n",
    "    \"\"\"\n",
    "    Generate RE and nRE matrices from the given data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dict: A dictionary containing 'id', 'time_start', 'time_stop', and 'event' keys.\n",
    "    \n",
    "    Returns:\n",
    "    - RE: A matrix with columns: [id, time_stop, cumulative_event_count].\n",
    "    - nRE: An array containing total number of recurrent events for each ID.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    data_df = pd.DataFrame({\n",
    "        'id': data_dict['id'],\n",
    "        'time_start': data_dict['time_start'],\n",
    "        'time_stop': data_dict['time_stop'],\n",
    "        'event': data_dict['event']\n",
    "    })\n",
    "    \n",
    "    data_sorted = data_df.sort_values(by=['id', 'time_start', 'time_stop'])\n",
    "    data_sorted['cum_event'] = data_sorted.groupby('id')['event'].cumsum()\n",
    "    \n",
    "    RE = data_sorted[['id', 'time_stop', 'cum_event']].values\n",
    "    nRE = data_sorted.groupby('id')['event'].sum().values\n",
    "    \n",
    "    return RE, nRE\n",
    "\n",
    "def generate_cis(data):\n",
    "    # Extract unique IDs\n",
    "    unique_ids = np.unique(data['id'])\n",
    "\n",
    "    # For each unique ID, get the last observed time\n",
    "    common_observation_times = [np.max(data['time_stop'][data['id'] == id_]) for id_ in unique_ids]\n",
    "\n",
    "    # Combine IDs and their corresponding common observation times\n",
    "    Cis = np.column_stack((unique_ids, common_observation_times))\n",
    "    \n",
    "    return Cis\n",
    "\n",
    "def est_cstat(score, N, Cis, RE, nRE):\n",
    "    den = 0\n",
    "    num = 0\n",
    "\n",
    "    # Create a mapping from ID to its index in nRE\n",
    "    unique_ids = np.unique(RE[:, 0].astype(int))\n",
    "    id_to_index = {id_: idx for idx, id_ in enumerate(unique_ids)}\n",
    "\n",
    "    for i in range(N - 1):\n",
    "        ID1 = int(Cis[i, 0])\n",
    "        \n",
    "        # ID1이 nRE의 범위 내에 있는지 확인합니다.\n",
    "        if ID1 >= len(nRE):\n",
    "            continue\n",
    "\n",
    "        # Filter events happening before or at Cis[i, 2]\n",
    "        search_event_indices = np.where(RE[:, 1] <= Cis[i, 1])[0]\n",
    "        REtemp = RE[search_event_indices, :]\n",
    "        \n",
    "        unique_ids, counts = np.unique(REtemp[:, 0].astype(int), return_counts=True)\n",
    "        \n",
    "        valid_indices = [idx for idx in unique_ids if idx < N]\n",
    "        valid_counts = counts[np.isin(unique_ids, valid_indices)]\n",
    "        \n",
    "        nREc = np.zeros(N, dtype=int)\n",
    "        nREc[valid_indices] = valid_counts\n",
    "\n",
    "        IDpair = np.sort(Cis[(i+1):N-1, 0])\n",
    "        IDpair = [id_ for id_ in IDpair if id_ in score and id_ != -1 and id_ >= 0]\n",
    "        \n",
    "        indexed_IDpair = [id_to_index[id_] for id_ in IDpair if id_ in id_to_index]\n",
    "        nREc = nREc[indexed_IDpair]\n",
    "\n",
    "        lt_obs = np.where(nRE[ID1] < nREc, 1, 0)\n",
    "        gt_obs = np.where(nRE[ID1] > nREc, 1, 0)\n",
    "\n",
    "        last_value_from_score = lambda id_: score.get(id_, [0])[-1]\n",
    "        score_values = np.array([last_value_from_score(id_) for id_ in IDpair])\n",
    "        score_current = np.full(score_values.shape, last_value_from_score(ID1))\n",
    "        \n",
    "        lt_pred = np.where(score_current < score_values, 1, 0)\n",
    "        gt_pred = np.where(score_current > score_values, 1, 0)\n",
    "\n",
    "        den += np.sum(lt_obs + gt_obs)\n",
    "        num += np.sum(lt_obs * lt_pred + gt_obs * gt_pred)\n",
    "\n",
    "    estcstat = num / den if den != 0 else 0\n",
    "    return estcstat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import check_array\n",
    "from joblib import Parallel, delayed\n",
    "from numpy.random import RandomState\n",
    "from sklearn.utils import check_random_state, check_array\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from scipy.sparse import issparse\n",
    "MAX_INT = np.iinfo(np.int32).max\n",
    "from sksurv.functions import StepFunction\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def average_mean_functions(times_list, mean_funcs_list):\n",
    "    all_times = []\n",
    "    for sublist in times_list:\n",
    "        if isinstance(sublist, Iterable) and not isinstance(sublist, str):\n",
    "            all_times.extend(sublist)\n",
    "        else:\n",
    "            all_times.append(sublist)\n",
    "    \n",
    "    all_times = sorted(list(set(all_times)))\n",
    "    averaged_mean_func = []\n",
    "    \n",
    "    for t in all_times:\n",
    "        sum_val, count = 0.0, 0\n",
    "        for i, times in enumerate(times_list):\n",
    "            if isinstance(times, int):\n",
    "                if t == times:\n",
    "                    sum_val += mean_funcs_list[i]\n",
    "                    count += 1\n",
    "            else:\n",
    "                if t in times:\n",
    "                    index = times.index(t)\n",
    "                    sum_val += mean_funcs_list[i][index]\n",
    "                    count += 1\n",
    "                else:\n",
    "                    # use the last available value up to that time\n",
    "                    valid_times = [time for time in times if time <= t]\n",
    "                    if valid_times:\n",
    "                        last_time = max(valid_times)\n",
    "                        index = times.index(last_time)\n",
    "                        sum_val += mean_funcs_list[i][index]\n",
    "                        count += 1\n",
    "        averaged_mean_func.append(sum_val / count)\n",
    "    \n",
    "    return all_times, averaged_mean_func\n",
    "\n",
    "\n",
    "def _generate_bootstrap_indices(tree, bootstrap, X, y, random_state, max_ids):\n",
    "    \"\"\"\n",
    "    Private function used to generate bootstrap sample indices in parallel.\n",
    "    \"\"\"\n",
    "    if bootstrap:\n",
    "        unique_ids = np.unique(y['id'])\n",
    "        if isinstance(random_state, np.random.RandomState):\n",
    "            rnd = random_state\n",
    "        else:\n",
    "            rnd = np.random.RandomState(random_state)\n",
    "        \n",
    "        sampled_ids = _generate_sampled_ids(rnd, unique_ids, max_ids)\n",
    "        bootstrap_indices = np.where(np.isin(y['id'], sampled_ids))[0]\n",
    "        return bootstrap_indices\n",
    "    else:\n",
    "        return np.arange(len(y['id']))  # return all indices\n",
    "\n",
    "def _fit_tree_with_bootstrap_samples(tree, X, y_converted, indices):\n",
    "    \"\"\"\n",
    "    Fit a single tree with given bootstrap samples.\n",
    "    \"\"\"\n",
    "    X_bootstrap = X[indices]\n",
    "    y_bootstrap = {\n",
    "        'id': y_converted['id'][indices],\n",
    "        'time_start': y_converted['time_start'][indices],\n",
    "        'time_stop': y_converted['time_stop'][indices],\n",
    "        'event': y_converted['event'][indices]\n",
    "    }\n",
    "    tree.fit(X_bootstrap, y_bootstrap['id'], y_bootstrap['time_start'], y_bootstrap['time_stop'], y_bootstrap['event'])\n",
    "    return tree\n",
    "\n",
    "def _get_unsampled_bootstrap_indices(tree, bootstrap, X, y, random_state, max_ids):\n",
    "    \"\"\"\n",
    "    Private function used to generate unsampled bootstrap sample indices in parallel.\n",
    "    \"\"\"\n",
    "    if bootstrap:\n",
    "        unique_ids = np.unique(y['id'])\n",
    "        if isinstance(random_state, np.random.RandomState):\n",
    "            rnd = random_state\n",
    "        else:\n",
    "            rnd = np.random.RandomState(random_state)\n",
    "        \n",
    "        sampled_ids = _generate_sampled_ids(rnd, unique_ids, max_ids)\n",
    "        unsampled_ids = _generate_unsampled_ids(unique_ids, sampled_ids)\n",
    "        unsampled_indices = np.where(np.isin(y['id'], unsampled_ids))[0]\n",
    "        return unsampled_indices\n",
    "    else:\n",
    "        return np.array([])  # return an empty array for non-bootstrap cases\n",
    "\n",
    "\n",
    "class RecurrentRandomForest(BaseEstimator):\n",
    "\n",
    "    \"\"\"\n",
    "    A Random Forest model designed for recurrent event data.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_ids_split=2,\n",
    "                 min_ids_leaf=1, bootstrap=True, oob_score=False, n_jobs=None,\n",
    "                 random_state=None, verbose=0, warm_start=False, max_ids=1.0,\n",
    "                 min_impurity_decrease=0.0, max_features=None, max_thresholds=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_ids_split = min_ids_split\n",
    "        self.min_ids_leaf = min_ids_leaf\n",
    "        self.bootstrap = bootstrap\n",
    "        self.oob_score = oob_score\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        self.warm_start = warm_start\n",
    "        self.max_ids = max_ids\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.max_features = max_features\n",
    "        self.max_thresholds = max_thresholds\n",
    "        \n",
    "        # Initialize the random state for the forest\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        \n",
    "        # Create the estimators using the updated random states\n",
    "        self.estimators_ = [self._make_estimator() for _ in range(self.n_estimators)]\n",
    "\n",
    "    def _make_estimator(self):\n",
    "        \"\"\"\n",
    "        Constructs a new instance of the 'RecurrentTree' with the specified hyperparameters.\n",
    "        Allows for creating each tree with a different 'random_state' for randomness.\n",
    "        \"\"\"\n",
    "        # Generate a new random state for each tree based on the forest's random state\n",
    "        if isinstance(self.random_state, np.random.Generator):\n",
    "            tree_random_state = self.random_state.integers(np.iinfo(np.int32).max)\n",
    "        else:\n",
    "            tree_random_state = self.random_state.randint(np.iinfo(np.int32).max)\n",
    "\n",
    "        return RecurrentTree(\n",
    "            max_depth=self.max_depth,\n",
    "            min_ids_split=self.min_ids_split,\n",
    "            min_ids_leaf=self.min_ids_leaf,\n",
    "            random_state=tree_random_state,  # Pass the generated random state for the tree\n",
    "            min_impurity_decrease=self.min_impurity_decrease,\n",
    "            max_features=self.max_features,\n",
    "            max_thresholds=self.max_thresholds\n",
    "        )\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the recurrent random forest.\n",
    "        \"\"\"\n",
    "        # Validate the input data\n",
    "        X = self._validate_data(X)\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "    \n",
    "        # Convert y to the required format\n",
    "        y_converted = {\n",
    "            'id': y['id'],\n",
    "            'time_start': y['time_start'],\n",
    "            'time_stop': y['time_stop'],\n",
    "            'event': y['event']\n",
    "        }\n",
    "    \n",
    "        # If max_ids is None, set it to 1.0\n",
    "        if self.max_ids is None:\n",
    "            self.max_ids = 1.0\n",
    "\n",
    "        # Get bootstrap indices for each tree in parallel\n",
    "        bootstrap_indices_list = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_generate_bootstrap_indices)(\n",
    "                tree=tree,\n",
    "                bootstrap=self.bootstrap,\n",
    "                X=X,\n",
    "                y=y_converted,\n",
    "                random_state=tree.random_state,\n",
    "                max_ids=self.max_ids\n",
    "            ) for tree in self.estimators_\n",
    "        )\n",
    "        \n",
    "        # Get unsampled bootstrap indices for each tree in parallel\n",
    "        unsampled_bootstrap_indices_list = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_get_unsampled_bootstrap_indices)(\n",
    "                tree=tree,\n",
    "                bootstrap=self.bootstrap,\n",
    "                X=X,\n",
    "                y=y_converted,\n",
    "                random_state=tree.random_state,\n",
    "                max_ids=self.max_ids\n",
    "            ) for tree in self.estimators_\n",
    "        )\n",
    "        \n",
    "        self.unsampled_bootstrap_indices_list_ = unsampled_bootstrap_indices_list\n",
    "\n",
    "        # Train each tree using its respective bootstrap indices in parallel\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_fit_tree_with_bootstrap_samples)(\n",
    "                tree=tree,\n",
    "                X=X,\n",
    "                y_converted=y_converted,\n",
    "                indices=indices\n",
    "            ) for tree, indices in zip(self.estimators_, bootstrap_indices_list)\n",
    "        )\n",
    "        if self.oob_score:\n",
    "            self._set_oob_score_and_attributes(X, y_converted)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _set_oob_score_and_attributes(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate OOB score and attributes for each unsampled ID.\n",
    "        \"\"\"\n",
    "        mean_function_predictions = defaultdict(lambda: {\"mean_function\": [], \"unique_times\": []})\n",
    "\n",
    "        for estimator, unsampled_indices in zip(self.estimators_, self.unsampled_bootstrap_indices_list_):\n",
    "            unsampled_ids = np.unique(y['id'][unsampled_indices])\n",
    "\n",
    "            tree_results, _, _, _ = estimator.predict_mean_function(X, unsampled_ids)\n",
    "        \n",
    "            for uid, result in tree_results.items():\n",
    "                mean_function_predictions[uid][\"mean_function\"].append(result['mean_function'])\n",
    "                mean_function_predictions[uid][\"unique_times\"].append(result[\"unique_times\"])\n",
    "\n",
    "        self.oob_prediction_ = {}\n",
    "        for uid, data in mean_function_predictions.items():\n",
    "            averaged_x, averaged_y = average_mean_functions(data['unique_times'], data['mean_function'])\n",
    "            self.oob_prediction_[uid] = {\n",
    "                \"unique_times\": averaged_x,\n",
    "                \"mean_function\": averaged_y\n",
    "            }\n",
    "        sorted_oob_predictions = {k: self.oob_prediction_[k] for k in sorted(self.oob_prediction_)}\n",
    "\n",
    "        oob_predictions_ = {uid: result[\"mean_function\"] for uid, result in sorted_oob_predictions.items()}\n",
    "    \n",
    "        self.oob_prediction_ = oob_predictions_\n",
    "\n",
    "        unsampled_ids = np.array(list(mean_function_predictions.keys()))\n",
    "        y_unsampled = {key: y[key][np.isin(y['id'], unsampled_ids)] for key in y}\n",
    "\n",
    "        # Generate RE and nRE matrices using unsampled data\n",
    "        RE, nRE = generate_RE_nRE(y_unsampled)\n",
    "\n",
    "        # Generate Cis using unsampled data\n",
    "        Cis = generate_cis(y_unsampled)\n",
    "\n",
    "        N = len(Cis)\n",
    "\n",
    "        # Calculate C-index using est_cstat function\n",
    "        self.oob_score_ = est_cstat(self.oob_prediction_, N, Cis, RE, nRE)\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"Validate input data('X') to ensure it's in the correct format and meets the necessary conditions for processing.\"\"\"\n",
    "    def _validate_data(self, X, accept_sparse=False, ensure_min_samples=1):\n",
    "        \"\"\"Validate input data('X') to ensure it's in the correct format and meets the necessary conditions for processing.\"\"\"\n",
    "        return check_array(X, accept_sparse=accept_sparse, ensure_min_samples=ensure_min_samples)\n",
    "    \n",
    "    def _validate_X_predict(self, X):\n",
    "        \"\"\"Validate X whenever one tries to predict.\"\"\"\n",
    "        X = check_array(X)\n",
    "        if X.shape[1] != self.n_features_in_:\n",
    "            raise ValueError(\"Number of features of the model must match the input. Model n_features is {} and input n_features is {}.\"\n",
    "                             .format(self.n_features_in_, X.shape[1]))\n",
    "        return X\n",
    "\n",
    "    def predict(self, X, ids):\n",
    "        # Initialize a dictionary to store mean_function and unique_times for each ID.\n",
    "        mean_function_predictions = {\n",
    "            uid: {\n",
    "                \"mean_function\": [],\n",
    "                \"unique_times\": []\n",
    "            } for uid in ids\n",
    "        }\n",
    "    \n",
    "        # Get predictions from each tree in the forest\n",
    "        all_tree_results = [tree.predict_mean_function(X, ids) for tree in self.estimators_]\n",
    "\n",
    "        # Aggregate the results from each tree\n",
    "        for tree_result_tuple in all_tree_results:\n",
    "            tree_result = tree_result_tuple[0]  # Assuming the first element is the dictionary.\n",
    "            for uid in tree_result:\n",
    "                result = tree_result[uid]\n",
    "            \n",
    "                # Append mean_function and unique_times values\n",
    "                mean_function_predictions[uid][\"mean_function\"].append(result['mean_function'])\n",
    "                mean_function_predictions[uid][\"unique_times\"].append(result[\"unique_times\"])\n",
    "\n",
    "        # Average the predictions and compile the final results\n",
    "        averaged_predictions = {}\n",
    "        for uid, data in mean_function_predictions.items():\n",
    "            averaged_x, averaged_y = average_mean_functions(data['unique_times'], data['mean_function'])\n",
    "            averaged_predictions[uid] = {\n",
    "                \"unique_times\": averaged_x,\n",
    "                \"mean_function\": averaged_y\n",
    "            }\n",
    "        \n",
    "        return averaged_predictions\n",
    "\n",
    "    def predict_mean_function(self, X, ids):\n",
    "        \"\"\"\n",
    "        Predict the mean function for the given samples using all trees in the forest.\n",
    "        \"\"\"\n",
    "        # Ensure that the model has been trained\n",
    "        if not hasattr(self, \"estimators_\"):\n",
    "            raise NotFittedError(\"The model is not fitted yet. Call 'fit' with appropriate arguments before using this method.\")\n",
    "    \n",
    "        # Use the `predict` method to get the predictions\n",
    "        results = self.predict(X, ids)\n",
    "    \n",
    "        # Extract the mean_function from the results\n",
    "        mean_functions = {uid: result[\"mean_function\"] for uid, result in results.items()}\n",
    "    \n",
    "        return mean_functions\n",
    "\n",
    "    def predict_rate_function(self, X, ids):\n",
    "        \"\"\"\n",
    "        Predict the rate function for the given samples using all trees in the forest.\n",
    "        \"\"\"\n",
    "        # mean_function을 예측\n",
    "        mean_functions = self.predict_mean_function(X, ids)\n",
    "\n",
    "        # 각 mean_function을 사용하여 rate function 계산\n",
    "        rate_functions = {}\n",
    "        for uid, mean_function_values in mean_functions.items():\n",
    "            # 시간 간격의 차이를 사용하여 rate function 계산\n",
    "            rate_function = np.diff(mean_function_values)\n",
    "            rate_functions[uid] = rate_function\n",
    "\n",
    "        return rate_functions\n",
    "\n",
    "\n",
    "RecurrentRandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_permutation_importance(model, X, y, n_repeats=30, random_state=None):\n",
    "    \"\"\"\n",
    "    주어진 RecurrentRandomForest 모델에 대한 특성의 Permutation Importance를 계산합니다.\n",
    "    이 함수는 아이디별로 데이터를 섞되, 특성 행렬의 값은 섞인 아이디의 첫 번째 값으로 채웁니다.\n",
    "\n",
    "    Parameters:\n",
    "    - model: RecurrentRandomForest 모델\n",
    "    - X: 입력 특성\n",
    "    - y: 타겟\n",
    "    - n_repeats: 중요도를 계산하기 위해 각 특성을 섞을 횟수.\n",
    "    - random_state: 재현성을 위한 시드\n",
    "\n",
    "    Returns:\n",
    "    - importances: 각 특성의 Permutation Importance를 포함하는 2D 배열.\n",
    "    - importances_mean: 각 특성의 중요도 평균.\n",
    "    - importances_std: 각 특성의 중요도 표준편차.\n",
    "    \"\"\"\n",
    "    \n",
    "    random_state = check_random_state(random_state)\n",
    "    \n",
    "    # 원래 데이터를 사용하여 예측 수행\n",
    "    baseline_preds = model.predict_mean_function(X, y['id'])\n",
    "    RE, nRE = generate_RE_nRE(y)\n",
    "    Cis = generate_cis(y)\n",
    "    N = len(Cis)\n",
    "    baseline_score = est_cstat(baseline_preds, N, Cis, RE, nRE)\n",
    "    \n",
    "    n_features = X.shape[1]\n",
    "    unique_ids = np.unique(y['id'])\n",
    "    importances = np.zeros((n_repeats, n_features))\n",
    "    \n",
    "    for feature_idx in range(n_features):\n",
    "        for repeat in range(n_repeats):\n",
    "            X_permuted = X.copy()\n",
    "            \n",
    "            # Shuffle data by ID\n",
    "            shuffled_ids = random_state.permutation(unique_ids)\n",
    "            for original_id, shuffled_id in zip(unique_ids, shuffled_ids):\n",
    "                idx_original = np.where(y['id'] == original_id)[0]\n",
    "                idx_shuffled = np.where(y['id'] == shuffled_id)[0]\n",
    "                X_permuted[idx_original, feature_idx] = X[idx_shuffled[0], feature_idx]\n",
    "            \n",
    "            # 섞인 데이터를 사용하여 예측 수행\n",
    "            preds_permuted = model.predict_mean_function(X_permuted, y['id'])\n",
    "            score_permuted = est_cstat(preds_permuted, N, Cis, RE, nRE)\n",
    "            \n",
    "            # 특성의 중요도는 모델의 성능이 임의로 섞였을 때 얼마나 감소하는지를 기반으로 합니다.\n",
    "            importances[repeat, feature_idx] = baseline_score - score_permuted\n",
    "                \n",
    "    importances_mean = importances.mean(axis=0)\n",
    "    importances_std = importances.std(axis=0)\n",
    "    \n",
    "    return importances, importances_mean, importances_std\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
